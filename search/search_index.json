{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cloud Pipeline Introduction Cloud Pipeline Introduction Why Cloud Pipeline Components Why Cloud Pipeline Cloud Pipeline solution from EPAM provides an easy and scalable approach to perform a wide range of analysis tasks in the cloud environment. This solution takes the best of two approaches: classic HPC solutions (based on GridEngine schedulers family) and SaaS cloud solutions. Feature Classic \"HPC\" \"Cloud Pipeline\" SaaS Pipelines customization High . Direct scripting - any level of customization High . Direct scripting - any level of customization Low . Provide specific pipeline definition languages or even only a graphical editor Backward compatibility N/A High . \"Classic\" HPC scripts and NGS tools can be run without any changes Low . Scripts have to be rewritten according to the supported languages and storage structures User Interface Command Line Graphical Interface for User interaction and a Command Line Interface for automation scripts Graphical Interface Calculation power scalability Low . New nodes shall be deployed and supported on-premises. Idle nodes are still consuming resources High . New nodes are started according to the job request and terminated as soon as they are not needed anymore. Each job can precisely define required CPU/RAM/Disk resources or even select optimal node up to speed up execution (e.g. memory optimized nodes for cellranger pipelines) High . Scalable as \"Cloud Pipeline\" but sometimes limits user to predefined nodes setup Deployment and vendor-lock Deployed on-premises and introduces no vendor-lock Can be deployed in AWS/GCP/Azure or on-premises, thus introduces no vendor-lock Consumed as an Internet service (no on-premises deployment available), all processes are tied to this specific vendor Security High . All data and analysis processes are located in a controlled network. High . All data and analysis processes are located in a controlled cloud VPC. All security configurations are performed by user's security officers Low . No direct control over security configuration. SaaS vendor has full access to the data storages. Components The main components of the Cloud Pipeline are shown below:","title":"Introduction"},{"location":"#cloud-pipeline-introduction","text":"Cloud Pipeline Introduction Why Cloud Pipeline Components","title":"Cloud Pipeline Introduction"},{"location":"#why-cloud-pipeline","text":"Cloud Pipeline solution from EPAM provides an easy and scalable approach to perform a wide range of analysis tasks in the cloud environment. This solution takes the best of two approaches: classic HPC solutions (based on GridEngine schedulers family) and SaaS cloud solutions. Feature Classic \"HPC\" \"Cloud Pipeline\" SaaS Pipelines customization High . Direct scripting - any level of customization High . Direct scripting - any level of customization Low . Provide specific pipeline definition languages or even only a graphical editor Backward compatibility N/A High . \"Classic\" HPC scripts and NGS tools can be run without any changes Low . Scripts have to be rewritten according to the supported languages and storage structures User Interface Command Line Graphical Interface for User interaction and a Command Line Interface for automation scripts Graphical Interface Calculation power scalability Low . New nodes shall be deployed and supported on-premises. Idle nodes are still consuming resources High . New nodes are started according to the job request and terminated as soon as they are not needed anymore. Each job can precisely define required CPU/RAM/Disk resources or even select optimal node up to speed up execution (e.g. memory optimized nodes for cellranger pipelines) High . Scalable as \"Cloud Pipeline\" but sometimes limits user to predefined nodes setup Deployment and vendor-lock Deployed on-premises and introduces no vendor-lock Can be deployed in AWS/GCP/Azure or on-premises, thus introduces no vendor-lock Consumed as an Internet service (no on-premises deployment available), all processes are tied to this specific vendor Security High . All data and analysis processes are located in a controlled network. High . All data and analysis processes are located in a controlled cloud VPC. All security configurations are performed by user's security officers Low . No direct control over security configuration. SaaS vendor has full access to the data storages.","title":"Why Cloud Pipeline"},{"location":"#components","text":"The main components of the Cloud Pipeline are shown below:","title":"Components"},{"location":"api/API_tutorials/API_tutorials/","text":"API tutorials - Usage scenario This sections provides a number of implementations of the data transfer/processing automation via the Cloud Pipeline API capabilities. We'll use a quite common usage scenario to implement the automation via different approaches: process a local 10xGenomics dataset (e.g. produced by the on-prem machinery) in the Cloud Pipeline compute environment. The scenario for the automation consists of the following steps: Upload a dataset (directory with the FASTQ files to the S3 bucket) Run the dataset processing using the cellranger count command Download the data processing results back to a local filesystem The subsequent sections provide implementation examples in a number of languages: pipe CLI implementation Direct HTTP calls via curl JavaScript implementation","title":"API tutorials"},{"location":"api/API_tutorials/API_tutorials/#api-tutorials-usage-scenario","text":"This sections provides a number of implementations of the data transfer/processing automation via the Cloud Pipeline API capabilities. We'll use a quite common usage scenario to implement the automation via different approaches: process a local 10xGenomics dataset (e.g. produced by the on-prem machinery) in the Cloud Pipeline compute environment. The scenario for the automation consists of the following steps: Upload a dataset (directory with the FASTQ files to the S3 bucket) Run the dataset processing using the cellranger count command Download the data processing results back to a local filesystem The subsequent sections provide implementation examples in a number of languages: pipe CLI implementation Direct HTTP calls via curl JavaScript implementation","title":"API tutorials - Usage scenario"},{"location":"api/API_tutorials/Automation_via_CLI/","text":"\"pipe\" CLI implementation This approach does not use the HTTP/REST API directly. Instead, a command-line wrapper is used. pipe CLI is a command line utility, distributed together with the Cloud Pipeline. It offers a variety of the commands for an easy automation of the common tasks. More details on the pipe command line interface are available in the corresponding docs section This utility can be used for any backend Cloud Provider, which is enabled for the CLoud Pipeline deployment (e.g. the command are all the same if using AWS/GCP/Azure). The listed below cellranger_pipe.sh script, implements the \"Usage scenario\" using the only \"pipe\" command to communicate with the cloud platform. #!/bin/bash ############################################################# # Example dataset: # https://cloud-pipeline-oss-builds.s3.amazonaws.com/tools/cellranger/data/tiny-fastq/tiny-fastq.tgz # Example run command: # cellranger.sh --fastqs ~/tiny-fastq \\ # --transcriptome tiny \\ # --workdir s3://my_bucket/tiny-example \\ # --copy-back ############################################################# ############################################################# # Parse options ############################################################# POSITIONAL=() while [[ $# -gt 0 ]]; do key= $1 case $key in -f|--fastqs) FASTQS= $2 shift shift ;; -t|--transcriptome) TRANSCRIPTOME= $2 shift shift ;; -w|--workdir) WORKDIR= $2 shift shift ;; -i|--instance) INSTANCE_TYPE= $2 shift shift ;; -d|--disk) INSTANCE_DISK= $2 shift shift ;; -c|--copy-back) COPY_BACK=1 shift esac done ############################################################# # Check prerequisites ############################################################# if ! command -v pipe /dev/null 2 1; then cat EOF [ERROR] `pipe` Command Line Interface is not available. Please follow the installation and configuration instructions, available in the Cloud Pipeline GUI: * Login to the GUI * Open Settings (from the left panel) * Click Get access key * Follow the installation instructions EOF exit 1 fi ############################################################# # Validate options ############################################################# if [ -z $FASTQS ] || [ ! -d $FASTQS ]; then echo [ERROR] Path to the fastq files is not set or is not a directory exit 1 fi if [ $TRANSCRIPTOME ]; then case $TRANSCRIPTOME in human) TRANSCRIPTOME_S3= s3://genome-bucket/human/transcriptome ;; mouse) TRANSCRIPTOME_S3= s3://genome-bucket/mouse/transcriptome ;; human-mouse) TRANSCRIPTOME_S3= s3://genome-bucket/human-mouse/transcriptome ;; tiny) TRANSCRIPTOME_S3= s3://genome-bucket/tiny/transcriptome ;; *) echo [ERROR] Transcriptome name does not match the supported types: human, mouse, human-mouse, tiny exit 1 ;; esac else echo [ERROR] Transcriptome name is not set exit 1 fi if [ -z $WORKDIR ] || [[ $WORKDIR != s3:// * ]]; then echo [ERROR] S3 working directory is not set or uses an unexpected schema (s3:// shall be used) exit 1 else WORKDIR_EXISTS=$(pipe storage ls $WORKDIR) if [ $WORKDIR_EXISTS ]; then echo [ERROR] S3 working directory ($WORKDIR) already exists, please specify a new location exit 1 fi fi EXTRA_OPTIONS=() if [ $INSTANCE_TYPE ]; then EXTRA_OPTIONS+=( --instance-type $INSTANCE_TYPE ) fi if [ $INSTANCE_TYPE ]; then EXTRA_OPTIONS+=( --instance-disk $INSTANCE_DISK ) fi ############################################################# # Transfer the local fastq files to the S3 working directory ############################################################# FASTQS_S3= $WORKDIR/fastq/$(basename $FASTQS) echo Transferring fastqs to the S3 working directory: $FASTQS - $FASTQS_S3/ pipe storage cp $FASTQS $FASTQS_S3/ --recursive if [ $? -ne 0 ]; then echo [ERROR] Cannot upload $FASTQS to $WORKDIR exit 1 fi ############################################################# # Setup the paths and run options ############################################################# RESULTS_S3= $WORKDIR/results DOCKER_IMAGE= ngs/cellranger:latest ############################################################# # Launch data processing ############################################################# echo Launch job with parameters: echo fastqs: $FASTQS_S3 echo transcriptome: $TRANSCRIPTOME_S3 echo results: $RESULTS_S3 pipe run --docker-image $DOCKER_IMAGE \\ --fastqs input?$FASTQS_S3 \\ --transcriptome input?$TRANSCRIPTOME_S3 \\ --results output?$RESULTS_S3 \\ --cmd-template 'cellranger count --id cloud-cellranger --fastqs $fastqs --transcriptome $transcriptome' \\ --yes \\ --sync ${EXTRA_OPTIONS[@]} if [ $? -ne 0 ]; then echo [ERROR] Failed to process the dataset exit 1 fi echo [OK] Job has finished ############################################################# # Copy the results back, if requested ############################################################# if [ $COPY_BACK ]; then RESULTS_LOCAL=$(pwd)/$(basename $RESULTS_S3) echo Transferring results locally: $RESULTS_S3 - $RESULTS_LOCAL pipe storage cp $RESULTS_S3 $RESULTS_LOCAL --recursive if [ $? -ne 0 ]; then echo [ERROR] Cannot download $RESULTS_S3 to $RESULTS_LOCAL fi echo [OK] Data processing results are downloaded to $RESULTS_LOCAL else echo [OK] Data processing results are available in the S3 working directory: $RESULTS_S3 fi To launch the data processing, the script above can be launched using the following command: # Get the tiny dataset (or use your own data) cd ~ wget https://cloud-pipeline-oss-builds.s3.amazonaws.com/tools/cellranger/data/tiny-fastq/tiny-fastq.tgz tar -zxvf tiny-fastq.tgz # Run the data processing # It is assumed that cellranger.sh is stored in ~/cellranger.sh chmod +x ~/cellranger.sh ./cellranger.sh --fastqs ~/tiny-fastq \\ --transcriptome tiny \\ --workdir s3://my_bucket/tiny-example \\ --copy-back Transferring fastqs to the S3 working directory: ~/tiny-fastq - s3://my_bucket/tiny-example/fastq/tiny-fastq/ Launch job with parameters: fastqs: s3://my_bucket/tiny-example/fastq/tiny-fastq transcriptome: s3://genome-bucket/tiny/transcriptome results: s3://my_bucket/tiny-example/results Pipeline run scheduled with RunId: 5865 Pipeline run 5865 completed with status SUCCESS Transferring results locally: s3://my_bucket/tiny-example/results - ~/results [OK] Data processing results are downloaded to ~/results","title":"Automation via CLI"},{"location":"api/API_tutorials/Automation_via_CLI/#pipe-cli-implementation","text":"This approach does not use the HTTP/REST API directly. Instead, a command-line wrapper is used. pipe CLI is a command line utility, distributed together with the Cloud Pipeline. It offers a variety of the commands for an easy automation of the common tasks. More details on the pipe command line interface are available in the corresponding docs section This utility can be used for any backend Cloud Provider, which is enabled for the CLoud Pipeline deployment (e.g. the command are all the same if using AWS/GCP/Azure). The listed below cellranger_pipe.sh script, implements the \"Usage scenario\" using the only \"pipe\" command to communicate with the cloud platform. #!/bin/bash ############################################################# # Example dataset: # https://cloud-pipeline-oss-builds.s3.amazonaws.com/tools/cellranger/data/tiny-fastq/tiny-fastq.tgz # Example run command: # cellranger.sh --fastqs ~/tiny-fastq \\ # --transcriptome tiny \\ # --workdir s3://my_bucket/tiny-example \\ # --copy-back ############################################################# ############################################################# # Parse options ############################################################# POSITIONAL=() while [[ $# -gt 0 ]]; do key= $1 case $key in -f|--fastqs) FASTQS= $2 shift shift ;; -t|--transcriptome) TRANSCRIPTOME= $2 shift shift ;; -w|--workdir) WORKDIR= $2 shift shift ;; -i|--instance) INSTANCE_TYPE= $2 shift shift ;; -d|--disk) INSTANCE_DISK= $2 shift shift ;; -c|--copy-back) COPY_BACK=1 shift esac done ############################################################# # Check prerequisites ############################################################# if ! command -v pipe /dev/null 2 1; then cat EOF [ERROR] `pipe` Command Line Interface is not available. Please follow the installation and configuration instructions, available in the Cloud Pipeline GUI: * Login to the GUI * Open Settings (from the left panel) * Click Get access key * Follow the installation instructions EOF exit 1 fi ############################################################# # Validate options ############################################################# if [ -z $FASTQS ] || [ ! -d $FASTQS ]; then echo [ERROR] Path to the fastq files is not set or is not a directory exit 1 fi if [ $TRANSCRIPTOME ]; then case $TRANSCRIPTOME in human) TRANSCRIPTOME_S3= s3://genome-bucket/human/transcriptome ;; mouse) TRANSCRIPTOME_S3= s3://genome-bucket/mouse/transcriptome ;; human-mouse) TRANSCRIPTOME_S3= s3://genome-bucket/human-mouse/transcriptome ;; tiny) TRANSCRIPTOME_S3= s3://genome-bucket/tiny/transcriptome ;; *) echo [ERROR] Transcriptome name does not match the supported types: human, mouse, human-mouse, tiny exit 1 ;; esac else echo [ERROR] Transcriptome name is not set exit 1 fi if [ -z $WORKDIR ] || [[ $WORKDIR != s3:// * ]]; then echo [ERROR] S3 working directory is not set or uses an unexpected schema (s3:// shall be used) exit 1 else WORKDIR_EXISTS=$(pipe storage ls $WORKDIR) if [ $WORKDIR_EXISTS ]; then echo [ERROR] S3 working directory ($WORKDIR) already exists, please specify a new location exit 1 fi fi EXTRA_OPTIONS=() if [ $INSTANCE_TYPE ]; then EXTRA_OPTIONS+=( --instance-type $INSTANCE_TYPE ) fi if [ $INSTANCE_TYPE ]; then EXTRA_OPTIONS+=( --instance-disk $INSTANCE_DISK ) fi ############################################################# # Transfer the local fastq files to the S3 working directory ############################################################# FASTQS_S3= $WORKDIR/fastq/$(basename $FASTQS) echo Transferring fastqs to the S3 working directory: $FASTQS - $FASTQS_S3/ pipe storage cp $FASTQS $FASTQS_S3/ --recursive if [ $? -ne 0 ]; then echo [ERROR] Cannot upload $FASTQS to $WORKDIR exit 1 fi ############################################################# # Setup the paths and run options ############################################################# RESULTS_S3= $WORKDIR/results DOCKER_IMAGE= ngs/cellranger:latest ############################################################# # Launch data processing ############################################################# echo Launch job with parameters: echo fastqs: $FASTQS_S3 echo transcriptome: $TRANSCRIPTOME_S3 echo results: $RESULTS_S3 pipe run --docker-image $DOCKER_IMAGE \\ --fastqs input?$FASTQS_S3 \\ --transcriptome input?$TRANSCRIPTOME_S3 \\ --results output?$RESULTS_S3 \\ --cmd-template 'cellranger count --id cloud-cellranger --fastqs $fastqs --transcriptome $transcriptome' \\ --yes \\ --sync ${EXTRA_OPTIONS[@]} if [ $? -ne 0 ]; then echo [ERROR] Failed to process the dataset exit 1 fi echo [OK] Job has finished ############################################################# # Copy the results back, if requested ############################################################# if [ $COPY_BACK ]; then RESULTS_LOCAL=$(pwd)/$(basename $RESULTS_S3) echo Transferring results locally: $RESULTS_S3 - $RESULTS_LOCAL pipe storage cp $RESULTS_S3 $RESULTS_LOCAL --recursive if [ $? -ne 0 ]; then echo [ERROR] Cannot download $RESULTS_S3 to $RESULTS_LOCAL fi echo [OK] Data processing results are downloaded to $RESULTS_LOCAL else echo [OK] Data processing results are available in the S3 working directory: $RESULTS_S3 fi To launch the data processing, the script above can be launched using the following command: # Get the tiny dataset (or use your own data) cd ~ wget https://cloud-pipeline-oss-builds.s3.amazonaws.com/tools/cellranger/data/tiny-fastq/tiny-fastq.tgz tar -zxvf tiny-fastq.tgz # Run the data processing # It is assumed that cellranger.sh is stored in ~/cellranger.sh chmod +x ~/cellranger.sh ./cellranger.sh --fastqs ~/tiny-fastq \\ --transcriptome tiny \\ --workdir s3://my_bucket/tiny-example \\ --copy-back Transferring fastqs to the S3 working directory: ~/tiny-fastq - s3://my_bucket/tiny-example/fastq/tiny-fastq/ Launch job with parameters: fastqs: s3://my_bucket/tiny-example/fastq/tiny-fastq transcriptome: s3://genome-bucket/tiny/transcriptome results: s3://my_bucket/tiny-example/results Pipeline run scheduled with RunId: 5865 Pipeline run 5865 completed with status SUCCESS Transferring results locally: s3://my_bucket/tiny-example/results - ~/results [OK] Data processing results are downloaded to ~/results","title":"\"pipe\" CLI implementation"},{"location":"api/API_tutorials/Direct_HTTP_API/","text":"Direct HTTP calls via curl Cloud Pipeline's HTTP/REST API can be also consumed directly. From any language/framework. Here we show to implement the Usage scenario using the curl command to query the API. All available HTTP/REST API methods are listed in the Cloud Pipeline Swagger UI (available as https:// host /pipeline/restapi/swagger-ui.html ) In this particular example script cellranger_curl.sh , which is listed below, the following API calls are performed: GET /datastorage/find to find the ID of the \"WORKDIR\" S3 bucket POST /datastorage/tempCredentials/ to get the access token to the \"WORKDIR\" S3 bucket, which can passed to the AWS SDK for the data transfer POST /pipeline/run to submit a job to the Cloud Pipeline compute environment GET /pipeline/run to get the running job status and wait for the completion #!/bin/bash ############################################################# # Setup the parameters ############################################################# # Cloud Pipeline API entrypoint, e.g. https:// host /pipeline API_URL= # Shall be generated in the Cloud Pipeline GUI - Settings - CLI - Generate access key API_TOKEN= # Data location: # S3 bucket, that is going to be used as a working directory # Local fastq files and processing results will uploaded there # Example value: s3://my_bucket/workdir WORKDIR= # Path to a local directory, that holds the FASTQ files for processing # E.g. ~/tiny-fastq FASTQS= # Path to the cellranger transcriptome # It shall be located in the S3 bucket already # In this example we use the tiny reference, which is only 300Mb and allows to debug jobs quickly TRANSCRIPTOME_S3= s3://genome-bucket/tiny/transcriptome # Path to the S3 location, that will hold the data processing results RESULTS_S3= $WORKDIR/results # Job parameters: # The docker image, which holds a cellranger binary and environment DOCKER_IMAGE= single-cell/cellranger:latest # The size of the machine, that is going to process the data # If it's not set - the default hardware for the $DOCKER_IMAGE is going to used INSTANCE_TYPE= r5.xlarge # The size of disk volume in Gb, that will be attached to the machine defined by $INSTANCE_TYPE # If it's not set - the default hardware for the $DOCKER_IMAGE is going to used INSTANCE_DISK= 100 ############################################################# # Get the S3 working directory bucket ID ############################################################# BUCKET_NAME=$(cut -d/ -f 3 $WORKDIR) RESPONSE=$(curl -ks -X GET -H Authorization: Bearer $API_TOKEN -H Accept: application/json $API_URL/restapi/datastorage/find?id=$BUCKET_NAME ) BUCKET_ID=$(jq -r .payload.id $RESPONSE) ############################################################# # Get the S3 working directory bucket access token by it's ID ############################################################# RESPONSE=$(curl -ks -X POST -H Authorization: Bearer $API_TOKEN -H Content-Type: application/json -H Accept: application/json -d [ { \\ id\\ : $BUCKET_ID, \\ read\\ : true, \\ write\\ : true } ] $API_URL/restapi/datastorage/tempCredentials/ ) # The resulting keys, shall be used to configure the AWS SDK for the data transfer # Here we use AWS CLI SDK, but this will work for any other (e.g. Java/JS/Go/Python/...) export AWS_ACCESS_KEY_ID=$(jq -r '.payload.keyID' $RESPONSE) export AWS_SECRET_ACCESS_KEY=$(jq -r '.payload.accessKey' $RESPONSE) export AWS_SESSION_TOKEN=$(jq -r '.payload.token' $RESPONSE) ############################################################# # Transfer the local fastq files to the S3 working directory using the direct call to AWS SDK ############################################################# # Files will be uploaded to e.g. s3://my_bucket/workdir/fastq/tiny-fastq/ FASTQS_S3= $WORKDIR/fastq/$(basename $FASTQS) aws s3 cp $FASTQS $FASTQS_S3/ --recursive ############################################################# # Run processing ############################################################# RESPONSE=$(curl -ks -X POST -H Authorization: Bearer $API_TOKEN -H Content-Type: application/json -H Accept: application/json -d { \\ cmdTemplate\\ : \\ cellranger count --id cloud-cellranger --fastqs \\$fastqs --transcriptome \\$transcriptome\\ , \\ dockerImage\\ : single-cell/cellranger:latest , \\ hddSize\\ : $INSTANCE_DISK, \\ instanceType\\ : \\ $INSTANCE_TYPE\\ , \\ params\\ : { \\ fastqs\\ : { \\ type\\ : \\ input\\ , \\ value\\ : \\ $FASTQS_S3\\ }, \\ results\\ : { \\ type\\ :\\ output\\ , \\ value\\ :\\ $RESULTS_S3\\ }, \\ transcriptome\\ : { \\ type\\ : \\ input\\ , \\ value\\ : \\ $TRANSCRIPTOME_S3\\ } } } $API_URL/restapi/run ) RUN_ID=$(jq -r .payload.id $RESPONSE) ############################################################# # Poll the run status each 30s, until it's finished ############################################################# RUN_STATUS= NA while [ $RUN_STATUS != SUCCESS ]; do sleep 30 RESPONSE=$(curl -ks -X GET -H Authorization: Bearer $API_TOKEN -H Accept: application/json $API_URL/restapi/run/$RUN_ID ) RUN_STATUS=$(jq -r .payload.status $RESPONSE) done","title":"Direct HTTP API implementation"},{"location":"api/API_tutorials/Direct_HTTP_API/#direct-http-calls-via-curl","text":"Cloud Pipeline's HTTP/REST API can be also consumed directly. From any language/framework. Here we show to implement the Usage scenario using the curl command to query the API. All available HTTP/REST API methods are listed in the Cloud Pipeline Swagger UI (available as https:// host /pipeline/restapi/swagger-ui.html ) In this particular example script cellranger_curl.sh , which is listed below, the following API calls are performed: GET /datastorage/find to find the ID of the \"WORKDIR\" S3 bucket POST /datastorage/tempCredentials/ to get the access token to the \"WORKDIR\" S3 bucket, which can passed to the AWS SDK for the data transfer POST /pipeline/run to submit a job to the Cloud Pipeline compute environment GET /pipeline/run to get the running job status and wait for the completion #!/bin/bash ############################################################# # Setup the parameters ############################################################# # Cloud Pipeline API entrypoint, e.g. https:// host /pipeline API_URL= # Shall be generated in the Cloud Pipeline GUI - Settings - CLI - Generate access key API_TOKEN= # Data location: # S3 bucket, that is going to be used as a working directory # Local fastq files and processing results will uploaded there # Example value: s3://my_bucket/workdir WORKDIR= # Path to a local directory, that holds the FASTQ files for processing # E.g. ~/tiny-fastq FASTQS= # Path to the cellranger transcriptome # It shall be located in the S3 bucket already # In this example we use the tiny reference, which is only 300Mb and allows to debug jobs quickly TRANSCRIPTOME_S3= s3://genome-bucket/tiny/transcriptome # Path to the S3 location, that will hold the data processing results RESULTS_S3= $WORKDIR/results # Job parameters: # The docker image, which holds a cellranger binary and environment DOCKER_IMAGE= single-cell/cellranger:latest # The size of the machine, that is going to process the data # If it's not set - the default hardware for the $DOCKER_IMAGE is going to used INSTANCE_TYPE= r5.xlarge # The size of disk volume in Gb, that will be attached to the machine defined by $INSTANCE_TYPE # If it's not set - the default hardware for the $DOCKER_IMAGE is going to used INSTANCE_DISK= 100 ############################################################# # Get the S3 working directory bucket ID ############################################################# BUCKET_NAME=$(cut -d/ -f 3 $WORKDIR) RESPONSE=$(curl -ks -X GET -H Authorization: Bearer $API_TOKEN -H Accept: application/json $API_URL/restapi/datastorage/find?id=$BUCKET_NAME ) BUCKET_ID=$(jq -r .payload.id $RESPONSE) ############################################################# # Get the S3 working directory bucket access token by it's ID ############################################################# RESPONSE=$(curl -ks -X POST -H Authorization: Bearer $API_TOKEN -H Content-Type: application/json -H Accept: application/json -d [ { \\ id\\ : $BUCKET_ID, \\ read\\ : true, \\ write\\ : true } ] $API_URL/restapi/datastorage/tempCredentials/ ) # The resulting keys, shall be used to configure the AWS SDK for the data transfer # Here we use AWS CLI SDK, but this will work for any other (e.g. Java/JS/Go/Python/...) export AWS_ACCESS_KEY_ID=$(jq -r '.payload.keyID' $RESPONSE) export AWS_SECRET_ACCESS_KEY=$(jq -r '.payload.accessKey' $RESPONSE) export AWS_SESSION_TOKEN=$(jq -r '.payload.token' $RESPONSE) ############################################################# # Transfer the local fastq files to the S3 working directory using the direct call to AWS SDK ############################################################# # Files will be uploaded to e.g. s3://my_bucket/workdir/fastq/tiny-fastq/ FASTQS_S3= $WORKDIR/fastq/$(basename $FASTQS) aws s3 cp $FASTQS $FASTQS_S3/ --recursive ############################################################# # Run processing ############################################################# RESPONSE=$(curl -ks -X POST -H Authorization: Bearer $API_TOKEN -H Content-Type: application/json -H Accept: application/json -d { \\ cmdTemplate\\ : \\ cellranger count --id cloud-cellranger --fastqs \\$fastqs --transcriptome \\$transcriptome\\ , \\ dockerImage\\ : single-cell/cellranger:latest , \\ hddSize\\ : $INSTANCE_DISK, \\ instanceType\\ : \\ $INSTANCE_TYPE\\ , \\ params\\ : { \\ fastqs\\ : { \\ type\\ : \\ input\\ , \\ value\\ : \\ $FASTQS_S3\\ }, \\ results\\ : { \\ type\\ :\\ output\\ , \\ value\\ :\\ $RESULTS_S3\\ }, \\ transcriptome\\ : { \\ type\\ : \\ input\\ , \\ value\\ : \\ $TRANSCRIPTOME_S3\\ } } } $API_URL/restapi/run ) RUN_ID=$(jq -r .payload.id $RESPONSE) ############################################################# # Poll the run status each 30s, until it's finished ############################################################# RUN_STATUS= NA while [ $RUN_STATUS != SUCCESS ]; do sleep 30 RESPONSE=$(curl -ks -X GET -H Authorization: Bearer $API_TOKEN -H Accept: application/json $API_URL/restapi/run/$RUN_ID ) RUN_STATUS=$(jq -r .payload.status $RESPONSE) done","title":"Direct HTTP calls via curl"},{"location":"api/API_tutorials/JavaScript_example/","text":"JavaScript implementation The JavaScript example implementation of the Usage scenario offers a Web form, that allows to specify the job input parameters and submit it into the Cloud Pipeline via the API. Prerequisites NodeJS 10.15.3+ npm 6.4.1+ Setup the configuration Locate the JavaScript sample application at js_example or clone the the Cloud Pipeline repository git clone https://github.com/epam/cloud-pipeline cd cloud-pipeline/docs/md/api/API_tutorials/attachments/js_example/ Open the configuration file config.js and replace the following values: host - set to the host of the Cloud Pipeline API storage_id - set to the ID of the bucket, that is going to be used as a \"working directory\" FASTQ files and processing results will placed into this bucket Start the application # Install dependencies and start the app # UI will be served on 0.0.0.0:3010 npm install npm run start Application description Once app is built and loaded in the web-browser one can perform the following operations: Setup the cellranger parameters Set the location of the FASTQ files Choose the transcriptome Specify the \"workdir\", where the job will keep the results Once the job parameters are set, user can click LAUNCH and the job is submitted to the Cloud Pipeline backend via the HTTP/REST API Application will poll the Cloud Pipeline API until the job is finished Once done, application will load the cellranger's data processing summary from the job results folder and display it in the web-browser","title":"JavaScript example"},{"location":"api/API_tutorials/JavaScript_example/#javascript-implementation","text":"The JavaScript example implementation of the Usage scenario offers a Web form, that allows to specify the job input parameters and submit it into the Cloud Pipeline via the API.","title":"JavaScript implementation"},{"location":"api/API_tutorials/JavaScript_example/#prerequisites","text":"NodeJS 10.15.3+ npm 6.4.1+","title":"Prerequisites"},{"location":"api/API_tutorials/JavaScript_example/#setup-the-configuration","text":"Locate the JavaScript sample application at js_example or clone the the Cloud Pipeline repository git clone https://github.com/epam/cloud-pipeline cd cloud-pipeline/docs/md/api/API_tutorials/attachments/js_example/ Open the configuration file config.js and replace the following values: host - set to the host of the Cloud Pipeline API storage_id - set to the ID of the bucket, that is going to be used as a \"working directory\" FASTQ files and processing results will placed into this bucket","title":"Setup the configuration"},{"location":"api/API_tutorials/JavaScript_example/#start-the-application","text":"# Install dependencies and start the app # UI will be served on 0.0.0.0:3010 npm install npm run start","title":"Start the application"},{"location":"api/API_tutorials/JavaScript_example/#application-description","text":"Once app is built and loaded in the web-browser one can perform the following operations: Setup the cellranger parameters Set the location of the FASTQ files Choose the transcriptome Specify the \"workdir\", where the job will keep the results Once the job parameters are set, user can click LAUNCH and the job is submitted to the Cloud Pipeline backend via the HTTP/REST API Application will poll the Cloud Pipeline API until the job is finished Once done, application will load the cellranger's data processing summary from the job results folder and display it in the web-browser","title":"Application description"},{"location":"installation/prerequisites/aws/","text":"AWS Note : Account specific information shall be updated in the JSONs below: Account name: account-name Account ID: account-id Region: region-id VPC A new VPC in the region-id region with the default configuration Subnets A routable subnet with /26 CIDR or a subnet with the Elastic IPs allowed. It will be used to deploy user-facing services Non routable subnets with any CIDR range (e.g. /16) in each of the available Availability Zones. These subnets will be used to launch the worker nodes. E.g. for us-east-1 region, the following subnets/CIDRs can be created: us-east-1a: 10.0.0.0/23 us-east-1b: 10.0.2.0/23 us-east-1c: 10.0.4.0/23 us-east-1d: 10.0.6.0/23 us-east-1e: 10.0.8.0/23 us-east-1f: 10.0.10.0/23 Security Groups CP-Cluster-Internal: Traffic type: ALL Ports: ALL Inbound: from Outbound: to CP-HTTPS-Access: Traffic type: HTTPS Port: 443 Inbound: from Internal networks or 0.0.0.0 (for the Elastic IPs usage) CP-Internet-Access: Traffic type: Any Port: 3128 Outbound: to Egress HTTP proxy, if applicable AMI The following AMIs shall be white-listed for the AWS Account: CPU-only: ami-0383ffd4e3eea1784 GPU/CUDA: ami-0b1141f33ec5cf631 VPC S3 Endpoint A new VPC endpoint shall be created for the S3 service. No specific configuration is needed SSH Key A new SSH key named CP-SSH-Key IAM Policies Name: CP-Service-Policy { Version : 2012-10-17 , Statement : [ { Sid : S3Allow , Effect : Allow , Action : [ s3:GetLifecycleConfiguration , s3:GetBucketTagging , s3:DeleteObjectVersion , s3:GetObjectVersionTagging , s3:ListBucketVersions , s3:RestoreObject , s3:CreateBucket , s3:ListBucket , s3:GetBucketPolicy , s3:GetObjectAcl , s3:AbortMultipartUpload , s3:PutBucketTagging , s3:PutLifecycleConfiguration , s3:PutBucketAcl , s3:GetObjectTagging , s3:PutObjectTagging , s3:*Delete* , s3:PutBucketVersioning , s3:PutObjectAcl , s3:DeleteObjectTagging , s3:ListBucketMultipartUploads , s3:PutObjectVersionTagging , s3:DeleteObjectVersionTagging , s3:GetBucketVersioning , s3:PutBucketCORS , s3:GetBucketAcl , s3:ListMultipartUploadParts , s3:PutObject , s3:GetObject , s3:GetBucketCORS , s3:PutBucketPolicy , s3:GetBucketLocation , s3:GetObjectVersion , s3:PutEncryptionConfiguration , s3:GetEncryptionConfiguration , s3:ListAllMyBuckets ], Resource : [ arn:aws:s3:::* ] }, { Sid : EFSAllow , Effect : Allow , Action : [ elasticfilesystem:* ], Resource : [ arn:aws:elasticfilesystem: region-id :*:file-system/* ] }, { Sid : OtherAllow , Effect : Allow , Action : [ ec2:AttachVolume , ec2:DeregisterImage , kms:Decrypt , ec2:DeleteSnapshot , ec2:RequestSpotInstances , ec2:DeleteTags , elasticfilesystem:CreateFileSystem , ec2:CancelSpotFleetRequests , ec2:CreateKeyPair , ec2:CreateImage , ec2:RequestSpotFleet , ec2:DeleteVolume , ec2:DescribeNetworkInterfaces , ec2:StartInstances , kms:Encrypt , ec2:CreateSnapshot , kms:ReEncryptTo , kms:DescribeKey , ec2:ModifyInstanceAttribute , ec2:DescribeInstanceStatus , ec2:DetachVolume , ec2:ReleaseAddress , ec2:RebootInstances , iam:GetRole , ec2:TerminateInstances , ec2:CreateTags , ec2:RegisterImage , iam:ListRoles , ec2:RunInstances , ec2:StopInstances , kms:ReEncryptFrom , ec2:CreateVolume , ec2:CreateNetworkInterface , ec2:CancelSpotInstanceRequests , ec2:Describe* , kms:GenerateDataKey , ec2:DescribeSubnets , ec2:DeleteKeyPair , cloudwatch:GetMetricStatistics , cloudwatch:ListMetrics , cloudwatch:PutMetricData , cloudwatch:GetMetricData ], Resource : * }, { Sid : STSAllow , Effect : Allow , Action : [ sts:AssumeRole ], Resource : [ arn:aws:iam:: account-id :role/CP-S3viaSTS ] } ] } Name: CP-KMS-Assume-Policy { Version : 2012-10-17 , Statement : [ { Sid : KMSAllowOps , Effect : Allow , Action : [ kms:Decrypt , kms:Encrypt , kms:DescribeKey , kms:ReEncrypt* , kms:GenerateDataKey* kms:ListKeys ], Resource : * } ] } Name: CP-S3viaSTS-Policy { Version : 2012-10-17 , Statement : [ { Effect : Allow , Action : [ s3:ListBucket , s3:GetObject , s3:PutObject , s3:DeleteObject , s3:GetObjectTagging , s3:PutObjectTagging , s3:DeleteObjectTagging , s3:GetObjectVersionTagging , s3:PutObjectVersionTagging , s3:DeleteObjectVersionTagging , s3:GetObjectVersion , s3:DeleteObjectVersion , s3:ListBucketVersions , s3:GetBucketTagging , s3:PutBucketTagging ], Resource : [ arn:aws:s3:::* ] } ] } Roles AWSServiceRoleForEC2Spot : policies according to the AWS Documentation Manually create the AWSServiceRoleForEC2Spot service-linked role CP-Service : CP-Service-Policy and CP-KMS-Assume-Policy CP-S3viaSTS : Policies: CP-S3viaSTS-Policy and CP-KMS-Assume-Policy Trust relationship: { Version : 2012-10-17 , Statement : [ { Effect : Allow , Principal : { AWS : arn:aws:iam:: account-id :role/CP-Service }, Action : sts:AssumeRole } ] } KMS The following AWS KMS key shall be created: * Region: region-id * Name: CP-KMS- region-id * Description: Cloud Pipeline KMS encryption key * Key Material: AWS_KMS The following policy shall be attached to the key: { Id : CP-KMS-Key-Policy , Version : 2012-10-17 , Statement : [ { Sid : Enable IAM User Permissions , Effect : Allow , Principal : { AWS : arn:aws:iam:: account-id :root }, Action : kms:* , Resource : * }, { Sid : Allow access for Key Administrators , Effect : Allow , Principal : { AWS : [ arn:aws:iam:: account-id :role/CP-Service , arn:aws:iam:: account-id :role/aws-service-role/spot.amazonaws.com/AWSServiceRoleForEC2Spot ] }, Action : [ kms:Create* , kms:Describe* , kms:Enable* , kms:List* , kms:Put* , kms:Update* , kms:Revoke* , kms:Disable* , kms:Get* , kms:Delete* , kms:TagResource , kms:UntagResource , kms:ScheduleKeyDeletion , kms:CancelKeyDeletion ], Resource : * }, { Sid : Allow use of the key , Effect : Allow , Principal : { AWS : [ arn:aws:iam:: account-id :role/CP-Service , arn:aws:iam:: account-id :role/aws-service-role/spot.amazonaws.com/AWSServiceRoleForEC2Spot ] }, Action : [ kms:Encrypt , kms:Decrypt , kms:ReEncrypt* , kms:GenerateDataKey* , kms:DescribeKey ], Resource : * }, { Sid : Allow attachment of persistent resources , Effect : Allow , Principal : { AWS : [ arn:aws:iam:: account-id :role/CP-Service , arn:aws:iam:: account-id :role/aws-service-role/spot.amazonaws.com/AWSServiceRoleForEC2Spot ] }, Action : [ kms:CreateGrant , kms:ListGrants , kms:RevokeGrant ], Resource : * , Condition : { Bool : { kms:GrantIsForAWSResource : true } } } ] }","title":"AWS"},{"location":"installation/prerequisites/aws/#aws","text":"Note : Account specific information shall be updated in the JSONs below: Account name: account-name Account ID: account-id Region: region-id","title":"AWS"},{"location":"installation/prerequisites/aws/#vpc","text":"A new VPC in the region-id region with the default configuration","title":"VPC"},{"location":"installation/prerequisites/aws/#subnets","text":"A routable subnet with /26 CIDR or a subnet with the Elastic IPs allowed. It will be used to deploy user-facing services Non routable subnets with any CIDR range (e.g. /16) in each of the available Availability Zones. These subnets will be used to launch the worker nodes. E.g. for us-east-1 region, the following subnets/CIDRs can be created: us-east-1a: 10.0.0.0/23 us-east-1b: 10.0.2.0/23 us-east-1c: 10.0.4.0/23 us-east-1d: 10.0.6.0/23 us-east-1e: 10.0.8.0/23 us-east-1f: 10.0.10.0/23","title":"Subnets"},{"location":"installation/prerequisites/aws/#security-groups","text":"CP-Cluster-Internal: Traffic type: ALL Ports: ALL Inbound: from Outbound: to CP-HTTPS-Access: Traffic type: HTTPS Port: 443 Inbound: from Internal networks or 0.0.0.0 (for the Elastic IPs usage) CP-Internet-Access: Traffic type: Any Port: 3128 Outbound: to Egress HTTP proxy, if applicable","title":"Security Groups"},{"location":"installation/prerequisites/aws/#ami","text":"The following AMIs shall be white-listed for the AWS Account: CPU-only: ami-0383ffd4e3eea1784 GPU/CUDA: ami-0b1141f33ec5cf631","title":"AMI"},{"location":"installation/prerequisites/aws/#vpc-s3-endpoint","text":"A new VPC endpoint shall be created for the S3 service. No specific configuration is needed","title":"VPC S3 Endpoint"},{"location":"installation/prerequisites/aws/#ssh-key","text":"A new SSH key named CP-SSH-Key","title":"SSH Key"},{"location":"installation/prerequisites/aws/#iam","text":"","title":"IAM"},{"location":"installation/prerequisites/aws/#policies","text":"Name: CP-Service-Policy { Version : 2012-10-17 , Statement : [ { Sid : S3Allow , Effect : Allow , Action : [ s3:GetLifecycleConfiguration , s3:GetBucketTagging , s3:DeleteObjectVersion , s3:GetObjectVersionTagging , s3:ListBucketVersions , s3:RestoreObject , s3:CreateBucket , s3:ListBucket , s3:GetBucketPolicy , s3:GetObjectAcl , s3:AbortMultipartUpload , s3:PutBucketTagging , s3:PutLifecycleConfiguration , s3:PutBucketAcl , s3:GetObjectTagging , s3:PutObjectTagging , s3:*Delete* , s3:PutBucketVersioning , s3:PutObjectAcl , s3:DeleteObjectTagging , s3:ListBucketMultipartUploads , s3:PutObjectVersionTagging , s3:DeleteObjectVersionTagging , s3:GetBucketVersioning , s3:PutBucketCORS , s3:GetBucketAcl , s3:ListMultipartUploadParts , s3:PutObject , s3:GetObject , s3:GetBucketCORS , s3:PutBucketPolicy , s3:GetBucketLocation , s3:GetObjectVersion , s3:PutEncryptionConfiguration , s3:GetEncryptionConfiguration , s3:ListAllMyBuckets ], Resource : [ arn:aws:s3:::* ] }, { Sid : EFSAllow , Effect : Allow , Action : [ elasticfilesystem:* ], Resource : [ arn:aws:elasticfilesystem: region-id :*:file-system/* ] }, { Sid : OtherAllow , Effect : Allow , Action : [ ec2:AttachVolume , ec2:DeregisterImage , kms:Decrypt , ec2:DeleteSnapshot , ec2:RequestSpotInstances , ec2:DeleteTags , elasticfilesystem:CreateFileSystem , ec2:CancelSpotFleetRequests , ec2:CreateKeyPair , ec2:CreateImage , ec2:RequestSpotFleet , ec2:DeleteVolume , ec2:DescribeNetworkInterfaces , ec2:StartInstances , kms:Encrypt , ec2:CreateSnapshot , kms:ReEncryptTo , kms:DescribeKey , ec2:ModifyInstanceAttribute , ec2:DescribeInstanceStatus , ec2:DetachVolume , ec2:ReleaseAddress , ec2:RebootInstances , iam:GetRole , ec2:TerminateInstances , ec2:CreateTags , ec2:RegisterImage , iam:ListRoles , ec2:RunInstances , ec2:StopInstances , kms:ReEncryptFrom , ec2:CreateVolume , ec2:CreateNetworkInterface , ec2:CancelSpotInstanceRequests , ec2:Describe* , kms:GenerateDataKey , ec2:DescribeSubnets , ec2:DeleteKeyPair , cloudwatch:GetMetricStatistics , cloudwatch:ListMetrics , cloudwatch:PutMetricData , cloudwatch:GetMetricData ], Resource : * }, { Sid : STSAllow , Effect : Allow , Action : [ sts:AssumeRole ], Resource : [ arn:aws:iam:: account-id :role/CP-S3viaSTS ] } ] } Name: CP-KMS-Assume-Policy { Version : 2012-10-17 , Statement : [ { Sid : KMSAllowOps , Effect : Allow , Action : [ kms:Decrypt , kms:Encrypt , kms:DescribeKey , kms:ReEncrypt* , kms:GenerateDataKey* kms:ListKeys ], Resource : * } ] } Name: CP-S3viaSTS-Policy { Version : 2012-10-17 , Statement : [ { Effect : Allow , Action : [ s3:ListBucket , s3:GetObject , s3:PutObject , s3:DeleteObject , s3:GetObjectTagging , s3:PutObjectTagging , s3:DeleteObjectTagging , s3:GetObjectVersionTagging , s3:PutObjectVersionTagging , s3:DeleteObjectVersionTagging , s3:GetObjectVersion , s3:DeleteObjectVersion , s3:ListBucketVersions , s3:GetBucketTagging , s3:PutBucketTagging ], Resource : [ arn:aws:s3:::* ] } ] }","title":"Policies"},{"location":"installation/prerequisites/aws/#roles","text":"AWSServiceRoleForEC2Spot : policies according to the AWS Documentation Manually create the AWSServiceRoleForEC2Spot service-linked role CP-Service : CP-Service-Policy and CP-KMS-Assume-Policy CP-S3viaSTS : Policies: CP-S3viaSTS-Policy and CP-KMS-Assume-Policy Trust relationship: { Version : 2012-10-17 , Statement : [ { Effect : Allow , Principal : { AWS : arn:aws:iam:: account-id :role/CP-Service }, Action : sts:AssumeRole } ] }","title":"Roles"},{"location":"installation/prerequisites/aws/#kms","text":"The following AWS KMS key shall be created: * Region: region-id * Name: CP-KMS- region-id * Description: Cloud Pipeline KMS encryption key * Key Material: AWS_KMS The following policy shall be attached to the key: { Id : CP-KMS-Key-Policy , Version : 2012-10-17 , Statement : [ { Sid : Enable IAM User Permissions , Effect : Allow , Principal : { AWS : arn:aws:iam:: account-id :root }, Action : kms:* , Resource : * }, { Sid : Allow access for Key Administrators , Effect : Allow , Principal : { AWS : [ arn:aws:iam:: account-id :role/CP-Service , arn:aws:iam:: account-id :role/aws-service-role/spot.amazonaws.com/AWSServiceRoleForEC2Spot ] }, Action : [ kms:Create* , kms:Describe* , kms:Enable* , kms:List* , kms:Put* , kms:Update* , kms:Revoke* , kms:Disable* , kms:Get* , kms:Delete* , kms:TagResource , kms:UntagResource , kms:ScheduleKeyDeletion , kms:CancelKeyDeletion ], Resource : * }, { Sid : Allow use of the key , Effect : Allow , Principal : { AWS : [ arn:aws:iam:: account-id :role/CP-Service , arn:aws:iam:: account-id :role/aws-service-role/spot.amazonaws.com/AWSServiceRoleForEC2Spot ] }, Action : [ kms:Encrypt , kms:Decrypt , kms:ReEncrypt* , kms:GenerateDataKey* , kms:DescribeKey ], Resource : * }, { Sid : Allow attachment of persistent resources , Effect : Allow , Principal : { AWS : [ arn:aws:iam:: account-id :role/CP-Service , arn:aws:iam:: account-id :role/aws-service-role/spot.amazonaws.com/AWSServiceRoleForEC2Spot ] }, Action : [ kms:CreateGrant , kms:ListGrants , kms:RevokeGrant ], Resource : * , Condition : { Bool : { kms:GrantIsForAWSResource : true } } } ] }","title":"KMS"},{"location":"installation/prerequisites/azure/","text":"","title":"Azure"},{"location":"installation/prerequisites/common/","text":"Cloud Pipeline installation prerequisites Cloud Provider specific configuration A number of the resources shall be created/configured in the underlying Cloud Provider. Please refer to the corresponding section for the details: AWS Azure GCP Domain names The following DNS entries are considered to access the Cloud Pipeline GUI and route the requests to the other services. Cloud Pipeline GUI/API Record type: A Record value: Cloud Pipeline Core Instance IP Example: cloud-pipeline.epam.com Embedded git access for the \"well-established\" pipelines Record type: CNAME Record value: Cloud Pipeline GUI/API Example: git.cloud-pipeline.epam.com Access to the docker registry for pull/push operation Record type: CNAME Record value: Cloud Pipeline GUI/API Example: docker.cloud-pipeline.epam.com Access to the \"interactive\" services, controlled by the Cloud Pipeline Record type: CNAME Record value: Cloud Pipeline GUI/API Example: edge.cloud-pipeline.epam.com SSL/TLS certificates SSL/TLS certificate shall be issued using an available CA (shall be trusted by the users' workstations) or purchased from the external CA. SAML/SSO configuration The following IdP configuration is required: Cloud Pipeline GUI Service Provider URL: https:// Cloud Pipeline GUI/API /pipeline Service Provider ACS URL: https:// Cloud Pipeline GUI/API /pipeline/saml SAML Binding: HTTP Redirect Assertion information: NameID Email FirstName LastName Cloud Pipeline Git Service Provider URL: https:// Embedded git Service Provider ACS URL: https:// Embedded git /users/auth/saml/callback SAML Binding: HTTP POST Assertion information: NameID Email FirstName LastName","title":"General requirements"},{"location":"installation/prerequisites/common/#cloud-pipeline-installation-prerequisites","text":"","title":"Cloud Pipeline installation prerequisites"},{"location":"installation/prerequisites/common/#cloud-provider-specific-configuration","text":"A number of the resources shall be created/configured in the underlying Cloud Provider. Please refer to the corresponding section for the details: AWS Azure GCP","title":"Cloud Provider specific configuration"},{"location":"installation/prerequisites/common/#domain-names","text":"The following DNS entries are considered to access the Cloud Pipeline GUI and route the requests to the other services. Cloud Pipeline GUI/API Record type: A Record value: Cloud Pipeline Core Instance IP Example: cloud-pipeline.epam.com Embedded git access for the \"well-established\" pipelines Record type: CNAME Record value: Cloud Pipeline GUI/API Example: git.cloud-pipeline.epam.com Access to the docker registry for pull/push operation Record type: CNAME Record value: Cloud Pipeline GUI/API Example: docker.cloud-pipeline.epam.com Access to the \"interactive\" services, controlled by the Cloud Pipeline Record type: CNAME Record value: Cloud Pipeline GUI/API Example: edge.cloud-pipeline.epam.com","title":"Domain names"},{"location":"installation/prerequisites/common/#ssltls-certificates","text":"SSL/TLS certificate shall be issued using an available CA (shall be trusted by the users' workstations) or purchased from the external CA.","title":"SSL/TLS certificates"},{"location":"installation/prerequisites/common/#samlsso-configuration","text":"The following IdP configuration is required: Cloud Pipeline GUI Service Provider URL: https:// Cloud Pipeline GUI/API /pipeline Service Provider ACS URL: https:// Cloud Pipeline GUI/API /pipeline/saml SAML Binding: HTTP Redirect Assertion information: NameID Email FirstName LastName Cloud Pipeline Git Service Provider URL: https:// Embedded git Service Provider ACS URL: https:// Embedded git /users/auth/saml/callback SAML Binding: HTTP POST Assertion information: NameID Email FirstName LastName","title":"SAML/SSO configuration"},{"location":"installation/prerequisites/gcp/","text":"","title":"GCP"},{"location":"manual/Cloud_Pipeline_-_Manual/","text":"Cloud Pipeline - Manual 1. Quick start 2. Getting started 3. Overview 4. Manage Folder 4.1. Create an object in Folder 4.2. Rename folder 4.3. Delete Folder 4.4. Clone a folder 4.5. Lock a folder 5. Manage Metadata 5.1. Add/Delete metadata items 5.2. Upload metadata 5.3. Customize view of the entity instance table 5.4. Launch a run configuration on metadata 5.5. Download data from external resources to the cloud data storage 6. Manage Pipeline 6.1. Create and configure pipeline 6.1.1. Building WDL pipeline with graphical PipelineBuilder 6.2. Launch a pipeline 6.3. Delete and unregister pipeline 7. Manage Detached configuration 7.1. Create and customize Detached configuration 7.2. Launch Detached Configuration 7.3. Expansion Expressions 7.4. Remove Detached configuration 8. Manage Data Storage 8.1. Create and edit storage 8.2. Upload/Download data 8.3. Create and Edit text files 8.4. Control File versions 8.5. Delete and unregister Data Storage 8.6. Delete Files and Folders from Storage 8.7. Create shared file system 8.8. Data sharing 8.9. Mapping storages 9. Manage Cluster nodes 10. Manage Tools 10.1. Add/Edit a Docker registry 10.2. Add/Edit a Tool group 10.3. Add a Tool 10.4. Edit a Tool 10.5. Launch a Tool 10.6. Tool security check 10.7. Tool version menu 10.8. \"Symlinked\" tools 11. Manage Runs 11.1. Manage runs lifecycles 11.2. Auto-commit Docker image 11.3. Sharing with other users or groups of users 11.4. Automatic labels and actions for the runs 12. Manage Settings 12.1. Add a new system event 12.2. Edit a system event 12.3. Create a new user 12.4. Edit/delete a user 12.5. Create a group 12.6. Edit a group/role 12.7. Delete a group 12.8. Change a set of roles/groups for a user 12.9. Change email notification 12.10. Manage system-level settings 12.11. Advanced features 12.12. System logs 13. Permissions 14. Command-line interface (CLI) 14.1. Install and setup CLI 14.2. View and manage Attributes via CLI 14.3. Manage Storage via CLI 14.4. View pipeline definitions via CLI 14.5. Manage pipeline executions via CLI 14.6. View cluster nodes via CLI 14.7. View and manage Permissions via CLI 14.8. View tools definitions via CLI 15. Interactive services 15.1. Starting an Interactive application 15.2. Using Terminal access 15.3. Expose node filesystem 15.4. Interactive service examples 16. Issues 17. CP objects tagging by additional attributes 18. Home page Appendix A. Instance and Docker container lifecycles Appendix B. Working with a Project Appendix C. Working with autoscaled cluster runs Appendix D. Costs management Appendix E. Pipeline objects concept Appendix F. \u0421omparison of using different FS storage types 1. Quick start This chapter will give you a basic knowledge of pipeline running procedure. 2. Getting started Find Thesaurus, list of supported browsers and authentication details. 3. Overview Get familiar with a Cloud Pipeline Graphical User Interface (GUI). 4. Manage Folder Learn how you can create your own hierarchical structured space. 5. Manage Metadata Learn how you can create a complex analysis environment using custom data entities. 6. Manage Pipeline Get details on pipeline creation, configuration, and launching. Get descriptions of \"Launch pipeline\" page. There is also a description of graphical \"Pipeline builder\" for WDL pipelines. 7. Manage Detached configuration Learn how to run a cluster of differently configurated instances. \"Launch cluster\" box vs LaunchofClusterconfiguration. 8. Manage Data Storage Create a new storage and learn how to manage it: download and upload, copy, move data, delete and unregister storages. 9. Manage Cluster Nodes Learn about active nodes, how to terminate and refresh them. Get familiar with a related dashboard. 10. Manage Tools Add and edit Docker registry. Learn how to add, modify and launch Tools. 11. Manage Runs Learn about Runs space and what information you can get from it. 12. Manage Settings Here you can find information about: how to install CLI, add a new system event, managing roles. 13. Permissions Learn how to manage permissions for Cloud Pipeline objects. 14. Command-line interface (CLI) Get familiar with a Cloud Pipeline Command Line Interface (CLI). 15. Interactive services Learn how to setup non-batch application in a cloud infrastructure and access it via a web interface. 16. Issues Learn how to share results with other users or get feedback. 17. CP objects tagging by additional attributes Learn how to manage custom sets of \"key-values\" attributes for data storage and files. 18. Home page Get details about homepage widgets, how to configure homepage view. Appendix A. Instance and Docker container lifecycles Learn basics about instance and Docker container lifecycle. Appendix B. Working with a Project Learn basics about working with a Project. Appendix C. Working with autoscaled cluster runs Learn basics about working with autoscaled cluster runs. Appendix D. Costs management Get details about costs management concept. Appendix E. Pipeline objects concept Get details about the Pipeline objects concept in Cloud Pipeline environment. Appendix F. \u0421omparison of using different FS storage types Get details about the comparison of using different FS storage types ( FSx for Lustre / EFS in AWS / BTRFS on EBS / LizardFS on EBS ) in Cloud Pipeline environment.","title":"Contents"},{"location":"manual/Cloud_Pipeline_-_Manual/#cloud-pipeline-manual","text":"1. Quick start 2. Getting started 3. Overview 4. Manage Folder 4.1. Create an object in Folder 4.2. Rename folder 4.3. Delete Folder 4.4. Clone a folder 4.5. Lock a folder 5. Manage Metadata 5.1. Add/Delete metadata items 5.2. Upload metadata 5.3. Customize view of the entity instance table 5.4. Launch a run configuration on metadata 5.5. Download data from external resources to the cloud data storage 6. Manage Pipeline 6.1. Create and configure pipeline 6.1.1. Building WDL pipeline with graphical PipelineBuilder 6.2. Launch a pipeline 6.3. Delete and unregister pipeline 7. Manage Detached configuration 7.1. Create and customize Detached configuration 7.2. Launch Detached Configuration 7.3. Expansion Expressions 7.4. Remove Detached configuration 8. Manage Data Storage 8.1. Create and edit storage 8.2. Upload/Download data 8.3. Create and Edit text files 8.4. Control File versions 8.5. Delete and unregister Data Storage 8.6. Delete Files and Folders from Storage 8.7. Create shared file system 8.8. Data sharing 8.9. Mapping storages 9. Manage Cluster nodes 10. Manage Tools 10.1. Add/Edit a Docker registry 10.2. Add/Edit a Tool group 10.3. Add a Tool 10.4. Edit a Tool 10.5. Launch a Tool 10.6. Tool security check 10.7. Tool version menu 10.8. \"Symlinked\" tools 11. Manage Runs 11.1. Manage runs lifecycles 11.2. Auto-commit Docker image 11.3. Sharing with other users or groups of users 11.4. Automatic labels and actions for the runs 12. Manage Settings 12.1. Add a new system event 12.2. Edit a system event 12.3. Create a new user 12.4. Edit/delete a user 12.5. Create a group 12.6. Edit a group/role 12.7. Delete a group 12.8. Change a set of roles/groups for a user 12.9. Change email notification 12.10. Manage system-level settings 12.11. Advanced features 12.12. System logs 13. Permissions 14. Command-line interface (CLI) 14.1. Install and setup CLI 14.2. View and manage Attributes via CLI 14.3. Manage Storage via CLI 14.4. View pipeline definitions via CLI 14.5. Manage pipeline executions via CLI 14.6. View cluster nodes via CLI 14.7. View and manage Permissions via CLI 14.8. View tools definitions via CLI 15. Interactive services 15.1. Starting an Interactive application 15.2. Using Terminal access 15.3. Expose node filesystem 15.4. Interactive service examples 16. Issues 17. CP objects tagging by additional attributes 18. Home page Appendix A. Instance and Docker container lifecycles Appendix B. Working with a Project Appendix C. Working with autoscaled cluster runs Appendix D. Costs management Appendix E. Pipeline objects concept Appendix F. \u0421omparison of using different FS storage types 1. Quick start This chapter will give you a basic knowledge of pipeline running procedure. 2. Getting started Find Thesaurus, list of supported browsers and authentication details. 3. Overview Get familiar with a Cloud Pipeline Graphical User Interface (GUI). 4. Manage Folder Learn how you can create your own hierarchical structured space. 5. Manage Metadata Learn how you can create a complex analysis environment using custom data entities. 6. Manage Pipeline Get details on pipeline creation, configuration, and launching. Get descriptions of \"Launch pipeline\" page. There is also a description of graphical \"Pipeline builder\" for WDL pipelines. 7. Manage Detached configuration Learn how to run a cluster of differently configurated instances. \"Launch cluster\" box vs LaunchofClusterconfiguration. 8. Manage Data Storage Create a new storage and learn how to manage it: download and upload, copy, move data, delete and unregister storages. 9. Manage Cluster Nodes Learn about active nodes, how to terminate and refresh them. Get familiar with a related dashboard. 10. Manage Tools Add and edit Docker registry. Learn how to add, modify and launch Tools. 11. Manage Runs Learn about Runs space and what information you can get from it. 12. Manage Settings Here you can find information about: how to install CLI, add a new system event, managing roles. 13. Permissions Learn how to manage permissions for Cloud Pipeline objects. 14. Command-line interface (CLI) Get familiar with a Cloud Pipeline Command Line Interface (CLI). 15. Interactive services Learn how to setup non-batch application in a cloud infrastructure and access it via a web interface. 16. Issues Learn how to share results with other users or get feedback. 17. CP objects tagging by additional attributes Learn how to manage custom sets of \"key-values\" attributes for data storage and files. 18. Home page Get details about homepage widgets, how to configure homepage view. Appendix A. Instance and Docker container lifecycles Learn basics about instance and Docker container lifecycle. Appendix B. Working with a Project Learn basics about working with a Project. Appendix C. Working with autoscaled cluster runs Learn basics about working with autoscaled cluster runs. Appendix D. Costs management Get details about costs management concept. Appendix E. Pipeline objects concept Get details about the Pipeline objects concept in Cloud Pipeline environment. Appendix F. \u0421omparison of using different FS storage types Get details about the comparison of using different FS storage types ( FSx for Lustre / EFS in AWS / BTRFS on EBS / LizardFS on EBS ) in Cloud Pipeline environment.","title":"Cloud Pipeline - Manual"},{"location":"manual/01_Quick_start/1._Quick_start/","text":"1. Quick start To launch a Pipeline you need to have EXECUTE permissions for that Pipeline . For more information see 13. Permissions . This quick start will help you to get a general idea of data processing via the Cloud Pipeline platform. Pipelines are set of data processing steps that are dependent on each other. Here we will describe an example of a pipeline launch. Note : Pipeline launch steps remain the same for different pipelines. Pipeline running procedure for basic users The first step you need to perform is to upload the data that you will work with. Generally, data in the Cloud Pipeline is kept in data storages . So, the first thing you need to do is to find a data storage in the Library tab on the left side of the screen. Then upload data to it. In this example, we will choose the data storage named \" INPUT \" ( 1 ) and then create a folder ( 2 ) named \" gromacs_benchmark \" ( 3 ). Then we will navigate to this folder by clicking on its name and click \" Upload \" button to upload your input data to the storage. Note : make sure that the size of data doesn't exceed 5 Gb. To upload more than 5 Gb you shall use CLI (see details here ). Then decide where to put results of pipeline processing. A good practice is to not mix input and output data in a one data storage, so we will use another data storage for output data. In this example, we will use data storage named \" ANALYSIS \" to store pipeline output data . In the \" Search \" field of the Library tab, find a pipeline that will process the input data . In this example, we will use the \" Gromacs \" pipeline that performs molecular modeling. Choose the pipeline version and click the Run button. On the \" Launch a pipeline \" page you'll see pipeline execution parameters . On the bottom, you'll see a \" Parameter \" section with the input parameter and output parameter. Here you can specify the paths to your folders in storages, which you've created in step 1 and step 2 . Click the controls to choose your folders. Check your folder in the list and click \"OK\" . Tip: If you want to create a new folder for the analysis, you can put /$RUN_ID at the end of the output string. The system will create a new folder with Run ID name. Click the Launch button to run the pipeline. You'll be redirected to the Runs tab of the CP . Here you can find launched \"Gromacs\" pipeline. Click a run with the pipeline name to see run logs . When all tasks of pipeline finish their execution, you'll be able to find the pipeline run in the Completed Runs section of the Runs tab. To see the output data, you shall navigate to the data storage with output data - ANALYSIS - and find the results of the pipeline run.","title":"1. Quick start"},{"location":"manual/01_Quick_start/1._Quick_start/#1-quick-start","text":"To launch a Pipeline you need to have EXECUTE permissions for that Pipeline . For more information see 13. Permissions . This quick start will help you to get a general idea of data processing via the Cloud Pipeline platform. Pipelines are set of data processing steps that are dependent on each other. Here we will describe an example of a pipeline launch. Note : Pipeline launch steps remain the same for different pipelines.","title":"1. Quick start"},{"location":"manual/01_Quick_start/1._Quick_start/#pipeline-running-procedure-for-basic-users","text":"The first step you need to perform is to upload the data that you will work with. Generally, data in the Cloud Pipeline is kept in data storages . So, the first thing you need to do is to find a data storage in the Library tab on the left side of the screen. Then upload data to it. In this example, we will choose the data storage named \" INPUT \" ( 1 ) and then create a folder ( 2 ) named \" gromacs_benchmark \" ( 3 ). Then we will navigate to this folder by clicking on its name and click \" Upload \" button to upload your input data to the storage. Note : make sure that the size of data doesn't exceed 5 Gb. To upload more than 5 Gb you shall use CLI (see details here ). Then decide where to put results of pipeline processing. A good practice is to not mix input and output data in a one data storage, so we will use another data storage for output data. In this example, we will use data storage named \" ANALYSIS \" to store pipeline output data . In the \" Search \" field of the Library tab, find a pipeline that will process the input data . In this example, we will use the \" Gromacs \" pipeline that performs molecular modeling. Choose the pipeline version and click the Run button. On the \" Launch a pipeline \" page you'll see pipeline execution parameters . On the bottom, you'll see a \" Parameter \" section with the input parameter and output parameter. Here you can specify the paths to your folders in storages, which you've created in step 1 and step 2 . Click the controls to choose your folders. Check your folder in the list and click \"OK\" . Tip: If you want to create a new folder for the analysis, you can put /$RUN_ID at the end of the output string. The system will create a new folder with Run ID name. Click the Launch button to run the pipeline. You'll be redirected to the Runs tab of the CP . Here you can find launched \"Gromacs\" pipeline. Click a run with the pipeline name to see run logs . When all tasks of pipeline finish their execution, you'll be able to find the pipeline run in the Completed Runs section of the Runs tab. To see the output data, you shall navigate to the data storage with output data - ANALYSIS - and find the results of the pipeline run.","title":"Pipeline running procedure for basic users"},{"location":"manual/02_Getting_started/2._Getting_started/","text":"2. Getting started Thesaurus Cloud Provider - supported cloud computing provider. Cloud Region - specific geographical location where compute cloud resources could be hosted. Each Cloud Provider has its own Cloud Regions. Objects - Cloud Pipeline (CP) entities such as Folder, Pipeline, Data Storage, Run configuration, Cluster node, Docker Registry, Tool group, Tool. Folder , or directory - CP object. It is an entity similar to the directories in the file system. Folders are used to structure other CP objects. Project - a special type of Folder. It might be used to organize data and metadata (see below) and simplify analysis runs for a large data set. Pipeline - CP object. Represents a workflow script with versioned source code, documentation, and configuration. Under the hood, it is a git repository. Data Storage - CP object. It is a cloud storage (e.g. S3 bucket for AWS or Blob Storage for MS Azure) representation in a folder hierarchy. FS Mount - a data storage based on network FS (File System). It is a distributed file system that can be used by several nodes during High-performance computing jobs. Docker Registry - CP object. It is a Docker registry representation in the CP. Docker registry stores docker images. For more details refer to https://docs.docker.com/registry/ . Tool - CP object. It is a Docker image representation in CP. Tool group - CP object. It allows organizing different Tools into groups. Detached configuration - Run configuration that is not attached to any particular pipeline. Run configuration - CP object specifying parameters of the run: what pipeline or tool, type of instance to run with what parameters. There are Run Configurations for Detached configuration and for Pipeline configuration. History - CP object. It shows all runs that are related to a project. Cluster configuration - CP object. It is Run Configuration of Detached Configuration. Represents a set of pipelines or tools that run as one cluster. Also is used to associate pipeline parameters with some Metadata Entity. Metadata - CP object that defines custom data entities associated with raw data files (fastq, bcl, etc.) or data parameters. Batch job - an automated job that doesn't require interaction with a human. Interactive application - type of an application that requires an interaction with a human to achieve certain results. Docker image - a stand-alone, executable package that includes everything needed to run a piece of software. Containerized software will always run the same, regardless of the environment. Docker container - a runtime instance of docker image - what the image turns into when actually executed. Run - Executed \"Pipeline\" or \"Tool\" object, contains log information and parameters of an execution process. Instance - virtual computing environment (e.g. EC2 instance for AWS or Azure Virtual Machines for MS Azure). On-Demand Instances \u2013 \"Stable\" type of instances that can't be overbought while in use. Spot Instances \u2013 instances, which has significantly lower costs. This type of instances are rented and can be overbought and shut down by cloud provider. Instances may have any type (number of CPU cores, processor type, amount of RAM and disk space, etc.) that describes optimization and available features for the instance. Each Cloud Provider calls these instances differently: spot instances for AWS, low priority instances for Azure, preemptible instances for GCP. Cluster node (Calculation node) - instance used for a \"Run\" object. Task - a discrete Pipeline step. Attributes - data that provides information about other data. CP allows creating a custom metadata set of \"key=values\" (attributes) for its objects. It helps to maintain traceability and use this information for search queries. Cloud Pipeline CLI - command line interface to the CP. It allows interaction with CP via command line instead of GUI. STS (Short-Term Storage) - data storage that is used for frequently accessed files. LTS (Long-Term Storage) - data storage that is used for infrequently accessed files. Access to data becomes more complicated but it's cheaper. Pipeline template - it is a template structure for creating a pipeline. Typically it includes source code folder structure with the executable script, doc file, and configuration file. Token - authentication key to getting access to an application. Endpoint - link to access a launched application. Pipeline version - A particular version of pipeline source code, documentation and configuration. Log - automatically produced and time-stamped documentation of events relevant to a particular system. Cluster - a collection of instances which are connected so that they can be used together on a task. Access Control List - a list of pairs of attributes - user id and permissions: read/write/execute. Supported Cloud Providers The following Cloud Providers are supported at the moment: Amazon Web Services\u200e ( AWS ) Microsoft Azure services ( MS Azure ) Google Cloud Platform ( GCP ) Supported Browsers The following web-browsers are supported at the moment: Google Chrome (best option) Firefox Microsoft EDGE Internet Explorer 11 Authentication SAML Authentication protocol is currently used in a Cloud Pipeline.","title":"2. Getting started"},{"location":"manual/02_Getting_started/2._Getting_started/#2-getting-started","text":"","title":"2. Getting started"},{"location":"manual/02_Getting_started/2._Getting_started/#thesaurus","text":"Cloud Provider - supported cloud computing provider. Cloud Region - specific geographical location where compute cloud resources could be hosted. Each Cloud Provider has its own Cloud Regions. Objects - Cloud Pipeline (CP) entities such as Folder, Pipeline, Data Storage, Run configuration, Cluster node, Docker Registry, Tool group, Tool. Folder , or directory - CP object. It is an entity similar to the directories in the file system. Folders are used to structure other CP objects. Project - a special type of Folder. It might be used to organize data and metadata (see below) and simplify analysis runs for a large data set. Pipeline - CP object. Represents a workflow script with versioned source code, documentation, and configuration. Under the hood, it is a git repository. Data Storage - CP object. It is a cloud storage (e.g. S3 bucket for AWS or Blob Storage for MS Azure) representation in a folder hierarchy. FS Mount - a data storage based on network FS (File System). It is a distributed file system that can be used by several nodes during High-performance computing jobs. Docker Registry - CP object. It is a Docker registry representation in the CP. Docker registry stores docker images. For more details refer to https://docs.docker.com/registry/ . Tool - CP object. It is a Docker image representation in CP. Tool group - CP object. It allows organizing different Tools into groups. Detached configuration - Run configuration that is not attached to any particular pipeline. Run configuration - CP object specifying parameters of the run: what pipeline or tool, type of instance to run with what parameters. There are Run Configurations for Detached configuration and for Pipeline configuration. History - CP object. It shows all runs that are related to a project. Cluster configuration - CP object. It is Run Configuration of Detached Configuration. Represents a set of pipelines or tools that run as one cluster. Also is used to associate pipeline parameters with some Metadata Entity. Metadata - CP object that defines custom data entities associated with raw data files (fastq, bcl, etc.) or data parameters. Batch job - an automated job that doesn't require interaction with a human. Interactive application - type of an application that requires an interaction with a human to achieve certain results. Docker image - a stand-alone, executable package that includes everything needed to run a piece of software. Containerized software will always run the same, regardless of the environment. Docker container - a runtime instance of docker image - what the image turns into when actually executed. Run - Executed \"Pipeline\" or \"Tool\" object, contains log information and parameters of an execution process. Instance - virtual computing environment (e.g. EC2 instance for AWS or Azure Virtual Machines for MS Azure). On-Demand Instances \u2013 \"Stable\" type of instances that can't be overbought while in use. Spot Instances \u2013 instances, which has significantly lower costs. This type of instances are rented and can be overbought and shut down by cloud provider. Instances may have any type (number of CPU cores, processor type, amount of RAM and disk space, etc.) that describes optimization and available features for the instance. Each Cloud Provider calls these instances differently: spot instances for AWS, low priority instances for Azure, preemptible instances for GCP. Cluster node (Calculation node) - instance used for a \"Run\" object. Task - a discrete Pipeline step. Attributes - data that provides information about other data. CP allows creating a custom metadata set of \"key=values\" (attributes) for its objects. It helps to maintain traceability and use this information for search queries. Cloud Pipeline CLI - command line interface to the CP. It allows interaction with CP via command line instead of GUI. STS (Short-Term Storage) - data storage that is used for frequently accessed files. LTS (Long-Term Storage) - data storage that is used for infrequently accessed files. Access to data becomes more complicated but it's cheaper. Pipeline template - it is a template structure for creating a pipeline. Typically it includes source code folder structure with the executable script, doc file, and configuration file. Token - authentication key to getting access to an application. Endpoint - link to access a launched application. Pipeline version - A particular version of pipeline source code, documentation and configuration. Log - automatically produced and time-stamped documentation of events relevant to a particular system. Cluster - a collection of instances which are connected so that they can be used together on a task. Access Control List - a list of pairs of attributes - user id and permissions: read/write/execute.","title":"Thesaurus"},{"location":"manual/02_Getting_started/2._Getting_started/#supported-cloud-providers","text":"The following Cloud Providers are supported at the moment: Amazon Web Services\u200e ( AWS ) Microsoft Azure services ( MS Azure ) Google Cloud Platform ( GCP )","title":"Supported Cloud Providers"},{"location":"manual/02_Getting_started/2._Getting_started/#supported-browsers","text":"The following web-browsers are supported at the moment: Google Chrome (best option) Firefox Microsoft EDGE Internet Explorer 11","title":"Supported Browsers"},{"location":"manual/02_Getting_started/2._Getting_started/#authentication","text":"SAML Authentication protocol is currently used in a Cloud Pipeline.","title":"Authentication"},{"location":"manual/03_Overview/3._Overview/","text":"3. Overview User Journey in a nutshell GUI menu tab bar Home Library Cluster Nodes Tools Runs Settings Search Logout User Journey in a nutshell Cloud Pipeline (CP) is a cloud-based web application which allows users to solve a wide range of analytical tasks and includes: Data processing : you can create data processing pipelines and run them in the cloud in an automated way. Data storing : create your data storage, download or upload data from it or edit text files within CP UI. File version control is supported. Tool management : you can create and deploy your own calculation environment using Docker's container concept. This Manual is mostly around data processing lifecycle which, in a nutshell, can be described in these several steps: To run a user's calculation script it shall be registered in CP as a pipeline . The script could be created in CP environment or uploaded from the local machine. See more details 6. Manage Pipeline . Note : If you need to run a pipeline in different environments simultaneously or set a specific type of data, you can use detach configuration object. See more 7. Manage Detached configuration . To store pipeline's inputs and outputs data files the Data Storage shall be determined in CP . Learn more 8. Manage Data Storage . Almost every pipeline requires a specific package of software to run it, which is defined in a docker image. So when a user starts a pipeline, CP starts a new cloud instance (nodes) and runs a docker image at it. See more details 9. Manage Cluster nodes and 10. Manage Tools . When the environment is set, pipeline starts execution. A user in CP can change and save configurations of the run. Learn more 6.2. Launch a pipeline . A user can monitor the status of active and completed runs and usage of active instances in CP . Learn more 11. Manage Runs and 9. Manage Cluster nodes . Note : CP can run a docker image at instance without any pipeline at all if needed. There just will be an instance with some installed and running software. A user can SSH to it or use it in the interactive mode. Also, CP supports CLI, which duplicates some of GUI features and has extra features unavailable via GUI, such as automation of interaction with CP during the pipeline script running, or uploading considerable amount of data (more than 5 Gb), etc. The basics of CLI you can learn here . GUI menu tab bar There are several menu tabs at the left edge of CP window. Home Home space is opened by default when Cloud Pipeline is loaded. It provides \"personal\" and often-used information for a user. By default, this dashboard shows 3 widgets: Active Runs (see picture below, 1 ) displays a list of user's active runs. See more details 11. Manage Runs . Note : press Explore all active runs to see a list of all runs. Tools (see picture below, 2 ) shows a list of Tools in a user's personal repository and Tools available to your group. Learn more 10. Manage Tools and 13. Permissions . Note : group-level Tools will be shown on the top of the Tools list. Note : user can search for a particular Tool by using the Search tools text box. System will suggest Tools based on a user's query. Data (see picture below, 3 ) displays available Data Storages for a user - user shall have OWNER rights or READ/WRITE access to a Data Storage. See more 8. Manage Data Storage and 13. Permissions . Note : personal Data storages (i.e. a user is an OWNER of this Storage) will be shown on top, WRITE - second priority, READ - third priority. Each widget has the Help icon . Hover this icon to get a brief description of a widget. Learn more about Home tab 18. Home page . Library Library space supports a hierarchical view of its content: Pipelines and its versions Data Storages Folders Configurations Metadata objects . The tab consists of two panels: \"Hierarchy\" view (see the picture below, 2 ) displays a hierarchical-structured list of pipelines, folders, storages and machine configurations, etc. Use the \" Search \" field to find a CP object by a name. \" Collapse/Expand \" button (see the picture below, 1 ) at the bottom-left corner of the screen: use it to collapse or expand \"Hierarchy\" view. \"Details\" view (see the picture below, 3 ) shows details of a selected item in hierarchy panel. Depends on a selected type of object it has a very different view. You can learn about each on respective pages of the manual. Note : Each time \"Hierarchy\" view loads, the first Folder in the hierarchy that has more than 1 child object (Folder, Pipeline, Data Storage, etc) is automatically selected. Contents of that Folder are automatically expanded. Cluster Nodes This space provides a list of working nodes. You can get information on their usage and terminate them in this tab. See more details 9. Manage Cluster nodes . Tools Tools space displays available Docker images and their tools, organized in tool groups and docker registries. You can edit information about them and run nodes with any Docker image in this tab. See more details 10. Manage Tools . Runs This space helps you monitor the state of your run instances. You can get parameters and logs of a specific run, stop a run, rerun a completed run. Learn more 11. Manage Runs . Settings This tab opens a Settings window which allows: generate a CLI installation and configuration commands to set CLI for CP , manage system events notifications, manage roles and permissions. See more details 12. Manage Settings . Search It's a search field which allows searching specific runs by its \"Run ID\" or \"Parameters\". Logout This is a Logout button which logs you out. Note : if automatic logging-in is configured, you will be re-logged at once. Note : if any changes occur in the CP application during an authorized session, the changes are applied after re-login.","title":"3. Overview"},{"location":"manual/03_Overview/3._Overview/#3-overview","text":"User Journey in a nutshell GUI menu tab bar Home Library Cluster Nodes Tools Runs Settings Search Logout","title":"3. Overview"},{"location":"manual/03_Overview/3._Overview/#user-journey-in-a-nutshell","text":"Cloud Pipeline (CP) is a cloud-based web application which allows users to solve a wide range of analytical tasks and includes: Data processing : you can create data processing pipelines and run them in the cloud in an automated way. Data storing : create your data storage, download or upload data from it or edit text files within CP UI. File version control is supported. Tool management : you can create and deploy your own calculation environment using Docker's container concept. This Manual is mostly around data processing lifecycle which, in a nutshell, can be described in these several steps: To run a user's calculation script it shall be registered in CP as a pipeline . The script could be created in CP environment or uploaded from the local machine. See more details 6. Manage Pipeline . Note : If you need to run a pipeline in different environments simultaneously or set a specific type of data, you can use detach configuration object. See more 7. Manage Detached configuration . To store pipeline's inputs and outputs data files the Data Storage shall be determined in CP . Learn more 8. Manage Data Storage . Almost every pipeline requires a specific package of software to run it, which is defined in a docker image. So when a user starts a pipeline, CP starts a new cloud instance (nodes) and runs a docker image at it. See more details 9. Manage Cluster nodes and 10. Manage Tools . When the environment is set, pipeline starts execution. A user in CP can change and save configurations of the run. Learn more 6.2. Launch a pipeline . A user can monitor the status of active and completed runs and usage of active instances in CP . Learn more 11. Manage Runs and 9. Manage Cluster nodes . Note : CP can run a docker image at instance without any pipeline at all if needed. There just will be an instance with some installed and running software. A user can SSH to it or use it in the interactive mode. Also, CP supports CLI, which duplicates some of GUI features and has extra features unavailable via GUI, such as automation of interaction with CP during the pipeline script running, or uploading considerable amount of data (more than 5 Gb), etc. The basics of CLI you can learn here .","title":"User Journey in a nutshell"},{"location":"manual/03_Overview/3._Overview/#gui-menu-tab-bar","text":"There are several menu tabs at the left edge of CP window.","title":"GUI menu tab bar"},{"location":"manual/03_Overview/3._Overview/#home","text":"Home space is opened by default when Cloud Pipeline is loaded. It provides \"personal\" and often-used information for a user. By default, this dashboard shows 3 widgets: Active Runs (see picture below, 1 ) displays a list of user's active runs. See more details 11. Manage Runs . Note : press Explore all active runs to see a list of all runs. Tools (see picture below, 2 ) shows a list of Tools in a user's personal repository and Tools available to your group. Learn more 10. Manage Tools and 13. Permissions . Note : group-level Tools will be shown on the top of the Tools list. Note : user can search for a particular Tool by using the Search tools text box. System will suggest Tools based on a user's query. Data (see picture below, 3 ) displays available Data Storages for a user - user shall have OWNER rights or READ/WRITE access to a Data Storage. See more 8. Manage Data Storage and 13. Permissions . Note : personal Data storages (i.e. a user is an OWNER of this Storage) will be shown on top, WRITE - second priority, READ - third priority. Each widget has the Help icon . Hover this icon to get a brief description of a widget. Learn more about Home tab 18. Home page .","title":"Home"},{"location":"manual/03_Overview/3._Overview/#library","text":"Library space supports a hierarchical view of its content: Pipelines and its versions Data Storages Folders Configurations Metadata objects . The tab consists of two panels: \"Hierarchy\" view (see the picture below, 2 ) displays a hierarchical-structured list of pipelines, folders, storages and machine configurations, etc. Use the \" Search \" field to find a CP object by a name. \" Collapse/Expand \" button (see the picture below, 1 ) at the bottom-left corner of the screen: use it to collapse or expand \"Hierarchy\" view. \"Details\" view (see the picture below, 3 ) shows details of a selected item in hierarchy panel. Depends on a selected type of object it has a very different view. You can learn about each on respective pages of the manual. Note : Each time \"Hierarchy\" view loads, the first Folder in the hierarchy that has more than 1 child object (Folder, Pipeline, Data Storage, etc) is automatically selected. Contents of that Folder are automatically expanded.","title":"Library"},{"location":"manual/03_Overview/3._Overview/#cluster-nodes","text":"This space provides a list of working nodes. You can get information on their usage and terminate them in this tab. See more details 9. Manage Cluster nodes .","title":"Cluster Nodes"},{"location":"manual/03_Overview/3._Overview/#tools","text":"Tools space displays available Docker images and their tools, organized in tool groups and docker registries. You can edit information about them and run nodes with any Docker image in this tab. See more details 10. Manage Tools .","title":"Tools"},{"location":"manual/03_Overview/3._Overview/#runs","text":"This space helps you monitor the state of your run instances. You can get parameters and logs of a specific run, stop a run, rerun a completed run. Learn more 11. Manage Runs .","title":"Runs"},{"location":"manual/03_Overview/3._Overview/#settings","text":"This tab opens a Settings window which allows: generate a CLI installation and configuration commands to set CLI for CP , manage system events notifications, manage roles and permissions. See more details 12. Manage Settings .","title":"Settings"},{"location":"manual/03_Overview/3._Overview/#search","text":"It's a search field which allows searching specific runs by its \"Run ID\" or \"Parameters\".","title":"Search"},{"location":"manual/03_Overview/3._Overview/#logout","text":"This is a Logout button which logs you out. Note : if automatic logging-in is configured, you will be re-logged at once. Note : if any changes occur in the CP application during an authorized session, the changes are applied after re-login.","title":"Logout"},{"location":"manual/04_Manage_Folder/4.1._Create_an_object_in_Folder/","text":"4.1. Create an object in Folder How to create Example: Create Folder To create new object in the Folder you need WRITE permissions for that folder and an appropriate ROLE, e.g. to create new folder you need to have the ROLE_FOLDER_MANAGER role. For more information see 13. Permissions . You can create Pipelines , Storages , Detached configurations and Folders in a Folder. How to create Navigate to the Folder . Click \" Create \". Choose a type of the object to be created from a drop-down list. Set properties of the object (such as a name for the object). See Create pipeline , Create storage , Create Detached Configuration pages. Click OK . Example: Create Folder Navigate to a folder. Click Create . Choose Folder from the drop-down list. Enter the name of the folder. Note : Folder name must be unique in its hierarchical layer. Click OK .","title":"4.1. Create an object in Folder"},{"location":"manual/04_Manage_Folder/4.1._Create_an_object_in_Folder/#41-create-an-object-in-folder","text":"How to create Example: Create Folder To create new object in the Folder you need WRITE permissions for that folder and an appropriate ROLE, e.g. to create new folder you need to have the ROLE_FOLDER_MANAGER role. For more information see 13. Permissions . You can create Pipelines , Storages , Detached configurations and Folders in a Folder.","title":"4.1. Create an object in Folder"},{"location":"manual/04_Manage_Folder/4.1._Create_an_object_in_Folder/#how-to-create","text":"Navigate to the Folder . Click \" Create \". Choose a type of the object to be created from a drop-down list. Set properties of the object (such as a name for the object). See Create pipeline , Create storage , Create Detached Configuration pages. Click OK .","title":"How to create"},{"location":"manual/04_Manage_Folder/4.1._Create_an_object_in_Folder/#example-create-folder","text":"Navigate to a folder. Click Create . Choose Folder from the drop-down list. Enter the name of the folder. Note : Folder name must be unique in its hierarchical layer. Click OK .","title":"Example: Create Folder"},{"location":"manual/04_Manage_Folder/4.2._Rename_folder/","text":"4.2. Rename folder To edit name of a Folder you need WRITE permission for that folder. For more information see 13. Permissions . You can rename a Folder . To do that: Navigate to a Folder . Click icon and choose Edit folder - the settings window will open. Enter the new name. Note : Folder name must be unique in its hierarchical layer. Click OK . Note : another way to rename folder is: Hover over the title of the folder - the editing icon will appear. Click on the highlight field - it's switched to edit-mode. Type a name. Click out of the active field - the name will be saved.","title":"4.2. Rename Folder"},{"location":"manual/04_Manage_Folder/4.2._Rename_folder/#42-rename-folder","text":"To edit name of a Folder you need WRITE permission for that folder. For more information see 13. Permissions . You can rename a Folder . To do that: Navigate to a Folder . Click icon and choose Edit folder - the settings window will open. Enter the new name. Note : Folder name must be unique in its hierarchical layer. Click OK . Note : another way to rename folder is: Hover over the title of the folder - the editing icon will appear. Click on the highlight field - it's switched to edit-mode. Type a name. Click out of the active field - the name will be saved.","title":"4.2. Rename folder"},{"location":"manual/04_Manage_Folder/4.3._Delete_Folder/","text":"4.3. Delete Folder To delete a Folder you need WRITE permissions for that folder and the ROLE_FOLDER_MANAGER role. For more information see 13. Permissions . Note : If the folder is not empty it will not be deleted. If a folder contains only metadata, the folder will be deleted. Navigate to the folder contains the folder to delete. Click the icon in the line with desired object to delete. To delete current (selected folder): Click button. Click Delete . Click OK .","title":"4.3. Delete Folder"},{"location":"manual/04_Manage_Folder/4.3._Delete_Folder/#43-delete-folder","text":"To delete a Folder you need WRITE permissions for that folder and the ROLE_FOLDER_MANAGER role. For more information see 13. Permissions . Note : If the folder is not empty it will not be deleted. If a folder contains only metadata, the folder will be deleted. Navigate to the folder contains the folder to delete. Click the icon in the line with desired object to delete. To delete current (selected folder): Click button. Click Delete . Click OK .","title":"4.3. Delete Folder"},{"location":"manual/04_Manage_Folder/4.4._Clone_a_folder/","text":"4.4. Clone a folder This feature allows a user to clone any folder to a specific destination: to user's personal folder, or user's project. It would be helpful to create a new project way faster due to copying metadata, configurations, storages from another project. To copy a folder, you need READ permissions for the copied folder and WRITE permissions for a folder selected as a destination. For more information see 13. Permissions . Note : learn more about metadata here . To clone a folder, the following steps shall be performed: Navigate to the desired folder page. Click icon and choose \"Clone\" menu item. The \"Select destination folder\" will pop-up. Note : The pop-up window will be open with parent folder of a copied folder as a default destination. Choose the destination if it needed by classical navigation actions. Note : The \"Clone\" button ( 1 ) displays which destination is selected at the moment. Name the new clone of the folder ( 2 ). Note : The name shall not break the uniqueness principal: there shouldn't be two folders with the same name in one destination. Click \"Clone\" button - and the folder will be cloned. The page of the clone of the folder will be open automatically. All child of the copied folder will be copied. Note : The exception is pipelines. Pipelines won't be copied as far as it may cause the collision. Note : The storages are copied by creating a new empty one with a unique name and path. No file will be copied from a copied storage to a new one.","title":"4.4. Clone a Folder"},{"location":"manual/04_Manage_Folder/4.4._Clone_a_folder/#44-clone-a-folder","text":"This feature allows a user to clone any folder to a specific destination: to user's personal folder, or user's project. It would be helpful to create a new project way faster due to copying metadata, configurations, storages from another project. To copy a folder, you need READ permissions for the copied folder and WRITE permissions for a folder selected as a destination. For more information see 13. Permissions . Note : learn more about metadata here . To clone a folder, the following steps shall be performed: Navigate to the desired folder page. Click icon and choose \"Clone\" menu item. The \"Select destination folder\" will pop-up. Note : The pop-up window will be open with parent folder of a copied folder as a default destination. Choose the destination if it needed by classical navigation actions. Note : The \"Clone\" button ( 1 ) displays which destination is selected at the moment. Name the new clone of the folder ( 2 ). Note : The name shall not break the uniqueness principal: there shouldn't be two folders with the same name in one destination. Click \"Clone\" button - and the folder will be cloned. The page of the clone of the folder will be open automatically. All child of the copied folder will be copied. Note : The exception is pipelines. Pipelines won't be copied as far as it may cause the collision. Note : The storages are copied by creating a new empty one with a unique name and path. No file will be copied from a copied storage to a new one.","title":"4.4. Clone a folder"},{"location":"manual/04_Manage_Folder/4.5._Lock_a_folder/","text":"4.5. Lock a folder To lock or unlock a folder, a user shall have OWNER permissions. For more permissions details see 13. Permissions . If you want to save your folder from the changes, you may lock it. Other users won't be able to make any changes in a locked folder and its children. To lock a folder the following steps shall be performed: Navigate to a folder you want to lock. Click icon \u2192 Lock . Confirm you decision in the dialog window. The folder will be locked and marked with a specific symbol . To unlock the folder, you just need to click icon \u2192 Unlock and confirm it in the dialog box.","title":"4.5. Lock a Folder"},{"location":"manual/04_Manage_Folder/4.5._Lock_a_folder/#45-lock-a-folder","text":"To lock or unlock a folder, a user shall have OWNER permissions. For more permissions details see 13. Permissions . If you want to save your folder from the changes, you may lock it. Other users won't be able to make any changes in a locked folder and its children. To lock a folder the following steps shall be performed: Navigate to a folder you want to lock. Click icon \u2192 Lock . Confirm you decision in the dialog window. The folder will be locked and marked with a specific symbol . To unlock the folder, you just need to click icon \u2192 Unlock and confirm it in the dialog box.","title":"4.5. Lock a folder"},{"location":"manual/04_Manage_Folder/4._Manage_Folder/","text":"4. Manage Folder \"Details\" view Controls Upload metadata + Create \"Displays\" icon \"Gear\" icon Child objects controls One of the key objects represented in the \" Library \" space is Folder . It is an entity similar to the directories in the file system. Folders are used to structure other CP objects. Folders can be arranged into a tree like a file system, which helps to store information divided by departments, users, programs or other logical groups. Note : there is a special type of Folder - a Project . For details see here . \"Details\" view \" Details \" panel shows contents of the selected folder: subfolders, files, pipelines etc. Controls The following buttons are available at the top of the \" Details \" view of the folder: Upload metadata \" Upload metadata \" ( 1 ) control allows a user to upload new metadata entities (e.g. Samples, Participants) from the .csv/.tsv/.tdf files in a project or another folder selected by a user. It helps to organize and manage metadata in a user's workspace (project or some sandbox folder for collecting metadata). Learn more 5.2. Upload metadata . + Create A user can add: a CP objects to the selected folder with the Create button ( 2 ). See 4.1. Create an object in Folder ; create a CP object from a template. See Appendix B. Working with a Project . Note : The list of templates could be extended. \"Displays\" icon The control (3) includes options to change view of the page: Feature Description Descriptions This feature makes visible addition description for child objects: child objects' attributes description in creating form if it exists. Attributes Attributes control opens/closes attributes pane. Folder's attributes - keys (a) and values (b) will be represented in the metadata pane on the right: Note : If a selected folder has any defined attribute, Attributes pane is shown by default. See how to edit attributes here . Issues This feature shows/hides the issues of the current folder to discuss. To learn more see here \"Gear\" icon There are the following features in \"Gear\" (4) icon: Feature Description Edit folder Rename a folder and set permissions for the folder. See 4.2. Rename folder and 13. Permissions . Clone This feature helps to copy the current folder and its child objects. See more 4.4. Clone a folder . Lock/Unlock You can save your folder and its content from changes by locking it. Learn more 4.5. Lock a folder . Delete A user can delete a folder with the Delete icon. A folder will be deleted if metadata is stored only. See 4.3. Delete Folder . Child objects controls Control CP objects Description Delete Folder Delete ( 1 ) a current folder. Learn more 4.3. Delete Folder . Discussion Folder, Pipeline This icon ( 2 ) allows to create discussion threads in child objects: you can create a topic, leave comments and address them to a specific user. To learn more see here . Run Pipeline Allows running a pipeline from the parent folder. Click on the icon ( 3 ) and the Launch form page will be open. See 6.2. Launch a pipeline . Edit Pipeline, Data storage, Run configuration Click this icon ( 4 ) to edit basic CP object's information: name, description, etc.","title":"4.0. Overview"},{"location":"manual/04_Manage_Folder/4._Manage_Folder/#4-manage-folder","text":"\"Details\" view Controls Upload metadata + Create \"Displays\" icon \"Gear\" icon Child objects controls One of the key objects represented in the \" Library \" space is Folder . It is an entity similar to the directories in the file system. Folders are used to structure other CP objects. Folders can be arranged into a tree like a file system, which helps to store information divided by departments, users, programs or other logical groups. Note : there is a special type of Folder - a Project . For details see here .","title":"4. Manage Folder"},{"location":"manual/04_Manage_Folder/4._Manage_Folder/#details-view","text":"\" Details \" panel shows contents of the selected folder: subfolders, files, pipelines etc.","title":"\"Details\"\u00a0view"},{"location":"manual/04_Manage_Folder/4._Manage_Folder/#controls","text":"The following buttons are available at the top of the \" Details \" view of the folder:","title":"Controls"},{"location":"manual/04_Manage_Folder/4._Manage_Folder/#upload-metadata","text":"\" Upload metadata \" ( 1 ) control allows a user to upload new metadata entities (e.g. Samples, Participants) from the .csv/.tsv/.tdf files in a project or another folder selected by a user. It helps to organize and manage metadata in a user's workspace (project or some sandbox folder for collecting metadata). Learn more 5.2. Upload metadata .","title":"Upload metadata"},{"location":"manual/04_Manage_Folder/4._Manage_Folder/#create","text":"A user can add: a CP objects to the selected folder with the Create button ( 2 ). See 4.1. Create an object in Folder ; create a CP object from a template. See Appendix B. Working with a Project . Note : The list of templates could be extended.","title":"+ Create"},{"location":"manual/04_Manage_Folder/4._Manage_Folder/#displays-icon","text":"The control (3) includes options to change view of the page: Feature Description Descriptions This feature makes visible addition description for child objects: child objects' attributes description in creating form if it exists. Attributes Attributes control opens/closes attributes pane. Folder's attributes - keys (a) and values (b) will be represented in the metadata pane on the right: Note : If a selected folder has any defined attribute, Attributes pane is shown by default. See how to edit attributes here . Issues This feature shows/hides the issues of the current folder to discuss. To learn more see here","title":"\"Displays\" icon"},{"location":"manual/04_Manage_Folder/4._Manage_Folder/#gear-icon","text":"There are the following features in \"Gear\" (4) icon: Feature Description Edit folder Rename a folder and set permissions for the folder. See 4.2. Rename folder and 13. Permissions . Clone This feature helps to copy the current folder and its child objects. See more 4.4. Clone a folder . Lock/Unlock You can save your folder and its content from changes by locking it. Learn more 4.5. Lock a folder . Delete A user can delete a folder with the Delete icon. A folder will be deleted if metadata is stored only. See 4.3. Delete Folder .","title":"\"Gear\" icon"},{"location":"manual/04_Manage_Folder/4._Manage_Folder/#child-objects-controls","text":"Control CP objects Description Delete Folder Delete ( 1 ) a current folder. Learn more 4.3. Delete Folder . Discussion Folder, Pipeline This icon ( 2 ) allows to create discussion threads in child objects: you can create a topic, leave comments and address them to a specific user. To learn more see here . Run Pipeline Allows running a pipeline from the parent folder. Click on the icon ( 3 ) and the Launch form page will be open. See 6.2. Launch a pipeline . Edit Pipeline, Data storage, Run configuration Click this icon ( 4 ) to edit basic CP object's information: name, description, etc.","title":"Child objects controls"},{"location":"manual/05_Manage_Metadata/5.1._Add_Delete_metadata_items/","text":"5.1. Add/Delete metadata items To manage metadata items, a user shall have WRITE permission for the parent folder and the ROLE_ENTITY_MANAGER role. For more information see 13. Permissions . A user is able to add metadata item manually. Add metadata item To add metadata item the following steps shall be performed: Navigate to Metadata of the desired folder or project. Note : or navigate to specific metadata entity folder, e.g. Participant, Sample, etc. Click control - the pop-up window is open. Fill up the required fields: ID . It should be a unique identification for a new metadata item. Type . Choose the metadata item type, e.g. Participant, Sample, etc Note : if you clicked control from the specific metadata entity folder, the type would be set by default, but you would be able to change it. Click Add parameter to set attributes for the new metadata instance. It could be: String attribute. You can add an attribute with any name and value. Link to a metadata entity. You can choose a link to what a metadata entity you want to add as an attribute, e.g. set a link to a participant existing in the CP as a sample's attribute. Click Create - the new metadata item will be created and shown in the chosen metadata entity table. Delete metadata item To delete metadata item the following steps shall be performed: Navigate to the metadata entity table that contains the metadata item you want to delete. Tick one metadata item or more - the bulk operation group of buttons is enabled. Click button. Confirm your choice in the dialog window. The items are removed.","title":"5.1. Add/delete Metadata items"},{"location":"manual/05_Manage_Metadata/5.1._Add_Delete_metadata_items/#51-adddelete-metadata-items","text":"To manage metadata items, a user shall have WRITE permission for the parent folder and the ROLE_ENTITY_MANAGER role. For more information see 13. Permissions . A user is able to add metadata item manually.","title":"5.1. Add/Delete metadata items"},{"location":"manual/05_Manage_Metadata/5.1._Add_Delete_metadata_items/#add-metadata-item","text":"To add metadata item the following steps shall be performed: Navigate to Metadata of the desired folder or project. Note : or navigate to specific metadata entity folder, e.g. Participant, Sample, etc. Click control - the pop-up window is open. Fill up the required fields: ID . It should be a unique identification for a new metadata item. Type . Choose the metadata item type, e.g. Participant, Sample, etc Note : if you clicked control from the specific metadata entity folder, the type would be set by default, but you would be able to change it. Click Add parameter to set attributes for the new metadata instance. It could be: String attribute. You can add an attribute with any name and value. Link to a metadata entity. You can choose a link to what a metadata entity you want to add as an attribute, e.g. set a link to a participant existing in the CP as a sample's attribute. Click Create - the new metadata item will be created and shown in the chosen metadata entity table.","title":"Add metadata item"},{"location":"manual/05_Manage_Metadata/5.1._Add_Delete_metadata_items/#delete-metadata-item","text":"To delete metadata item the following steps shall be performed: Navigate to the metadata entity table that contains the metadata item you want to delete. Tick one metadata item or more - the bulk operation group of buttons is enabled. Click button. Confirm your choice in the dialog window. The items are removed.","title":"Delete metadata item"},{"location":"manual/05_Manage_Metadata/5.2._Upload_metadata/","text":"5.2. Upload metadata To upload a Metadata to a Folder you need to have WRITE permission for that folder and the ROLE_ENTITY_MANAGER role. For more information see 13. Permissions . Each folder allows uploading metadata to its space. But to get the full set of features it is advised to upload the metadata into the special types of folders called Projects . How to create a project described here . Uploading could be executed from the csv or tsv file. The structure of load file should be as in Table 1 . Table 1 - Load file structure (referenced_entity_type):ID membership:MembershipAttributeName:(referenced_entity_type):ID AttributeName referenced_uid_value referenced_uid_value attribute_value referenced_uid_value referenced_uid_value attribute_value referenced_uid_value referenced_uid_value attribute_value Examples: wes-11-rep-samples.csv , wes-11-rep-set.csv (referenced_entity_type):ID - here you set up what type of entity you are uploading and what is an ID of it, e.g. Sample:ID = FZ700059549 . membership:MembershipAttributeName:(referenced_entity_type):ID - this is a reference to an instance of another entity. If you have a sample owned by a participant, you can show this connection - membership:Participants:Participant:ID = TY90000044343 . Note : make sure, that the reference entity id already exists in the system. AttributeName - any attribute name of an instance, e.g. RNA or DNA type, Blood or Tissue, etc. Upload instances of an entity Click on button. The Uploading metadata window is open. Select the appropriate file. Confirm choice by clicking OK . Note : if your file contains a link to the instance that doesn't exist, the whole metadata file won't be uploaded. For example, you set a link to a participant ID that doesn't exist in the file with samples. Note : while uploading the progress bar will be shown: The new metadata is uploaded in the folder of a specific entity in the project. Note : the instances will be uploaded to the entity with a name, related to the first column if exists (e.g. \"Sample\"). Otherwise, the new entity folder will be created. Note : if the file contains instances already exist in the system, the attributes of it will be updated.","title":"5.2. Upload Metadata"},{"location":"manual/05_Manage_Metadata/5.2._Upload_metadata/#52-upload-metadata","text":"To upload a Metadata to a Folder you need to have WRITE permission for that folder and the ROLE_ENTITY_MANAGER role. For more information see 13. Permissions . Each folder allows uploading metadata to its space. But to get the full set of features it is advised to upload the metadata into the special types of folders called Projects . How to create a project described here . Uploading could be executed from the csv or tsv file. The structure of load file should be as in Table 1 . Table 1 - Load file structure (referenced_entity_type):ID membership:MembershipAttributeName:(referenced_entity_type):ID AttributeName referenced_uid_value referenced_uid_value attribute_value referenced_uid_value referenced_uid_value attribute_value referenced_uid_value referenced_uid_value attribute_value Examples: wes-11-rep-samples.csv , wes-11-rep-set.csv (referenced_entity_type):ID - here you set up what type of entity you are uploading and what is an ID of it, e.g. Sample:ID = FZ700059549 . membership:MembershipAttributeName:(referenced_entity_type):ID - this is a reference to an instance of another entity. If you have a sample owned by a participant, you can show this connection - membership:Participants:Participant:ID = TY90000044343 . Note : make sure, that the reference entity id already exists in the system. AttributeName - any attribute name of an instance, e.g. RNA or DNA type, Blood or Tissue, etc.","title":"5.2. Upload metadata"},{"location":"manual/05_Manage_Metadata/5.2._Upload_metadata/#upload-instances-of-an-entity","text":"Click on button. The Uploading metadata window is open. Select the appropriate file. Confirm choice by clicking OK . Note : if your file contains a link to the instance that doesn't exist, the whole metadata file won't be uploaded. For example, you set a link to a participant ID that doesn't exist in the file with samples. Note : while uploading the progress bar will be shown: The new metadata is uploaded in the folder of a specific entity in the project. Note : the instances will be uploaded to the entity with a name, related to the first column if exists (e.g. \"Sample\"). Otherwise, the new entity folder will be created. Note : if the file contains instances already exist in the system, the attributes of it will be updated.","title":"Upload instances of an entity"},{"location":"manual/05_Manage_Metadata/5.3._Customize_view_of_the_entity_instance_table/","text":"5.3. Customize view of the entity instance table To customize view of the table with instances of an entity you need to have READ permissions for the folder with that metadata. For more information see 13. Permissions . This page describes how a user can customize an entity instance table and make it more easy to read. Any user, who has permissions for reading, will be able to customize the view of the table: change set of viewed attributes, change the order of attributes and reset the default settings. Press the \"Change view\" control. A list of accessible attributes shows up. The list contains all attributes of entities, uploaded in the current project. The attributes of current view are ticked. Choose desired attributes by ticking. Note : The last tick will be disabled, so you can't clear all checkboxes. To change the order, click a control in front of the desired attribute and pulling it up or down. The changes are applied instantly. The table view has only selected attributes. Note : This customization saved only for logged in user. To leave the form a user shall click outside the form. Note : you are able to restore the initial order of columns. To do that, click Reset Columns control.","title":"5.3. Customize view of the entity instance table"},{"location":"manual/05_Manage_Metadata/5.3._Customize_view_of_the_entity_instance_table/#53-customize-view-of-the-entity-instance-table","text":"To customize view of the table with instances of an entity you need to have READ permissions for the folder with that metadata. For more information see 13. Permissions . This page describes how a user can customize an entity instance table and make it more easy to read. Any user, who has permissions for reading, will be able to customize the view of the table: change set of viewed attributes, change the order of attributes and reset the default settings. Press the \"Change view\" control. A list of accessible attributes shows up. The list contains all attributes of entities, uploaded in the current project. The attributes of current view are ticked. Choose desired attributes by ticking. Note : The last tick will be disabled, so you can't clear all checkboxes. To change the order, click a control in front of the desired attribute and pulling it up or down. The changes are applied instantly. The table view has only selected attributes. Note : This customization saved only for logged in user. To leave the form a user shall click outside the form. Note : you are able to restore the initial order of columns. To do that, click Reset Columns control.","title":"5.3. Customize view of the entity instance table"},{"location":"manual/05_Manage_Metadata/5.4._Launch_a_run_configuration_on_metadata/","text":"5.4. Launch a run configuration on metadata To launch a run configuration on metadata a user shall have the following permissions: READ permissions a for folder contains metadata items; READ and EXECUTE permissions for a run configuration. For more information see 13. Permissions . A user could launch a run configuration on selected metadata from the metadata space. To launch a run configuration, the following steps shall be performed: Navigate to the desired folder with metadata items. Tick desired metadata items (see the picture below, 1 ). Click Run button (see the picture above, 2 ) - and the \"Select configuration\" pop-up window will be open. Note : Clicking on a configuration item, you'll see configuration's parameters. The second click will hide the parameters. Note : if you want to choose a run configuration with Root entity that differs from selected metadata items, please, specify it in \"Define expression\" field. To learn more about expressions, see here . Click \"OK\" button - and the runs will be scheduled. You'll be redirected into Run space automatically.","title":"5.4. Launch a Run configuration"},{"location":"manual/05_Manage_Metadata/5.4._Launch_a_run_configuration_on_metadata/#54-launch-a-run-configuration-on-metadata","text":"To launch a run configuration on metadata a user shall have the following permissions: READ permissions a for folder contains metadata items; READ and EXECUTE permissions for a run configuration. For more information see 13. Permissions . A user could launch a run configuration on selected metadata from the metadata space. To launch a run configuration, the following steps shall be performed: Navigate to the desired folder with metadata items. Tick desired metadata items (see the picture below, 1 ). Click Run button (see the picture above, 2 ) - and the \"Select configuration\" pop-up window will be open. Note : Clicking on a configuration item, you'll see configuration's parameters. The second click will hide the parameters. Note : if you want to choose a run configuration with Root entity that differs from selected metadata items, please, specify it in \"Define expression\" field. To learn more about expressions, see here . Click \"OK\" button - and the runs will be scheduled. You'll be redirected into Run space automatically.","title":"5.4. Launch a run configuration on metadata"},{"location":"manual/05_Manage_Metadata/5.5._Download_data_from_external_resources_to_the_cloud_data_storage/","text":"5.5. Download data from external resources to the cloud data storage Overview Upload metadata with list of external resources from CSV/TSV file Download data from external resources Overview Users often get the raw datasets from the external partners for processing. CP provides comfortable way to load a list of such files as external links to the CP GUI and launch a data load procedure to the cloud storage in background mode. Users can provide CSV/TSV files with the external links and submit a data transfer job, so that the files will be moved to the cloud storage in the background. Upload metadata with list of external resources from CSV/TSV file Note : To upload a Metadata to a Folder you need to have WRITE permission for that folder and the ROLE_ENTITY_MANAGER role. For more information see 13. Permissions . Note : file with list of external resources should have PATH column and http/ftp links in this column (example: sample.csv ) Navigate on folder. Click on \"Upload metadata\" button. Navigate to CSV/TSV file with list of external resources in appeared dialog and select file. Click \" Open \" button. Click on \"Metadata\" object: Click on \"Sample\" class: List of samples with the external links will be appeared: Download data from external resources Above the list of external links, uploaded on previous step, click \" Transfer to the cloud \" button: The pop-up window for preparing for transferring will appear: In this window fill fields: a . Input data storage which will be use as a destination for downloading data. Click on button. In opened pop-up window select required storage (on the left panel with folder-tree) ( 1 ). In selected storage set the checkbox opposite the folder name, where external data will be downloaded ( 2 ). Click \" Ok \" button ( 3 ): Note : you need to have READ and WRITE permissions for folder and data storage, that contain folder for downloading. b . Select CSV/TSV columns (only from PATH columns), which shall be used to get external URLs (if several columns contain URLs - all can be used). c . ( optionally ) Select whether to rename resulting path to some other value (can be specified as another column cell, e.g. sample name). For do that click on button and select from dropdown list. d . ( optionally ) Input max threads count, if needs to limit. e . ( optionally ) Set if needs to create new folders within destination in case when several columns are selected for \" Path fields \" option ( b ). E.g. if two columns contain URLs and both are selected - then folders will be created for the corresponding column name and used for appropriate files storage. f . ( optionally ) Set if needs to update external URL within a table to the new location of the files. If set - http/ftp URLs will be changed to data storage path. Such data structure can be then used for a processing by a pipeline: URLs are changed to the clickable storage-hyperlinks (checkbox is set): URLs aren't changed and not-clickable hyperlinks (checkbox isn't set): Once you filled the form, click \" Start download \" button: System pipeline (transfer job) will be started automatically: Once the transfer job will be finished successfully, files will be located into the selected data storage:","title":"5.5. Download data from external resources"},{"location":"manual/05_Manage_Metadata/5.5._Download_data_from_external_resources_to_the_cloud_data_storage/#55-download-data-from-external-resources-to-the-cloud-data-storage","text":"Overview Upload metadata with list of external resources from CSV/TSV file Download data from external resources","title":"5.5. Download data from external resources to the cloud data storage"},{"location":"manual/05_Manage_Metadata/5.5._Download_data_from_external_resources_to_the_cloud_data_storage/#overview","text":"Users often get the raw datasets from the external partners for processing. CP provides comfortable way to load a list of such files as external links to the CP GUI and launch a data load procedure to the cloud storage in background mode. Users can provide CSV/TSV files with the external links and submit a data transfer job, so that the files will be moved to the cloud storage in the background.","title":"Overview"},{"location":"manual/05_Manage_Metadata/5.5._Download_data_from_external_resources_to_the_cloud_data_storage/#upload-metadata-with-list-of-external-resources-from-csvtsv-file","text":"Note : To upload a Metadata to a Folder you need to have WRITE permission for that folder and the ROLE_ENTITY_MANAGER role. For more information see 13. Permissions . Note : file with list of external resources should have PATH column and http/ftp links in this column (example: sample.csv ) Navigate on folder. Click on \"Upload metadata\" button. Navigate to CSV/TSV file with list of external resources in appeared dialog and select file. Click \" Open \" button. Click on \"Metadata\" object: Click on \"Sample\" class: List of samples with the external links will be appeared:","title":"Upload metadata with list of external resources from CSV/TSV file"},{"location":"manual/05_Manage_Metadata/5.5._Download_data_from_external_resources_to_the_cloud_data_storage/#download-data-from-external-resources","text":"Above the list of external links, uploaded on previous step, click \" Transfer to the cloud \" button: The pop-up window for preparing for transferring will appear: In this window fill fields: a . Input data storage which will be use as a destination for downloading data. Click on button. In opened pop-up window select required storage (on the left panel with folder-tree) ( 1 ). In selected storage set the checkbox opposite the folder name, where external data will be downloaded ( 2 ). Click \" Ok \" button ( 3 ): Note : you need to have READ and WRITE permissions for folder and data storage, that contain folder for downloading. b . Select CSV/TSV columns (only from PATH columns), which shall be used to get external URLs (if several columns contain URLs - all can be used). c . ( optionally ) Select whether to rename resulting path to some other value (can be specified as another column cell, e.g. sample name). For do that click on button and select from dropdown list. d . ( optionally ) Input max threads count, if needs to limit. e . ( optionally ) Set if needs to create new folders within destination in case when several columns are selected for \" Path fields \" option ( b ). E.g. if two columns contain URLs and both are selected - then folders will be created for the corresponding column name and used for appropriate files storage. f . ( optionally ) Set if needs to update external URL within a table to the new location of the files. If set - http/ftp URLs will be changed to data storage path. Such data structure can be then used for a processing by a pipeline: URLs are changed to the clickable storage-hyperlinks (checkbox is set): URLs aren't changed and not-clickable hyperlinks (checkbox isn't set): Once you filled the form, click \" Start download \" button: System pipeline (transfer job) will be started automatically: Once the transfer job will be finished successfully, files will be located into the selected data storage:","title":"Download data\u00a0from external resources"},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/","text":"5. Manage Metadata Overview \"Details\" view Controls Search field Sorting control \"Change view\" + Add instance Upload metadata Show attributes/Hide attributes Bulk operation panel Overview Metadata is a CP object that defines custom data entities (see the definition below) associated with raw data files (fastq, bcl, etc.) or data parameters (see the picture below, arrow 1 ). By using this object a user can create a complex analysis environment. For example, you can customize your analysis to work with a subset of your data. Two important concepts of the metadata object is an Entity and an Instance of an entity . Entity - abstract category of comparable objects. For example, entity \"Sample\" can contain sequencing data from different people (see the picture below, arrow 2 ). An Instance of an entity - a specific representation of an entity. For example, sequencing data from a particular patient in the \"Sample\" entity is an instance of that entity (see the picture below, arrow 3 ). \"Details\" view \"Details\" panel displays content as a table of entity instances. Each column is an attribute of an instance, which is duplicated in the \"Attribute\" panel. Note : more about managing instance's attribute you can learn here . Controls The following buttons are available in the metadata entity space: Search field To find a particular instance of an entity a user shall use the Search field (see the picture above, 1 ), which is searching for the occurrence of entered text in the ID column of the table. Sorting control To sort instances of an entity in a table, a user shall click a header of the desired column: 1 click sorts a list in an ascending order, the next click sorts a list in a descending order, the next click reset sorting. \"Change view\" This control (see the picture above, 2 ) allows customizing the view of the table with instances of an entity. For more information see 5.3. Customize view of the entity instance table . + Add instance To add a new instance in the current metadata container, click + Add instance control (see the picture above, 3 ). For more information see 5.1. Add/Delete metadata items . Upload metadata Use this control (see the picture above, 4 ) to create the metadata object or to add entities to the metadata object/to add instances of an entity to the existing entity. See here for more information - 5.2. Upload metadata . Show attributes/Hide attributes This button (see the picture above, 5 ) allows to view or edit attributes of a particular instance of an entity. For more information see 17. CP objects tagging by additional attributes . Bulk operation panel This panel allows to execute operations for more than one item. You can tick desired items and the panel switch to active mode. Control Description DELETE To delete one or more metadata item ( 1 ). See more details here . CLEAR SELECTION Clears all selected items ( 2 ). The panel is deactivated. TRANSFER TO THE CLOUD To download files from the external ftp/http resources ( 3 ). See more details here . RUN Allows to execute run configurations for the selected items ( 4 ). See details here .","title":"5.0. Overview"},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/#5-manage-metadata","text":"Overview \"Details\" view Controls Search field Sorting control \"Change view\" + Add instance Upload metadata Show attributes/Hide attributes Bulk operation panel","title":"5. Manage Metadata"},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/#overview","text":"Metadata is a CP object that defines custom data entities (see the definition below) associated with raw data files (fastq, bcl, etc.) or data parameters (see the picture below, arrow 1 ). By using this object a user can create a complex analysis environment. For example, you can customize your analysis to work with a subset of your data. Two important concepts of the metadata object is an Entity and an Instance of an entity . Entity - abstract category of comparable objects. For example, entity \"Sample\" can contain sequencing data from different people (see the picture below, arrow 2 ). An Instance of an entity - a specific representation of an entity. For example, sequencing data from a particular patient in the \"Sample\" entity is an instance of that entity (see the picture below, arrow 3 ).","title":"Overview"},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/#details-view","text":"\"Details\" panel displays content as a table of entity instances. Each column is an attribute of an instance, which is duplicated in the \"Attribute\" panel. Note : more about managing instance's attribute you can learn here .","title":"\"Details\"\u00a0view"},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/#controls","text":"The following buttons are available in the metadata entity space:","title":"Controls"},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/#search-field","text":"To find a particular instance of an entity a user shall use the Search field (see the picture above, 1 ), which is searching for the occurrence of entered text in the ID column of the table.","title":"Search field"},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/#sorting-control","text":"To sort instances of an entity in a table, a user shall click a header of the desired column: 1 click sorts a list in an ascending order, the next click sorts a list in a descending order, the next click reset sorting.","title":"Sorting control"},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/#change-view","text":"This control (see the picture above, 2 ) allows customizing the view of the table with instances of an entity. For more information see 5.3. Customize view of the entity instance table .","title":"\"Change view\""},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/#add-instance","text":"To add a new instance in the current metadata container, click + Add instance control (see the picture above, 3 ). For more information see 5.1. Add/Delete metadata items .","title":"+ Add instance"},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/#upload-metadata","text":"Use this control (see the picture above, 4 ) to create the metadata object or to add entities to the metadata object/to add instances of an entity to the existing entity. See here for more information - 5.2. Upload metadata .","title":"Upload metadata"},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/#show-attributeshide-attributes","text":"This button (see the picture above, 5 ) allows to view or edit attributes of a particular instance of an entity. For more information see 17. CP objects tagging by additional attributes .","title":"Show attributes/Hide\u00a0attributes"},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/#bulk-operation-panel","text":"This panel allows to execute operations for more than one item. You can tick desired items and the panel switch to active mode. Control Description DELETE To delete one or more metadata item ( 1 ). See more details here . CLEAR SELECTION Clears all selected items ( 2 ). The panel is deactivated. TRANSFER TO THE CLOUD To download files from the external ftp/http resources ( 3 ). See more details here . RUN Allows to execute run configurations for the selected items ( 4 ). See details here .","title":"Bulk operation panel"},{"location":"manual/06_Manage_Pipeline/6.1.1._Building_WDL_pipeline_with_graphical_PipelineBuilder/","text":"6.1.1. Building WDL pipeline with graphical PipelineBuilder Overview Creating a new pipeline with a Pipeline Builder Overriding docker image for a specific task Example Pipeline Search over the Graph To create a new WDL pipeline in a Folder you need to have WRITE permissions for that folder and the ROLE_PIPELINE_MANAGER role. For more information see 13. Permissions . Overview Cloud Pipeline allows creating pipelines using graphical IDE called \" PipelineBuilder \". \" PipelineBuilder \" provides GUI approach to construct WDL pipeline workflow supported dependencies, loops, etc without programming. \" PipelineBuilder \" is based on WDL language (by Broad Institute, https://github.com/openwdl/wdl ) that is executed by \"Cromwell\" service. Creating a new pipeline with a Pipeline Builder To start using Pipeline Builder - create a new pipeline from \" WDL \" template: + Create \u2192 Pipeline \u2192 WDL . Name it (e.g. \"pipeline-builder-test\"). This will create a new pipeline with a draft version. Click on the created pipeline and open the pipeline draft version. Navigate to the GRAPH tab. Default pipeline will be generated with a single task \" HelloWorld_print \". Auxiliary controls (\"Save\", \"Revert changes\", \"Layout\", \"Fit to screen\", \"Show links\", \"Zoom out\", \"Zoom in\", \"Search\", \"Fullscreen\") are in the left side of the WDL GRAPH: To add more tasks: click PROPERTIES in the top right corner to open \" Properties \" panel then click ADD TASK button Note : The ADD SCATTER button allows adding scatters . This will create a new task in the main workflow: Click on the just-created task. It will become highlighted and the task editor will appear at \" Properties \" panel: That panel contains the following controls: Name ( a ) - a name of a task (it will be used for visualizing in a workflow and logging). If you want to change it - click on that field and input a new task name. Valid names are marked with green \"OK\" icon: invalid - with red \"cross\" icon: Inputs (collapsed header) ( b ) - a list of parameters that a task can accept from upstream tasks. To add parameter - click on ADD button. The header will be expanded automatically: In appeared fields input attributes of a new input parameter. Outputs (collapsed header) ( c ) - a list of parameters that a task will pass to the downstream tasks. To add parameter - click on ADD button. The header will be expanded automatically: In appeared fields input attributes of a new output parameter. Use another docker image ( d ) - if ticked - docker image, that is used within a task, can be overridden (i.e. different tools/images can be used for each task of the workflow). If you want to use another docker image - set that checkbox and then click upon an appeared field to select docker image: See more info below . Use another compute node ( e ) - if ticked - instance type, that is used within a task, can be changed (e.g. more productive node can be used for the specific task). If you want to use another instance type - set that checkbox and then select instance from an appeared dropdown list: Command ( f ) - a shell script that will be executed within a task. DELETE task button ( g ) - to delete a task. Inputted changes are automatically being represented at the graph. The following picture presents an example of a basic task creating: After input values of a new task, click Save and Commit . The visualization with a new \"task1\" with one output will be displayed. To create a \"real\" workflow - create a second task with one input: Then click Save and Commit . Link task1 output with task2 input with a mouse cursor (click \"output1\" and slide to \"input1\"). Then click Save and Commit . Note : to remove link hover mouse pointer over it and then click on \"cross\" button: Note : \" HelloWorld_print \" task isn't linked to other tasks. When code generates from a graph, tasks without links to other tasks can be executed in any order (e.g. task1 \u2192 HelloWorld_print \u2192 task2 or HelloWorld_print \u2192 task1 \u2192 task2 , etc). If you want to delete a task - click upon it, open \"Properties\" panel and click DELETE task button in the bottom of the panel: To add skatter: click PROPERTIES in the top right corner to open \" Properties \" panel then click ADD SCATTER button This will create a new scatter in the main workflow: Click on the just-created scatter. It will become highlighted and the scatter editor will appear at \" Properties \" panel: That panel contains the following controls: ADD TASK button ( a ) - to add a task into the scatter item. Creation of a task in the scatter is the same as described above. You can add inputs/outputs for that scatter task, links them with outputs/inputs of other tasks. Inputs (collapsed header) ( b ) - a mandatory parameter of a ScatterItem type that a scatter can accept from upstream task. User couldn't remove it or change its type, only change its name. For that - click upon the header and specify a new name: DELETE scatter button ( c ) - to delete a scatter. Inputted changes are automatically being represented at the graph. The following picture presents an example with a scatter, that has a task, which output is linked to \"HelloWorld_print\" task's input: Overriding docker image for a specific task By default, all tasks (and their commands) will run within a docker image that is specified for the initial run. This is useful when all tools/libraries are packed into a single docker image. But if a specific step requires tools that are not packed into the same docker image - \" PipelineBuilder \" allows to specify another docker image: Open any task details and check the Use another docker image option. This will bring a docker image selector. In an appeared pop-up choose Registry , Tool group . Select an image and its version. Then click OK button. Specified docker image will be used instead of the initial one. This means that a command specified for a task will be executed in another docker container. Example Pipeline As an example - R-based scRNA secondary analysis script. This script uses 10xGenomics matrix as input. Workflow diagram: Note : this workflow uses \"overridden\" docker image for the last task to show how it behaves (as described in Overriding docker image for a specific task section). Search over the Graph To search some element over the Graph: Click the Search button in the auxiliary controls menu: In the appeared field input the name or type of the element you want to find. Search results will appear immediately while typing: In the list, click the element you want to find (e.g. task3 ) Found element will be highlighted and placed into the focus:","title":"6.1.1. Building WDL Pipeline with graphical PipelineBuilder"},{"location":"manual/06_Manage_Pipeline/6.1.1._Building_WDL_pipeline_with_graphical_PipelineBuilder/#611-building-wdl-pipeline-with-graphical-pipelinebuilder","text":"Overview Creating a new pipeline with a Pipeline Builder Overriding docker image for a specific task Example Pipeline Search over the Graph To create a new WDL pipeline in a Folder you need to have WRITE permissions for that folder and the ROLE_PIPELINE_MANAGER role. For more information see 13. Permissions .","title":"6.1.1. Building WDL pipeline with graphical PipelineBuilder"},{"location":"manual/06_Manage_Pipeline/6.1.1._Building_WDL_pipeline_with_graphical_PipelineBuilder/#overview","text":"Cloud Pipeline allows creating pipelines using graphical IDE called \" PipelineBuilder \". \" PipelineBuilder \" provides GUI approach to construct WDL pipeline workflow supported dependencies, loops, etc without programming. \" PipelineBuilder \" is based on WDL language (by Broad Institute, https://github.com/openwdl/wdl ) that is executed by \"Cromwell\" service.","title":"Overview"},{"location":"manual/06_Manage_Pipeline/6.1.1._Building_WDL_pipeline_with_graphical_PipelineBuilder/#creating-a-new-pipeline-with-a-pipeline-builder","text":"To start using Pipeline Builder - create a new pipeline from \" WDL \" template: + Create \u2192 Pipeline \u2192 WDL . Name it (e.g. \"pipeline-builder-test\"). This will create a new pipeline with a draft version. Click on the created pipeline and open the pipeline draft version. Navigate to the GRAPH tab. Default pipeline will be generated with a single task \" HelloWorld_print \". Auxiliary controls (\"Save\", \"Revert changes\", \"Layout\", \"Fit to screen\", \"Show links\", \"Zoom out\", \"Zoom in\", \"Search\", \"Fullscreen\") are in the left side of the WDL GRAPH: To add more tasks: click PROPERTIES in the top right corner to open \" Properties \" panel then click ADD TASK button Note : The ADD SCATTER button allows adding scatters . This will create a new task in the main workflow: Click on the just-created task. It will become highlighted and the task editor will appear at \" Properties \" panel: That panel contains the following controls: Name ( a ) - a name of a task (it will be used for visualizing in a workflow and logging). If you want to change it - click on that field and input a new task name. Valid names are marked with green \"OK\" icon: invalid - with red \"cross\" icon: Inputs (collapsed header) ( b ) - a list of parameters that a task can accept from upstream tasks. To add parameter - click on ADD button. The header will be expanded automatically: In appeared fields input attributes of a new input parameter. Outputs (collapsed header) ( c ) - a list of parameters that a task will pass to the downstream tasks. To add parameter - click on ADD button. The header will be expanded automatically: In appeared fields input attributes of a new output parameter. Use another docker image ( d ) - if ticked - docker image, that is used within a task, can be overridden (i.e. different tools/images can be used for each task of the workflow). If you want to use another docker image - set that checkbox and then click upon an appeared field to select docker image: See more info below . Use another compute node ( e ) - if ticked - instance type, that is used within a task, can be changed (e.g. more productive node can be used for the specific task). If you want to use another instance type - set that checkbox and then select instance from an appeared dropdown list: Command ( f ) - a shell script that will be executed within a task. DELETE task button ( g ) - to delete a task. Inputted changes are automatically being represented at the graph. The following picture presents an example of a basic task creating: After input values of a new task, click Save and Commit . The visualization with a new \"task1\" with one output will be displayed. To create a \"real\" workflow - create a second task with one input: Then click Save and Commit . Link task1 output with task2 input with a mouse cursor (click \"output1\" and slide to \"input1\"). Then click Save and Commit . Note : to remove link hover mouse pointer over it and then click on \"cross\" button: Note : \" HelloWorld_print \" task isn't linked to other tasks. When code generates from a graph, tasks without links to other tasks can be executed in any order (e.g. task1 \u2192 HelloWorld_print \u2192 task2 or HelloWorld_print \u2192 task1 \u2192 task2 , etc). If you want to delete a task - click upon it, open \"Properties\" panel and click DELETE task button in the bottom of the panel: To add skatter: click PROPERTIES in the top right corner to open \" Properties \" panel then click ADD SCATTER button This will create a new scatter in the main workflow: Click on the just-created scatter. It will become highlighted and the scatter editor will appear at \" Properties \" panel: That panel contains the following controls: ADD TASK button ( a ) - to add a task into the scatter item. Creation of a task in the scatter is the same as described above. You can add inputs/outputs for that scatter task, links them with outputs/inputs of other tasks. Inputs (collapsed header) ( b ) - a mandatory parameter of a ScatterItem type that a scatter can accept from upstream task. User couldn't remove it or change its type, only change its name. For that - click upon the header and specify a new name: DELETE scatter button ( c ) - to delete a scatter. Inputted changes are automatically being represented at the graph. The following picture presents an example with a scatter, that has a task, which output is linked to \"HelloWorld_print\" task's input:","title":"Creating a new pipeline with a Pipeline Builder"},{"location":"manual/06_Manage_Pipeline/6.1.1._Building_WDL_pipeline_with_graphical_PipelineBuilder/#overriding-docker-image-for-a-specific-task","text":"By default, all tasks (and their commands) will run within a docker image that is specified for the initial run. This is useful when all tools/libraries are packed into a single docker image. But if a specific step requires tools that are not packed into the same docker image - \" PipelineBuilder \" allows to specify another docker image: Open any task details and check the Use another docker image option. This will bring a docker image selector. In an appeared pop-up choose Registry , Tool group . Select an image and its version. Then click OK button. Specified docker image will be used instead of the initial one. This means that a command specified for a task will be executed in another docker container.","title":"Overriding docker image for a specific task"},{"location":"manual/06_Manage_Pipeline/6.1.1._Building_WDL_pipeline_with_graphical_PipelineBuilder/#example-pipeline","text":"As an example - R-based scRNA secondary analysis script. This script uses 10xGenomics matrix as input. Workflow diagram: Note : this workflow uses \"overridden\" docker image for the last task to show how it behaves (as described in Overriding docker image for a specific task section).","title":"Example Pipeline"},{"location":"manual/06_Manage_Pipeline/6.1.1._Building_WDL_pipeline_with_graphical_PipelineBuilder/#search-over-the-graph","text":"To search some element over the Graph: Click the Search button in the auxiliary controls menu: In the appeared field input the name or type of the element you want to find. Search results will appear immediately while typing: In the list, click the element you want to find (e.g. task3 ) Found element will be highlighted and placed into the focus:","title":"Search over the Graph"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/","text":"6.1. Create and configure pipeline Create a pipeline in a Library space Customize a pipeline version Edit documentation (optional) Edit code section Edit pipeline configuration (optional) Add/delete storage rules (optional) Edit a pipeline info Example: Create Pipeline Pipeline input data Pipeline output folder Configure the main_file Configure pipeline input/output parameters via GUI Check the results of pipeline execution Example: Add pipeline configuration Example: Create a configuration that uses system parameter Example: Limit mounted storages To create a Pipeline in a Folder you need to have WRITE permission for that folder and the ROLE_PIPELINE_MANAGER role. To edit pipeline you need just WRITE permissions for a pipeline. For more information see 13. Permissions . To create a working pipeline version you need: Create a pipeline in a Library space Customize a pipeline version: Edit documentation (optional) Edit Code file Edit Configuration, Add new configuration (optional) Add storage rules (optional) . Create a pipeline in a Library space Go to the \"Library\" tab and select a folder. Click + Create \u2192 Pipeline and choose one of the built-in pipeline templates ( Python , Shell , Snakemake , Luigi , WDL , Nextflow ) or choose DEFAULT item to create a pipeline without a template. Pipeline template defines the programming language for a pipeline. As templates are empty user shall write pipeline logic on his own. Enter pipeline's name (pipeline description is optional) in the popped-up form. Click the Create button. A new pipeline will appear in the folder. Note : To configure repository where to store pipeline versions click the Edit repository settings button. Click on the button and two additional fields will appear: Repository (repository address) and Token (password to access a repository). The new pipeline will appear in a Library space. Customize a pipeline version Click a pipeline version to start its configuration process. Edit documentation (optional) This option allows you to make a detailed description of your pipelines. Navigate to the Documents tab and: Click Edit . Change the document using a markdown language . Click the Save button. Enter a description of the change and click Commit . Changes are saved. Edit code section It is not optional because you need to create a pipeline that will be tailored to your specific needs. For that purpose, you need to extend basic pipeline templates/add new files. Navigate to the Code tab. Click on any file you want to edit. Note : each pipeline version has a default code file: it named after a pipeline and has a respective extension. A new window with file contents will open. Click the Edit button and change the code file in the desired way. When you are done, click the Save button. You'll be asked to write a Commit message (e.g. 'added second \"echo\" command'). Then click the Commit button. After that changes will be applied to your file. Note : all code files are downloaded to the node to run the pipeline. Just adding a new file to the Code section doesn't change anything. You need to specify the order of scripts execution by yourself. E.g. you have three files in your pipeline: first.sh ( main_file ), second.sh and config.json . cmd_template parameter is chmod +x $SCRIPTS_DIR/src/* $SCRIPTS_DIR/src/[main_file] . So in the first.sh file you need to explicitly specify execution of second.sh script for them both to run inside your pipeline, otherwise this file will be ignored. Edit pipeline configuration (optional) See details about pipeline configuration parameters here . Every pipeline has default pipeline configuration from the moment it was created. To change default pipeline configuration: Navigate to the Configuration tab. Expand \"Exec environment\" and \"Advanced\" tabs to see a full list of pipeline parameters. \"Parameters\" tab is opened by default. Change any parameter you need. In this example, we will set Cloud Region to Europe Ireland, Disk to 40 Gb and set the Timeout to 400 mins. Click the Save button. Now this will be the default pipeline configuration for the pipeline execution. Add/delete storage rules (optional) This section allows configuring what data will be transferred to an STS after pipeline execution. To add a new rule: Click the Add new rule button. A pop-up will appear. Enter File mask and then tick the box \"Move to STS\" to move pipeline output data to STS after pipeline execution. Note : If many rules with different Masks are present all of them are checked one by one. If a file corresponds to any of rules - it will be uploaded to the bucket. To delete storage rule click the Delete button in the right part of the storage rule's row. Edit a pipeline info To edit a pipeline info: Click the Gear icon in the right upper corner of the pipeline page The popup with the pipeline info will be opened: Here you can edit pipeline name ( a ) and description ( b ) To edit repository settings click the corresponding button ( c ): Here you can edit access token to a repository ( d ) Note : the \"Repository\" field is disabled for the existing pipelines Click the SAVE button to save changes Note : if you rename a pipeline the corresponding GitLab repo will be automatically renamed too. So, the clone/pull/push URL will change. Make sure to change the remote address, if this pipeline is used somewhere. How it works: Open the pipeline: Click the GIT REPOSITORY button in the right upper corner of the page: Pipeline name and repository name are identical Click the Gear icon in the right upper corner. In the popup change pipeline name and click the SAVE button: Click the GIT REPOSITORY button again: Pipeline name and repository name are identical Also, if you want just rename a pipeline without changing its other info fields: Hover over the pipeline name at the \"breadcrumbs\" control in the top of the pipeline page - the \"edit\" symbol will appear: Click the pipeline name - the field will become available to edit. Rename the pipeline: Press the Enter key or click any empty space - a new pipeline name will be saved: Example: Create Pipeline We will create a simple Shell pipeline (Shell template used). For that purpose, we will click + Create \u2192 Pipeline \u2192 SHELL . Then we will write Pipeline name ( 1 ), Pipeline description ( 2 ) and click Create ( 3 ). This pipeline will: Download a file. Rename it. Upload renamed the file to the bucket. Pipeline input data This is where pipeline input data is stored. About storages see here . This path will be used in pipeline parameters later on. Pipeline output folder This is where pipeline output data will be stored after pipeline execution. About storages see here . This path will be used in pipeline parameters later on. Configure the main_file The pipeline will consist of 2 files: main_file and config.json . Let's extend the main_file so that it renames the input file and puts it into the $ANALYSIS_DIR folder on the node from which data will be uploaded to the bucket. To do that click the main_file name and click the Edit button. Then type all the pipeline instructions. Click the Save button, input a commit message and click the Commit button. Configure pipeline input/output parameters via GUI Click the Run button. In the pipeline run configuration select the arrow near the Add parameter button and select the \"Input path parameter\" option from the drop-down list. Name the parameter (e.g. \"input\") and click on the grey \"download\" icon to select the path to the pipeline input data (we described pipeline input data above ). For pipeline output folder parameter choose the \"Output path parameter\" option from the drop-down list, name it and click on the grey \"upload\" icon to select the path to the pipeline output folder (we described pipeline output data above ). This is how everything looks after these parameters are set: Leave all other parameters default and click the Launch button. Check the results of pipeline execution After pipeline finished its execution, you can find the renamed file in the output folder: Example: Add pipeline configuration In this example, we will create a new pipeline configuration for the example pipeline and set it as default one. To add new pipeline configuration perform the following steps: Select a pipeline Select a pipeline version Navigate to the CONFIGURATION tab Click the + ADD button in the upper-right corner of the screen Specify Configuration name , Description (optionally) and the Template - this is a pipeline configuration, from which the new pipeline configuration will inherit its parameters (right now only the \"default\" template is available). Click the Create button. As you can see, the new configuration has the same parameters as the default configuration. Use Delete ( 1 ), Set as default ( 2 ) or Save ( 3 ) buttons to delete, set as default or save this configuration respectively. Expand the Exec environment section ( 1 ) and then Specify 30 GB Disk size ( 2 ), click the control to choose another Docker image ( 3 ). Click the Save button ( 4 ). Set \"new-configuration\" as default with the Set as default button. Navigate to the CODE tab. As you can see, config.json file now contains information about two configurations: \"default\" and \"new-configuration\". \"new-configuration\" is default one for pipeline execution. Example: Create a configuration that uses system parameter Users can specify system parameters (per run), that change/configure special behavior for the current run. In the example below we will use the system parameter, that installs and allows using of the DIND (Docker in Docker) in the launched container: Select a pipeline Select a pipeline version Navigate to the CONFIGURATION tab In the CONFIGURATION tab expand the Advanced section, set \"Start idle\" checkbox and click the Add system parameter button: Click the CP_CAP_DIND_CONTAINER option and then click the OK button: This option will enable docker engine for a run using a containerized approach. Added system parameter appears on the configuration page. Save the configuration - now it will use \"Docker inside Docker\" technology while running: To see it, click the Run button in the upper-right corner to launch the configuration. Edit or add any parameters you want on the Launch page and click the Launch button in the upper-right corner: Confirm the launch in the appeared pop-up. In the ACTIVE RUNS tab press the just-launched pipeline name. Wait until the SSH hyperlink will appear in the upper-right corner, click it: On the opened tab specify the command docker version and then press \"Enter\" key: As you can see, DinD works correctly. Example: Limit mounted storages By default, all available to a user storages are mounted to the launched container during the run initialization. User could have access to them via /cloud-data or ~/cloud-data folder using the interactive sessions (SSH/Web endpoints/Desktop) or pipeline runs. Note : to a user only storages are available for which he has READ permission. For more information see 13. Permissions . To limit the number of data storages being mounted to a specific pipeline run: Select a pipeline Select a pipeline version Navigate to the CONFIGURATION tab In the CONFIGURATION tab expand the Advanced section, click the field next to the \"Limit mounts\" label: In the pop-up select storages you want to mount during the run, e.g.: Confirm your choise by click the OK button Selected storages will appear in the field next to the \"Limit mounts\" label: Set \"Start idle\" checkbox and click the Save button: Click the Run button in the upper-right corner to launch the configuration. Edit or add any parameters you want on the Launch page and click the Launch button in the upper-right corner: Confirm the launch in the appeared pop-up. In the ACTIVE RUNS tab press the just-launched pipeline name. At the Run logs page expand the \"Parameters\" section: Here you can see IDs of the storages selected at step 5 Wait until the SSH hyperlink will appear in the upper-right corner, click it. On the opened tab specify the command ls cloud-data/ and then press \"Enter\" key: Here you can see the list of the mounted storages that is equal to the list of the selected storages at step 5. Other storages were not mounted. Each mounted storage is available for the interactive/batch jobs using the path /cloud-data/{storage_name} .","title":"6.1. Create and configure Pipeline"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#61-create-and-configure-pipeline","text":"Create a pipeline in a Library space Customize a pipeline version Edit documentation (optional) Edit code section Edit pipeline configuration (optional) Add/delete storage rules (optional) Edit a pipeline info Example: Create Pipeline Pipeline input data Pipeline output folder Configure the main_file Configure pipeline input/output parameters via GUI Check the results of pipeline execution Example: Add pipeline configuration Example: Create a configuration that uses system parameter Example: Limit mounted storages To create a Pipeline in a Folder you need to have WRITE permission for that folder and the ROLE_PIPELINE_MANAGER role. To edit pipeline you need just WRITE permissions for a pipeline. For more information see 13. Permissions . To create a working pipeline version you need: Create a pipeline in a Library space Customize a pipeline version: Edit documentation (optional) Edit Code file Edit Configuration, Add new configuration (optional) Add storage rules (optional) .","title":"6.1. Create and configure pipeline"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#create-a-pipeline-in-a-library-space","text":"Go to the \"Library\" tab and select a folder. Click + Create \u2192 Pipeline and choose one of the built-in pipeline templates ( Python , Shell , Snakemake , Luigi , WDL , Nextflow ) or choose DEFAULT item to create a pipeline without a template. Pipeline template defines the programming language for a pipeline. As templates are empty user shall write pipeline logic on his own. Enter pipeline's name (pipeline description is optional) in the popped-up form. Click the Create button. A new pipeline will appear in the folder. Note : To configure repository where to store pipeline versions click the Edit repository settings button. Click on the button and two additional fields will appear: Repository (repository address) and Token (password to access a repository). The new pipeline will appear in a Library space.","title":"Create a pipeline in a Library space"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#customize-a-pipeline-version","text":"Click a pipeline version to start its configuration process.","title":"Customize a pipeline version"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#edit-documentation-optional","text":"This option allows you to make a detailed description of your pipelines. Navigate to the Documents tab and: Click Edit . Change the document using a markdown language . Click the Save button. Enter a description of the change and click Commit . Changes are saved.","title":"Edit documentation (optional)"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#edit-code-section","text":"It is not optional because you need to create a pipeline that will be tailored to your specific needs. For that purpose, you need to extend basic pipeline templates/add new files. Navigate to the Code tab. Click on any file you want to edit. Note : each pipeline version has a default code file: it named after a pipeline and has a respective extension. A new window with file contents will open. Click the Edit button and change the code file in the desired way. When you are done, click the Save button. You'll be asked to write a Commit message (e.g. 'added second \"echo\" command'). Then click the Commit button. After that changes will be applied to your file. Note : all code files are downloaded to the node to run the pipeline. Just adding a new file to the Code section doesn't change anything. You need to specify the order of scripts execution by yourself. E.g. you have three files in your pipeline: first.sh ( main_file ), second.sh and config.json . cmd_template parameter is chmod +x $SCRIPTS_DIR/src/* $SCRIPTS_DIR/src/[main_file] . So in the first.sh file you need to explicitly specify execution of second.sh script for them both to run inside your pipeline, otherwise this file will be ignored.","title":"Edit code section"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#edit-pipeline-configuration-optional","text":"See details about pipeline configuration parameters here . Every pipeline has default pipeline configuration from the moment it was created. To change default pipeline configuration: Navigate to the Configuration tab. Expand \"Exec environment\" and \"Advanced\" tabs to see a full list of pipeline parameters. \"Parameters\" tab is opened by default. Change any parameter you need. In this example, we will set Cloud Region to Europe Ireland, Disk to 40 Gb and set the Timeout to 400 mins. Click the Save button. Now this will be the default pipeline configuration for the pipeline execution.","title":"Edit pipeline configuration (optional)"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#adddelete-storage-rules-optional","text":"This section allows configuring what data will be transferred to an STS after pipeline execution. To add a new rule: Click the Add new rule button. A pop-up will appear. Enter File mask and then tick the box \"Move to STS\" to move pipeline output data to STS after pipeline execution. Note : If many rules with different Masks are present all of them are checked one by one. If a file corresponds to any of rules - it will be uploaded to the bucket. To delete storage rule click the Delete button in the right part of the storage rule's row.","title":"Add/delete storage rules (optional)"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#edit-a-pipeline-info","text":"To edit a pipeline info: Click the Gear icon in the right upper corner of the pipeline page The popup with the pipeline info will be opened: Here you can edit pipeline name ( a ) and description ( b ) To edit repository settings click the corresponding button ( c ): Here you can edit access token to a repository ( d ) Note : the \"Repository\" field is disabled for the existing pipelines Click the SAVE button to save changes Note : if you rename a pipeline the corresponding GitLab repo will be automatically renamed too. So, the clone/pull/push URL will change. Make sure to change the remote address, if this pipeline is used somewhere. How it works: Open the pipeline: Click the GIT REPOSITORY button in the right upper corner of the page: Pipeline name and repository name are identical Click the Gear icon in the right upper corner. In the popup change pipeline name and click the SAVE button: Click the GIT REPOSITORY button again: Pipeline name and repository name are identical Also, if you want just rename a pipeline without changing its other info fields: Hover over the pipeline name at the \"breadcrumbs\" control in the top of the pipeline page - the \"edit\" symbol will appear: Click the pipeline name - the field will become available to edit. Rename the pipeline: Press the Enter key or click any empty space - a new pipeline name will be saved:","title":"Edit a pipeline info"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#example-create-pipeline","text":"We will create a simple Shell pipeline (Shell template used). For that purpose, we will click + Create \u2192 Pipeline \u2192 SHELL . Then we will write Pipeline name ( 1 ), Pipeline description ( 2 ) and click Create ( 3 ). This pipeline will: Download a file. Rename it. Upload renamed the file to the bucket.","title":"Example: Create Pipeline"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#pipeline-input-data","text":"This is where pipeline input data is stored. About storages see here . This path will be used in pipeline parameters later on.","title":"Pipeline input data"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#pipeline-output-folder","text":"This is where pipeline output data will be stored after pipeline execution. About storages see here . This path will be used in pipeline parameters later on.","title":"Pipeline output folder"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#configure-the-main_file","text":"The pipeline will consist of 2 files: main_file and config.json . Let's extend the main_file so that it renames the input file and puts it into the $ANALYSIS_DIR folder on the node from which data will be uploaded to the bucket. To do that click the main_file name and click the Edit button. Then type all the pipeline instructions. Click the Save button, input a commit message and click the Commit button.","title":"Configure the main_file"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#configure-pipeline-inputoutput-parameters-via-gui","text":"Click the Run button. In the pipeline run configuration select the arrow near the Add parameter button and select the \"Input path parameter\" option from the drop-down list. Name the parameter (e.g. \"input\") and click on the grey \"download\" icon to select the path to the pipeline input data (we described pipeline input data above ). For pipeline output folder parameter choose the \"Output path parameter\" option from the drop-down list, name it and click on the grey \"upload\" icon to select the path to the pipeline output folder (we described pipeline output data above ). This is how everything looks after these parameters are set: Leave all other parameters default and click the Launch button.","title":"Configure pipeline input/output parameters via GUI"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#check-the-results-of-pipeline-execution","text":"After pipeline finished its execution, you can find the renamed file in the output folder:","title":"Check the results of pipeline execution"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#example-add-pipeline-configuration","text":"In this example, we will create a new pipeline configuration for the example pipeline and set it as default one. To add new pipeline configuration perform the following steps: Select a pipeline Select a pipeline version Navigate to the CONFIGURATION tab Click the + ADD button in the upper-right corner of the screen Specify Configuration name , Description (optionally) and the Template - this is a pipeline configuration, from which the new pipeline configuration will inherit its parameters (right now only the \"default\" template is available). Click the Create button. As you can see, the new configuration has the same parameters as the default configuration. Use Delete ( 1 ), Set as default ( 2 ) or Save ( 3 ) buttons to delete, set as default or save this configuration respectively. Expand the Exec environment section ( 1 ) and then Specify 30 GB Disk size ( 2 ), click the control to choose another Docker image ( 3 ). Click the Save button ( 4 ). Set \"new-configuration\" as default with the Set as default button. Navigate to the CODE tab. As you can see, config.json file now contains information about two configurations: \"default\" and \"new-configuration\". \"new-configuration\" is default one for pipeline execution.","title":"Example: Add pipeline configuration"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#example-create-a-configuration-that-uses-system-parameter","text":"Users can specify system parameters (per run), that change/configure special behavior for the current run. In the example below we will use the system parameter, that installs and allows using of the DIND (Docker in Docker) in the launched container: Select a pipeline Select a pipeline version Navigate to the CONFIGURATION tab In the CONFIGURATION tab expand the Advanced section, set \"Start idle\" checkbox and click the Add system parameter button: Click the CP_CAP_DIND_CONTAINER option and then click the OK button: This option will enable docker engine for a run using a containerized approach. Added system parameter appears on the configuration page. Save the configuration - now it will use \"Docker inside Docker\" technology while running: To see it, click the Run button in the upper-right corner to launch the configuration. Edit or add any parameters you want on the Launch page and click the Launch button in the upper-right corner: Confirm the launch in the appeared pop-up. In the ACTIVE RUNS tab press the just-launched pipeline name. Wait until the SSH hyperlink will appear in the upper-right corner, click it: On the opened tab specify the command docker version and then press \"Enter\" key: As you can see, DinD works correctly.","title":"Example: Create a configuration that\u00a0uses system parameter"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#example-limit-mounted-storages","text":"By default, all available to a user storages are mounted to the launched container during the run initialization. User could have access to them via /cloud-data or ~/cloud-data folder using the interactive sessions (SSH/Web endpoints/Desktop) or pipeline runs. Note : to a user only storages are available for which he has READ permission. For more information see 13. Permissions . To limit the number of data storages being mounted to a specific pipeline run: Select a pipeline Select a pipeline version Navigate to the CONFIGURATION tab In the CONFIGURATION tab expand the Advanced section, click the field next to the \"Limit mounts\" label: In the pop-up select storages you want to mount during the run, e.g.: Confirm your choise by click the OK button Selected storages will appear in the field next to the \"Limit mounts\" label: Set \"Start idle\" checkbox and click the Save button: Click the Run button in the upper-right corner to launch the configuration. Edit or add any parameters you want on the Launch page and click the Launch button in the upper-right corner: Confirm the launch in the appeared pop-up. In the ACTIVE RUNS tab press the just-launched pipeline name. At the Run logs page expand the \"Parameters\" section: Here you can see IDs of the storages selected at step 5 Wait until the SSH hyperlink will appear in the upper-right corner, click it. On the opened tab specify the command ls cloud-data/ and then press \"Enter\" key: Here you can see the list of the mounted storages that is equal to the list of the selected storages at step 5. Other storages were not mounted. Each mounted storage is available for the interactive/batch jobs using the path /cloud-data/{storage_name} .","title":"Example: Limit mounted storages"},{"location":"manual/06_Manage_Pipeline/6.2._Launch_a_pipeline/","text":"6.2. Launch a pipeline To launch a pipeline you need to have EXECUTE permissions for the pipeline. For more information see 13. Permissions . Also you can launch a pipeline via CLI. See 14.5 Manage pipeline executions via CLI . Select a pipeline in the \" Library \" menu ( 3. Overview ). Select a pipeline version to run. Click the Run button. Launch pipeline page will be opened: Feel free to change settings of run configuration if you need to. See an example of editing configuration here . If the Price type is set as \" On-demand \" - at the Launch page , an additional checkbox Auto pause appears: This checkbox allows to enable automatic pausing on-demand instance if it is not used. Such behavior could be controlled by Administrators using a set of parameters at System Preferences (see here ). Please note, this checkbox will not be displayed if any cluster is configured (\"Static\" or \"Autoscaled\"). If the Price type is set as \" On-demand \" - at the Launch page , an additional control Maintenance appears. It allows to configure schedule for automatical pause/resume a pipeline run. It could be useful when the pipeline is launched for a long time (several days/weeks) but it shall not stand idle, just increasing costs, in weekends and holidays, for example. Schedule is defined as a list of rules (user is able to specify any number of them). For each rule in the list the user is able to set: the action: PAUSE or RESUME the recurrence: Daily : every N days, time or Weekly : every weekday(s) , time Conflicting rules are not allowed (i.e. rules that are configured on the same execution time). If any schedule rule is configured for the launched active run - that run will be paused/restarted accordingly in the scheduled day and time. To set a schedule for pause/restart a job: Click the Configure button: The \"Maintenance\" popup will appear: Click the Add rule button. The first rule will appear: Using available controls configure the rule according to your wish, e.g. to automatically pause a job every 2 days at 15:00 : To add another rule click the Add rule button. Configure a new rule using available controls, e.g. to automatically restart (resume) a job every monday and friday at 18:30 : Click the OK button to save specified rules Saved rules will be displayed at the Launch form: Please note, the Maintenance control will not be displayed if any cluster is configured (\"Static\" or \"Autoscaled\"). Users (who have permissions to pause/resume a run) can create/view/modify/delete schedule rules anytime launched run is active via the Run logs page - for more details see 11. Manage runs . Click Launch . Please note, that the current user can launch a pipeline only if he/his group has corresponding permissions on that pipeline (for more information see 13. Permissions ), but the Launch button may be disabled also for one of the following reasons: execution isn't allowed for specified docker image; read operations aren't allowed for specified input or common path parameters; write operations aren't allowed for specified output path parameters. In such cases, hover over the Launch button to view warning notification with a reason of a run forbiddance, e.g.: Note : you can also launch a pipeline with the same settings via the CLI command or API request. To generate the corresponding command/request click the button near the \"Launch\" button. For more details see here . Confirm launch in the appeared popup. You'll be redirected to the \"Runs\" area. Here you'll find your pipeline running. You can monitor status of your run and see additional information (see 11. Manage Runs ). Note : after some initialization time, a new node will appear in the \" Cluster nodes \" tab. See 9. Manage Cluster nodes . Note : to learn about launching a pipeline as an Interactive service, refer to 15. Interactive services .","title":"6.2. Launch a Pipeline"},{"location":"manual/06_Manage_Pipeline/6.2._Launch_a_pipeline/#62-launch-a-pipeline","text":"To launch a pipeline you need to have EXECUTE permissions for the pipeline. For more information see 13. Permissions . Also you can launch a pipeline via CLI. See 14.5 Manage pipeline executions via CLI . Select a pipeline in the \" Library \" menu ( 3. Overview ). Select a pipeline version to run. Click the Run button. Launch pipeline page will be opened: Feel free to change settings of run configuration if you need to. See an example of editing configuration here . If the Price type is set as \" On-demand \" - at the Launch page , an additional checkbox Auto pause appears: This checkbox allows to enable automatic pausing on-demand instance if it is not used. Such behavior could be controlled by Administrators using a set of parameters at System Preferences (see here ). Please note, this checkbox will not be displayed if any cluster is configured (\"Static\" or \"Autoscaled\"). If the Price type is set as \" On-demand \" - at the Launch page , an additional control Maintenance appears. It allows to configure schedule for automatical pause/resume a pipeline run. It could be useful when the pipeline is launched for a long time (several days/weeks) but it shall not stand idle, just increasing costs, in weekends and holidays, for example. Schedule is defined as a list of rules (user is able to specify any number of them). For each rule in the list the user is able to set: the action: PAUSE or RESUME the recurrence: Daily : every N days, time or Weekly : every weekday(s) , time Conflicting rules are not allowed (i.e. rules that are configured on the same execution time). If any schedule rule is configured for the launched active run - that run will be paused/restarted accordingly in the scheduled day and time. To set a schedule for pause/restart a job: Click the Configure button: The \"Maintenance\" popup will appear: Click the Add rule button. The first rule will appear: Using available controls configure the rule according to your wish, e.g. to automatically pause a job every 2 days at 15:00 : To add another rule click the Add rule button. Configure a new rule using available controls, e.g. to automatically restart (resume) a job every monday and friday at 18:30 : Click the OK button to save specified rules Saved rules will be displayed at the Launch form: Please note, the Maintenance control will not be displayed if any cluster is configured (\"Static\" or \"Autoscaled\"). Users (who have permissions to pause/resume a run) can create/view/modify/delete schedule rules anytime launched run is active via the Run logs page - for more details see 11. Manage runs . Click Launch . Please note, that the current user can launch a pipeline only if he/his group has corresponding permissions on that pipeline (for more information see 13. Permissions ), but the Launch button may be disabled also for one of the following reasons: execution isn't allowed for specified docker image; read operations aren't allowed for specified input or common path parameters; write operations aren't allowed for specified output path parameters. In such cases, hover over the Launch button to view warning notification with a reason of a run forbiddance, e.g.: Note : you can also launch a pipeline with the same settings via the CLI command or API request. To generate the corresponding command/request click the button near the \"Launch\" button. For more details see here . Confirm launch in the appeared popup. You'll be redirected to the \"Runs\" area. Here you'll find your pipeline running. You can monitor status of your run and see additional information (see 11. Manage Runs ). Note : after some initialization time, a new node will appear in the \" Cluster nodes \" tab. See 9. Manage Cluster nodes . Note : to learn about launching a pipeline as an Interactive service, refer to 15. Interactive services .","title":"6.2. Launch a pipeline"},{"location":"manual/06_Manage_Pipeline/6.3._Delete_a_pipeline/","text":"6.3. Delete and unregister Pipeline Delete pipeline Unregister pipeline To delete a Pipeline you need to have WRITE permission for that pipeline and the ROLE_PIPELINE_MANAGER role. For more details see 13. Permissions . Delete pipeline Select a pipeline. Click the Gear icon in the right upper corner of the pipeline page: In the popup click the DELETE button: You will be offered to unregister or delete a pipeline. Click the Delete button to remove the pipeline permanently: Unregister pipeline A user can unregister pipeline. Git repository neither will be deleted nor will be accessible in Cloud Pipeline. Do the same actions as in the Delete pipeline section but click the Unregister button at step 4 : If you want to register in the Cloud Pipeline the pipeline that was previously unregistered: Start create a new pipeline. For details see here . At the \"Create pipeline\" popup click the \"Edit repository settings\" button. Into the \"Repository\" field specify the git repository address of the unregistered pipeline: Specify a pipeline name. Click the CREATE button. The re-registered pipeline will appear in the library.","title":"6.3. Delete and unregister Pipeline"},{"location":"manual/06_Manage_Pipeline/6.3._Delete_a_pipeline/#63-delete-and-unregister-pipeline","text":"Delete pipeline Unregister pipeline To delete a Pipeline you need to have WRITE permission for that pipeline and the ROLE_PIPELINE_MANAGER role. For more details see 13. Permissions .","title":"6.3. Delete and unregister Pipeline"},{"location":"manual/06_Manage_Pipeline/6.3._Delete_a_pipeline/#delete-pipeline","text":"Select a pipeline. Click the Gear icon in the right upper corner of the pipeline page: In the popup click the DELETE button: You will be offered to unregister or delete a pipeline. Click the Delete button to remove the pipeline permanently:","title":"Delete pipeline"},{"location":"manual/06_Manage_Pipeline/6.3._Delete_a_pipeline/#unregister-pipeline","text":"A user can unregister pipeline. Git repository neither will be deleted nor will be accessible in Cloud Pipeline. Do the same actions as in the Delete pipeline section but click the Unregister button at step 4 : If you want to register in the Cloud Pipeline the pipeline that was previously unregistered: Start create a new pipeline. For details see here . At the \"Create pipeline\" popup click the \"Edit repository settings\" button. Into the \"Repository\" field specify the git repository address of the unregistered pipeline: Specify a pipeline name. Click the CREATE button. The re-registered pipeline will appear in the library.","title":"Unregister pipeline"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/","text":"6. Manage Pipeline Pipeline object GUI \"Details\" view pane \"Details\" controls Pipeline versions GUI Pipeline controls Pipeline launching page Pipeline version tabs DOCUMENTS CODE CONFIGURATION HISTORY STORAGE RULES GRAPH Default environment variables Pipelines represent a sequence of tasks that are executed along with each other in order to process some data. They help to automate complex tasks that consist of many sub-tasks. This chapter describes Pipeline space GUI and the main working scenarios. Pipeline object GUI As far as the pipeline is one of CP objects which stored in \" Library \" space, the Pipeline workspace is separated into two panes: \"Hierarchy\" view pane \"Details\" view pane. Note : also you can view general information and some details about the specific pipeline via CLI. See 14.4 View pipeline definitions via CLI . \"Details\" view pane The \"Details\" view pane displays content of a selected object. In case of a pipeline, you will see: a list of pipeline versions with a description of last update and date of the last update; specific space's controls . \"Details\" controls Control Description Displays icon This icon includes: \" Attributes \" control ( 1 ) opens Attributes pane. Here you can see a list of \"key=value\" attributes of the pipeline. For more info see here . Note : If the selected pipeline has any defined attribute, Attributes pane is shown by default. Issues shows/hides the issues of the current pipeline to discuss. To learn more see here . \"Gear\" icon This control ( 2 ) allows to rename, change the description, delete and edit repository settings i.e. Repository address and Token Git repository \" Git repository \" control ( 3 ) shows a git repository address where pipeline versions are stored, which could be copied and pasted into a browser address field: If for your purposes ssh protocol is required, you may click the HTTPS/SSH selector and choose the SSH item: In that case, you will get a reformatted SSH address: Also a user can work directly with git from the console on the running node. For more information, how to configure Git client to work with the Cloud Pipeline, see here . To clone a pipeline a user shall have READ permissions, to push WRITE permission are also needed. For more info see 13. Permissions . Release \" Release \" control ( 4 ) is used to tag a particular pipeline version with a name. A draft pipeline version has the control only. Note : you can edit the last pipeline version only. Run Each pipeline version item of the selected pipeline's list has a \" Run \" control ( 5 ) to launch a pipeline version. Pipeline versions GUI Pipeline version interface displays full information about a pipeline version: supporting documentation, code files, and configurations, history of version runnings, etc. Pipeline controls The following buttons are available to manage this space. Control Description Run This button launch a pipeline version. When a user clicks the button, the \"Launch a pipeline\" page opens. \"Gear\" icon This control allows to rename, change the description, delete and edit repository settings i.e. Repository address and Token . Git repository Shows a git repository address where pipeline versions are stored, which could be copied and pasted in a browser line. Also a user can work directly with git from the console on the running node. For more information, how to configure Git client to work with the Cloud Pipeline, see here . To clone a pipeline a user shall have READ permissions, to push WRITE permission is also needed. For more info see 13. Permissions . Pipeline launching page \" Launch a pipeline \" page shows parameters of a default configuration the pipeline version. This page has the same view as a \"Configuration\" tab of a pipeline version. Here you can select any other configuration from the list and/or change parameters for this specific run (changes in configuration will be applied only to this specific run). Pipeline version tabs Pipeline version space dramatically differs from the Pipeline space. You can open it: just click on it. The whole information is organized into the following tabs in \"Details\" view pane. DOCUMENTS The \"Documents\" tab contains documentation associated with the pipeline, e.g. README, pipeline description, etc. See an example here . Note : README.md file is created automatically and contains default text which could be easily edited by a user. Documents tab controls The following buttons are available to manage this section: Control Description Upload ( a ) This control ( a ) allows to upload documentation files. Delete ( b ) \"Delete\" control ( b ) helps to delete a file. Rename ( c ) To rename a file a user shall use a \"Rename\" control ( c ). Download ( d ) This control ( d ) allows downloading pipeline documentation file to your local machine. Edit ( e ) \"Edit\" control ( e ) helps a user to edit any text files (e.g. README) here in a text editor using a markdown language . CODE This section contains a list of scripts to run a pipeline. Here you can create new files, folders and upload files here. Each script file could be edited (see details here ). Note : .json configuration file can also be edited in the Configuration tab via GUI. Code tab controls The following controls are available: Control Description Plus button ( a ) This control is to create a new folder in a pipeline version. The folder's name shall be specified. + New file ( b ) To create a new file in the current folder. Upload ( c ) To upload files from your local file system to a pipeline version. Rename ( d ) Each file or folder has a \"Rename\" control which allows renaming a file/folder. Delete ( e ) Each file or folder has a \"Delete\" control which deletes a file/folder. The list of system files All newly created pipelines have at least 2 starting files no matter what pipeline template you've chosen. Only newly created DEFAULT pipeline has 1 starting file ( config.json ). main_file This file contains a pipeline scenario. By default, it is named after a pipeline, but this may be changed in the configuration file. Note : the main_file is usually an entry point to start pipeline execution. To create your own scenario the default template of the main file shall be edited (see details here ). Example : below is the piece of the main_file of the Gromacs pipeline: config.json This file contains pipeline execution parameters. You can not rename or delete it because of it's used in pipeline scripts and they will not work without it. Note : it is advised that pipeline execution settings are modified via CONFIGURATION tab (e.g. if you want to change default settings for pipeline execution) or via Launch pipeline page (e.g. if you want to change pipeline settings for a current run). Manual config.json editing should be used only for advanced users (primarily developers) since json format is not validated in this case. Note : all attributes from config.json are available as environment variables for pipeline execution. The config.json file for every pipeline template have the following settings: Setting Description main_file A name of the main file for that pipeline. instance_size instance type in terms of the specific Cloud Provider that specifies an amount of RAM in Gb, CPU and GPU cores number (e.g. \"m4.xlarge\" for AWS EC2 instance). instance_disk An instance's disk size in Gb. docker_image A name of the Docker image that will be used in the current pipeline. cmd_template Command line template that will be executed at the running instance in the pipeline. cmd_template can use environment variables: To address the main_file parameter value, use the following construction - [main_file] To address all other parameters, usual Linux environment variables style shall be used (e.g. $docker_image ) parameters Pipeline execution parameters (e.g. path to the data storage with input data). A parameter has a name and set of attributes. There are 3 possible keys for each parameter: \"type\" - key specifies a type for current parameter, \"value\" - key specifies default value for parameter, \"required\" - key specifies whether this parameter must be set ( \"required\": true ) or might not ( \"required\": false ) Example : config.json file of the Gromacs pipeline: Note : In addition to main_file and config.json you can add any number of files to the CODE section and combine it in one whole scenario. CONFIGURATION This section represents pipeline execution parameters which are set in config.json file. The parameters can be changed here and config.json file will be changed respectively. See how to edit configuration here . A configuration specifies: Section Control Description Name Pipeline and its configuration names. Estimated price per hour Control shows machine hours prices. If you navigate mouse to \"info\" icon, you'll see the maximum, minimum and average price for a particular pipeline version run as well as the price per hour. Exec environment This section lists execution environment parameters. Docker image A name of a Docker image to use for a pipeline execution (e.g. \"library/gromacs-gpu\"). Node type An instance type in terms of the specific Cloud Provider: CPU, RAM, GPU (e.g. 2 CPU cores, 8 Gb RAM, 0 GPU cores). Disk Size of a disk in gigabytes, that will be attached to the instance in Gb. Configure cluster button On-click, pop-up window will be shown: Here you can configure cluster or auto-scaled cluster. Cluster is a collection of instances which are connected so that they can be used together on a task. In both cases, a number of additional worker nodes with some main node as cluster head are launching (total number of pipelines = \"number of working nodes\" + 1). See v.0.14 - 7.2. Launch Detached Configuration for details. In case of using cluster , an exact count of worker nodes is directly defined by the user before launching the task and could not changing during the run. In case of using auto-scaled cluster , a max count of worker nodes is defined by the user before launching the task but really used count of worker nodes can change during the run depending on the jobs queue load. See Appendix C. Working with autoscaled cluster runs for details. For configure cluster: in opened window click Cluster button specify a number of child nodes (workers' count) if you want to use GridEngine server for the cluster, tick the Enable GridEngine checkbox. Setting of that checkbox automatically adds the CP_CAP_SGE system parameter with value true . Note : you may set this and other system parameters manually - see the example of using system parameters here . if you want to use Apache Spark for the cluster, tick the Enable Apache Spark checkbox. Setting of that checkbox automatically adds the CP_CAP_SPARK system parameter with value true . See the example of using Apache Spark here . if you want to use Slurm for the cluster, tick the Enable Slurm checkbox. Setting of that checkbox automatically adds the CP_CAP_SLURM system parameter with value true . See the example of using Slurm here . click OK button: When user selects Cluster option, information on total cluster resources is shown. Resources are calculated as (CPU/RAM/GPU)*(NUMBER_OF_WORKERS+1) : For configure auto-scaled cluster : in opened window click Auto-scaled cluster button specify a number of child nodes (workers' count) in field Auto-scaled up to and click OK button: Note : that number is meaning total count of \"auto-scaled\" nodes - it is the max count of worker nodes that could be attached to the main node to work together as cluster. These nodes will be attached to the cluster only in case if some jobs are in waiting state longer than a specific time. Also these nodes will be dropped from the cluster in case when jobs queue is empty or all jobs are running and there are some idle nodes longer than a specific time. Note : about timeout periods for scale-up and scale-down of auto-scaled cluster see here . additionally you may enable hybrid mode for the auto-scaled cluster - it allows to scale-up the cluster (attach a worker node) with the instance type distinct of the master - worker is being picked up based on the amount of unsatisfied CPU requirements of all pending jobs. For that behavior, set the \" Enable Hybrid cluster \" checkbox. For more details see here . additionally you may specify a number of \"persistent\" child nodes (workers' count) - click Setup default child nodes count , input Default child nodes number and click Ok button: These default child nodes will be never \"scaled-down\" during the run regardless of jobs queue load. In the example above, total count of \"auto-scaled\" nodes - 3, and 1 of them is \"persistent\". Note : total count of child nodes always must be greater than count of default (\"persistent\") child nodes. if you don't want to use default (\"persistent\") child nodes in your auto-scaled cluster - click Reset button opposite the Default child nodes field. additionally you may choose a price type for workers that will be attached during the run - via the \" Workers price type \" dropdown list - workers' price type can be automatically the same as the master node type (by default) or forcibly specified regardless on the master's type: When user selects Auto-scaled cluster , information on total cluster resources is shown as interval - from the \"min\" configuration to \"max\" configuration: \"min\" configuration resources are calculated as (CPU/RAM/GPU)*(NUMBER_OF_DEFAULT_WORKERS+1) \"max\" configuration resources are calculated as (CPU/RAM/GPU)*(TOTAL_NUMBER_OF_WORKERS+1) E.g. for auto-scaled cluster with 2 child nodes and without default (\"persistent\") child nodes (NUMBER_OF_DEFAULT_WORKERS = 0; TOTAL_NUMBER_OF_WORKERS = 2) : E.g. for auto-scaled cluster with 2 child nodes and 1 default (\"persistent\") child node (NUMBER_OF_DEFAULT_WORKERS = 1; TOTAL_NUMBER_OF_WORKERS = 2) : Note : in some specific configurations such as hybrid autoscaling clusters amount of resources can vary beyond the shown interval. Note : if you don't want to use any cluster - click Single node button and then click OK button. Cloud Region A specific region for a compute node placement. Please note, if a non-default region is selected - certain CP features may be unavailable: FS mounts usage from the another region (e.g. \" EU West \" region cannot use FS mounts from the \" US East \"). Regular storages will be still available If a specific tool, used for a run, requires an on-premise license server (e.g. monolix, matlab, schrodinger, etc.) - such instances shall be run in a region, that hosts those license servers. Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - in that control Cloud Provider auxiliary icons also will be displayed, e.g.: For a single-Provider deployments only Cloud Region icons are displayed. Advanced Price type Choose Spot or On-demand type of instance. You can look information about price types hovering \"Info\" icon and based on it make your choice. Timeout (min) After this time pipeline will shut down (optional). Limit mounts Allow to specify storages that should be mounted. See here . Cmd template A shell command that will be executed to start a pipeline. \"Start idle\" The flag sets cmd_template to sleep infinity . For more information about starting a job in this mode refer to 15. Interactive services . Parameters This section lists pipeline specific parameters that can be used during a pipeline run. Pipeline parameters can be assigned the following types: String - generic scalar value (e.g. Sample name). Boolean - boolean value. Path - path in a data storage hierarchy. Input - path in a data storage hierarchy. During pipeline initialization, this path will be used to download input data on the calculation node for processing from a storage. Output - path in a data storage hierarchy. During pipeline finalization, this path will be used to upload resulting data to a storage. Common - path in a data storage hierarchy. Similar to \"Input\" type, but this data will not be erased from a calculation node, when a pipeline is finished (this is useful for reference data, as it can be reused by further pipeline runs that share the same reference). Note : You can use Project attribute values as parameters for the Run : Click an empty parameter value field. Enter \" project. \" In the drop-down list select the Project attribute value: Add parameter This control helps to add an additional parameter to a configuration. Configuration tab controls Control Description Add To create a customized configuration for the pipeline, click the + ADD button in the upper-right corner of the screen. For more details see here . Save This button saves changes in a configuration. HISTORY This section contains information about all the current pipeline version's runs. Runs info is organized into a table with the following columns: Run - each record of that column contains two rows: in upper - run name that consists of pipeline name and run id, in bottom - Cloud Region. Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - corresponding text information also has a Provider name, e.g.: Parent-run - id of the run that executed current run (this field is non-empty only for runs that are executed by other runs). Pipeline - each record of that column contains two rows: in upper - pipeline name, in bottom - pipeline version. Docker image - base docker image name. Started - time pipeline started running. Completed - time pipeline finished execution. Elapsed - each record of that column contains two rows: in upper - pipeline running time, in bottom - run's estimated price, which is calculated based on the run duration, region and instance type. Owner - user who launched run. You can filter runs by clicking the filter icon . By using the filter control you can choose whether display runs for current pipeline version or display runs for all pipeline versions. History tab controls Control Description PAUSE (a) To pause running pipeline press this control. This control is available only for on-demand instances. STOP (b) To stop running pipeline press this control. LOG (c) \"Log\" control opens detailed information about the run. You'll be redirected to \" Runs \" space (see 11. Manage Runs ). RESUME (d) To resume pausing pipeline press this control. This control is available only for on-demand instances. TERMINATE (e) To terminate node without waiting of the pipeline resuming. This control is available only for on-demand instances, which were paused. RERUN (f) This control reruns completed pipeline's runs. Pipeline run's states Icons at the left represent the current state of the pipeline runs: - Queued state (\"sandglass\" icon) - a run is waiting in the queue for the available compute node. - Initializing state (\"rotating\" icon) - a run is being initialized. - Pulling state (\"download\" icon) - now pipeline Docker image is downloaded to the node. - Running state (\"play\" icon) - a pipeline is running. The node is appearing and pipeline input data is being downloaded to the node before the \" InitializeEnvironment \" service task appears. - Paused state (\"pause\" icon) - a run is paused. At this moment compute node is already stopped but keeps it's state. Such run may be resumed. - Success state (\"OK\" icon) - successful pipeline execution. - Failed state (\"caution\" icon) - unsuccessful pipeline execution. - Stopped state (\"clock\" icon) - a pipeline manually stopped. Also, help tooltips are provided when hovering a run state icon, e.g.: STORAGE RULES This section displays a list of rules used to upload data to the output data storage, once pipeline finished. It helps to store only data you need and minimize the amount of interim data in data storages. Info is organized into a table with the following columns: Mask column contains a relative path from the $ANALYSIS_DIR folder (see Default environment variables section below for more information). Mask uses bash syntax to specify the data that you want to upload from the $ANALYSIS_DIR. Data from the specified path will be uploaded to the bucket from the pipeline node. Note : by default whole $ANALYSIS_DIR folder is uploaded to the cloud bucket (default Mask is - \"*\"). For example, \"*.txt*\" mask specifies that all files with .txt extension need to be uploaded from the $ANALYSIS_DIR to the data storage. Note : Be accurate when specifying masks - if wildcard mask (\"*\") is specified, all files will be uploaded, no matter what additional masks are specified. The Created column shows date and time of rules creation. Move to Short-Term Storage column indicates whether pipeline output data will be moved to a short-term storage. Storage rules tab control Control Description Add new rule (a) This control allows adding a new data managing rule. Delete (b) To delete a data managing rule press this control. GRAPH This section represents the sequence of pipeline tasks as a directed graph. Tasks are graph vertices, edges represent execution order. A task can be executed only when all input edges - associated tasks - are completed (see more information about creating a pipeline with GRAPH section here ). Note : only for Luigi and WDL pipelines. Note : If main_file has mistakes, pipeline workflow won't be visualized. Graph tab controls When a PipelineBuilder graph is loaded, the following layout controls become available to the user. Control Description Save saves changes. Revert reverts all changes to the last saving. Layout performs graph linearization, make it more readable. Fit zooms graph to fit the screen. Show links enables/disables workflow level links to the tasks. It is disabled by default, as for large workflows it overwhelms the visualization. Zoom out zooms graph out. Zoom in zooms graph in. Search element allows to find specific object at the graph. Fullscreen expands graph to the full screen. Default environment variables Pipeline scripts (e.g. main_file ) use default environmental variables for pipeline execution. These variables are set in internal CP scripts: RUN_ID - pipeline run ID. PIPELINE_NAME - pipeline name. COMMON_DIR - directory where pipeline common data (parameter with \"type\": \"common\" ) will be stored. ANALYSIS_DIR - directory where output data of the pipeline (parameter with \"type\": \"output\" ) will be stored. INPUT_DIR - directory where input data of the pipeline (parameter with \"type\": \"input\" ) will be stored. SCRIPTS_DIR - directory where all pipeline scripts and config.json file will be stored.","title":"6.0. Overview"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#6-manage-pipeline","text":"Pipeline object GUI \"Details\" view pane \"Details\" controls Pipeline versions GUI Pipeline controls Pipeline launching page Pipeline version tabs DOCUMENTS CODE CONFIGURATION HISTORY STORAGE RULES GRAPH Default environment variables Pipelines represent a sequence of tasks that are executed along with each other in order to process some data. They help to automate complex tasks that consist of many sub-tasks. This chapter describes Pipeline space GUI and the main working scenarios.","title":"6. Manage Pipeline"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#pipeline-object-gui","text":"As far as the pipeline is one of CP objects which stored in \" Library \" space, the Pipeline workspace is separated into two panes: \"Hierarchy\" view pane \"Details\" view pane. Note : also you can view general information and some details about the specific pipeline via CLI. See 14.4 View pipeline definitions via CLI .","title":"Pipeline object GUI"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#details-view-pane","text":"The \"Details\" view pane displays content of a selected object. In case of a pipeline, you will see: a list of pipeline versions with a description of last update and date of the last update; specific space's controls .","title":"\"Details\" view pane"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#details-controls","text":"Control Description Displays icon This icon includes: \" Attributes \" control ( 1 ) opens Attributes pane. Here you can see a list of \"key=value\" attributes of the pipeline. For more info see here . Note : If the selected pipeline has any defined attribute, Attributes pane is shown by default. Issues shows/hides the issues of the current pipeline to discuss. To learn more see here . \"Gear\" icon This control ( 2 ) allows to rename, change the description, delete and edit repository settings i.e. Repository address and Token Git repository \" Git repository \" control ( 3 ) shows a git repository address where pipeline versions are stored, which could be copied and pasted into a browser address field: If for your purposes ssh protocol is required, you may click the HTTPS/SSH selector and choose the SSH item: In that case, you will get a reformatted SSH address: Also a user can work directly with git from the console on the running node. For more information, how to configure Git client to work with the Cloud Pipeline, see here . To clone a pipeline a user shall have READ permissions, to push WRITE permission are also needed. For more info see 13. Permissions . Release \" Release \" control ( 4 ) is used to tag a particular pipeline version with a name. A draft pipeline version has the control only. Note : you can edit the last pipeline version only. Run Each pipeline version item of the selected pipeline's list has a \" Run \" control ( 5 ) to launch a pipeline version.","title":"\"Details\" controls"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#pipeline-versions-gui","text":"Pipeline version interface displays full information about a pipeline version: supporting documentation, code files, and configurations, history of version runnings, etc.","title":"Pipeline versions GUI"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#pipeline-controls","text":"The following buttons are available to manage this space. Control Description Run This button launch a pipeline version. When a user clicks the button, the \"Launch a pipeline\" page opens. \"Gear\" icon This control allows to rename, change the description, delete and edit repository settings i.e. Repository address and Token . Git repository Shows a git repository address where pipeline versions are stored, which could be copied and pasted in a browser line. Also a user can work directly with git from the console on the running node. For more information, how to configure Git client to work with the Cloud Pipeline, see here . To clone a pipeline a user shall have READ permissions, to push WRITE permission is also needed. For more info see 13. Permissions .","title":"Pipeline controls"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#pipeline-launching-page","text":"\" Launch a pipeline \" page shows parameters of a default configuration the pipeline version. This page has the same view as a \"Configuration\" tab of a pipeline version. Here you can select any other configuration from the list and/or change parameters for this specific run (changes in configuration will be applied only to this specific run).","title":"Pipeline launching page"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#pipeline-version-tabs","text":"Pipeline version space dramatically differs from the Pipeline space. You can open it: just click on it. The whole information is organized into the following tabs in \"Details\" view pane.","title":"Pipeline version tabs"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#documents","text":"The \"Documents\" tab contains documentation associated with the pipeline, e.g. README, pipeline description, etc. See an example here . Note : README.md file is created automatically and contains default text which could be easily edited by a user.","title":"DOCUMENTS"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#documents-tab-controls","text":"The following buttons are available to manage this section: Control Description Upload ( a ) This control ( a ) allows to upload documentation files. Delete ( b ) \"Delete\" control ( b ) helps to delete a file. Rename ( c ) To rename a file a user shall use a \"Rename\" control ( c ). Download ( d ) This control ( d ) allows downloading pipeline documentation file to your local machine. Edit ( e ) \"Edit\" control ( e ) helps a user to edit any text files (e.g. README) here in a text editor using a markdown language .","title":"Documents tab controls"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#code","text":"This section contains a list of scripts to run a pipeline. Here you can create new files, folders and upload files here. Each script file could be edited (see details here ). Note : .json configuration file can also be edited in the Configuration tab via GUI.","title":"CODE"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#code-tab-controls","text":"The following controls are available: Control Description Plus button ( a ) This control is to create a new folder in a pipeline version. The folder's name shall be specified. + New file ( b ) To create a new file in the current folder. Upload ( c ) To upload files from your local file system to a pipeline version. Rename ( d ) Each file or folder has a \"Rename\" control which allows renaming a file/folder. Delete ( e ) Each file or folder has a \"Delete\" control which deletes a file/folder.","title":"Code tab controls"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#the-list-of-system-files","text":"All newly created pipelines have at least 2 starting files no matter what pipeline template you've chosen. Only newly created DEFAULT pipeline has 1 starting file ( config.json ).","title":"The list of system files"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#main_file","text":"This file contains a pipeline scenario. By default, it is named after a pipeline, but this may be changed in the configuration file. Note : the main_file is usually an entry point to start pipeline execution. To create your own scenario the default template of the main file shall be edited (see details here ). Example : below is the piece of the main_file of the Gromacs pipeline:","title":"main_file"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#configjson","text":"This file contains pipeline execution parameters. You can not rename or delete it because of it's used in pipeline scripts and they will not work without it. Note : it is advised that pipeline execution settings are modified via CONFIGURATION tab (e.g. if you want to change default settings for pipeline execution) or via Launch pipeline page (e.g. if you want to change pipeline settings for a current run). Manual config.json editing should be used only for advanced users (primarily developers) since json format is not validated in this case. Note : all attributes from config.json are available as environment variables for pipeline execution. The config.json file for every pipeline template have the following settings: Setting Description main_file A name of the main file for that pipeline. instance_size instance type in terms of the specific Cloud Provider that specifies an amount of RAM in Gb, CPU and GPU cores number (e.g. \"m4.xlarge\" for AWS EC2 instance). instance_disk An instance's disk size in Gb. docker_image A name of the Docker image that will be used in the current pipeline. cmd_template Command line template that will be executed at the running instance in the pipeline. cmd_template can use environment variables: To address the main_file parameter value, use the following construction - [main_file] To address all other parameters, usual Linux environment variables style shall be used (e.g. $docker_image ) parameters Pipeline execution parameters (e.g. path to the data storage with input data). A parameter has a name and set of attributes. There are 3 possible keys for each parameter: \"type\" - key specifies a type for current parameter, \"value\" - key specifies default value for parameter, \"required\" - key specifies whether this parameter must be set ( \"required\": true ) or might not ( \"required\": false ) Example : config.json file of the Gromacs pipeline: Note : In addition to main_file and config.json you can add any number of files to the CODE section and combine it in one whole scenario.","title":"config.json"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#configuration","text":"This section represents pipeline execution parameters which are set in config.json file. The parameters can be changed here and config.json file will be changed respectively. See how to edit configuration here . A configuration specifies: Section Control Description Name Pipeline and its configuration names. Estimated price per hour Control shows machine hours prices. If you navigate mouse to \"info\" icon, you'll see the maximum, minimum and average price for a particular pipeline version run as well as the price per hour. Exec environment This section lists execution environment parameters. Docker image A name of a Docker image to use for a pipeline execution (e.g. \"library/gromacs-gpu\"). Node type An instance type in terms of the specific Cloud Provider: CPU, RAM, GPU (e.g. 2 CPU cores, 8 Gb RAM, 0 GPU cores). Disk Size of a disk in gigabytes, that will be attached to the instance in Gb. Configure cluster button On-click, pop-up window will be shown: Here you can configure cluster or auto-scaled cluster. Cluster is a collection of instances which are connected so that they can be used together on a task. In both cases, a number of additional worker nodes with some main node as cluster head are launching (total number of pipelines = \"number of working nodes\" + 1). See v.0.14 - 7.2. Launch Detached Configuration for details. In case of using cluster , an exact count of worker nodes is directly defined by the user before launching the task and could not changing during the run. In case of using auto-scaled cluster , a max count of worker nodes is defined by the user before launching the task but really used count of worker nodes can change during the run depending on the jobs queue load. See Appendix C. Working with autoscaled cluster runs for details. For configure cluster: in opened window click Cluster button specify a number of child nodes (workers' count) if you want to use GridEngine server for the cluster, tick the Enable GridEngine checkbox. Setting of that checkbox automatically adds the CP_CAP_SGE system parameter with value true . Note : you may set this and other system parameters manually - see the example of using system parameters here . if you want to use Apache Spark for the cluster, tick the Enable Apache Spark checkbox. Setting of that checkbox automatically adds the CP_CAP_SPARK system parameter with value true . See the example of using Apache Spark here . if you want to use Slurm for the cluster, tick the Enable Slurm checkbox. Setting of that checkbox automatically adds the CP_CAP_SLURM system parameter with value true . See the example of using Slurm here . click OK button: When user selects Cluster option, information on total cluster resources is shown. Resources are calculated as (CPU/RAM/GPU)*(NUMBER_OF_WORKERS+1) : For configure auto-scaled cluster : in opened window click Auto-scaled cluster button specify a number of child nodes (workers' count) in field Auto-scaled up to and click OK button: Note : that number is meaning total count of \"auto-scaled\" nodes - it is the max count of worker nodes that could be attached to the main node to work together as cluster. These nodes will be attached to the cluster only in case if some jobs are in waiting state longer than a specific time. Also these nodes will be dropped from the cluster in case when jobs queue is empty or all jobs are running and there are some idle nodes longer than a specific time. Note : about timeout periods for scale-up and scale-down of auto-scaled cluster see here . additionally you may enable hybrid mode for the auto-scaled cluster - it allows to scale-up the cluster (attach a worker node) with the instance type distinct of the master - worker is being picked up based on the amount of unsatisfied CPU requirements of all pending jobs. For that behavior, set the \" Enable Hybrid cluster \" checkbox. For more details see here . additionally you may specify a number of \"persistent\" child nodes (workers' count) - click Setup default child nodes count , input Default child nodes number and click Ok button: These default child nodes will be never \"scaled-down\" during the run regardless of jobs queue load. In the example above, total count of \"auto-scaled\" nodes - 3, and 1 of them is \"persistent\". Note : total count of child nodes always must be greater than count of default (\"persistent\") child nodes. if you don't want to use default (\"persistent\") child nodes in your auto-scaled cluster - click Reset button opposite the Default child nodes field. additionally you may choose a price type for workers that will be attached during the run - via the \" Workers price type \" dropdown list - workers' price type can be automatically the same as the master node type (by default) or forcibly specified regardless on the master's type: When user selects Auto-scaled cluster , information on total cluster resources is shown as interval - from the \"min\" configuration to \"max\" configuration: \"min\" configuration resources are calculated as (CPU/RAM/GPU)*(NUMBER_OF_DEFAULT_WORKERS+1) \"max\" configuration resources are calculated as (CPU/RAM/GPU)*(TOTAL_NUMBER_OF_WORKERS+1) E.g. for auto-scaled cluster with 2 child nodes and without default (\"persistent\") child nodes (NUMBER_OF_DEFAULT_WORKERS = 0; TOTAL_NUMBER_OF_WORKERS = 2) : E.g. for auto-scaled cluster with 2 child nodes and 1 default (\"persistent\") child node (NUMBER_OF_DEFAULT_WORKERS = 1; TOTAL_NUMBER_OF_WORKERS = 2) : Note : in some specific configurations such as hybrid autoscaling clusters amount of resources can vary beyond the shown interval. Note : if you don't want to use any cluster - click Single node button and then click OK button. Cloud Region A specific region for a compute node placement. Please note, if a non-default region is selected - certain CP features may be unavailable: FS mounts usage from the another region (e.g. \" EU West \" region cannot use FS mounts from the \" US East \"). Regular storages will be still available If a specific tool, used for a run, requires an on-premise license server (e.g. monolix, matlab, schrodinger, etc.) - such instances shall be run in a region, that hosts those license servers. Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - in that control Cloud Provider auxiliary icons also will be displayed, e.g.: For a single-Provider deployments only Cloud Region icons are displayed. Advanced Price type Choose Spot or On-demand type of instance. You can look information about price types hovering \"Info\" icon and based on it make your choice. Timeout (min) After this time pipeline will shut down (optional). Limit mounts Allow to specify storages that should be mounted. See here . Cmd template A shell command that will be executed to start a pipeline. \"Start idle\" The flag sets cmd_template to sleep infinity . For more information about starting a job in this mode refer to 15. Interactive services . Parameters This section lists pipeline specific parameters that can be used during a pipeline run. Pipeline parameters can be assigned the following types: String - generic scalar value (e.g. Sample name). Boolean - boolean value. Path - path in a data storage hierarchy. Input - path in a data storage hierarchy. During pipeline initialization, this path will be used to download input data on the calculation node for processing from a storage. Output - path in a data storage hierarchy. During pipeline finalization, this path will be used to upload resulting data to a storage. Common - path in a data storage hierarchy. Similar to \"Input\" type, but this data will not be erased from a calculation node, when a pipeline is finished (this is useful for reference data, as it can be reused by further pipeline runs that share the same reference). Note : You can use Project attribute values as parameters for the Run : Click an empty parameter value field. Enter \" project. \" In the drop-down list select the Project attribute value: Add parameter This control helps to add an additional parameter to a configuration.","title":"CONFIGURATION"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#configuration-tab-controls","text":"Control Description Add To create a customized configuration for the pipeline, click the + ADD button in the upper-right corner of the screen. For more details see here . Save This button saves changes in a configuration.","title":"Configuration tab controls"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#history","text":"This section contains information about all the current pipeline version's runs. Runs info is organized into a table with the following columns: Run - each record of that column contains two rows: in upper - run name that consists of pipeline name and run id, in bottom - Cloud Region. Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - corresponding text information also has a Provider name, e.g.: Parent-run - id of the run that executed current run (this field is non-empty only for runs that are executed by other runs). Pipeline - each record of that column contains two rows: in upper - pipeline name, in bottom - pipeline version. Docker image - base docker image name. Started - time pipeline started running. Completed - time pipeline finished execution. Elapsed - each record of that column contains two rows: in upper - pipeline running time, in bottom - run's estimated price, which is calculated based on the run duration, region and instance type. Owner - user who launched run. You can filter runs by clicking the filter icon . By using the filter control you can choose whether display runs for current pipeline version or display runs for all pipeline versions.","title":"HISTORY"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#history-tab-controls","text":"Control Description PAUSE (a) To pause running pipeline press this control. This control is available only for on-demand instances. STOP (b) To stop running pipeline press this control. LOG (c) \"Log\" control opens detailed information about the run. You'll be redirected to \" Runs \" space (see 11. Manage Runs ). RESUME (d) To resume pausing pipeline press this control. This control is available only for on-demand instances. TERMINATE (e) To terminate node without waiting of the pipeline resuming. This control is available only for on-demand instances, which were paused. RERUN (f) This control reruns completed pipeline's runs.","title":"History tab controls"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#pipeline-runs-states","text":"Icons at the left represent the current state of the pipeline runs: - Queued state (\"sandglass\" icon) - a run is waiting in the queue for the available compute node. - Initializing state (\"rotating\" icon) - a run is being initialized. - Pulling state (\"download\" icon) - now pipeline Docker image is downloaded to the node. - Running state (\"play\" icon) - a pipeline is running. The node is appearing and pipeline input data is being downloaded to the node before the \" InitializeEnvironment \" service task appears. - Paused state (\"pause\" icon) - a run is paused. At this moment compute node is already stopped but keeps it's state. Such run may be resumed. - Success state (\"OK\" icon) - successful pipeline execution. - Failed state (\"caution\" icon) - unsuccessful pipeline execution. - Stopped state (\"clock\" icon) - a pipeline manually stopped. Also, help tooltips are provided when hovering a run state icon, e.g.:","title":"Pipeline run's states"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#storage-rules","text":"This section displays a list of rules used to upload data to the output data storage, once pipeline finished. It helps to store only data you need and minimize the amount of interim data in data storages. Info is organized into a table with the following columns: Mask column contains a relative path from the $ANALYSIS_DIR folder (see Default environment variables section below for more information). Mask uses bash syntax to specify the data that you want to upload from the $ANALYSIS_DIR. Data from the specified path will be uploaded to the bucket from the pipeline node. Note : by default whole $ANALYSIS_DIR folder is uploaded to the cloud bucket (default Mask is - \"*\"). For example, \"*.txt*\" mask specifies that all files with .txt extension need to be uploaded from the $ANALYSIS_DIR to the data storage. Note : Be accurate when specifying masks - if wildcard mask (\"*\") is specified, all files will be uploaded, no matter what additional masks are specified. The Created column shows date and time of rules creation. Move to Short-Term Storage column indicates whether pipeline output data will be moved to a short-term storage.","title":"STORAGE RULES"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#storage-rules-tab-control","text":"Control Description Add new rule (a) This control allows adding a new data managing rule. Delete (b) To delete a data managing rule press this control.","title":"Storage rules tab control"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#graph","text":"This section represents the sequence of pipeline tasks as a directed graph. Tasks are graph vertices, edges represent execution order. A task can be executed only when all input edges - associated tasks - are completed (see more information about creating a pipeline with GRAPH section here ). Note : only for Luigi and WDL pipelines. Note : If main_file has mistakes, pipeline workflow won't be visualized.","title":"GRAPH"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#graph-tab-controls","text":"When a PipelineBuilder graph is loaded, the following layout controls become available to the user. Control Description Save saves changes. Revert reverts all changes to the last saving. Layout performs graph linearization, make it more readable. Fit zooms graph to fit the screen. Show links enables/disables workflow level links to the tasks. It is disabled by default, as for large workflows it overwhelms the visualization. Zoom out zooms graph out. Zoom in zooms graph in. Search element allows to find specific object at the graph. Fullscreen expands graph to the full screen.","title":"Graph tab controls"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#default-environment-variables","text":"Pipeline scripts (e.g. main_file ) use default environmental variables for pipeline execution. These variables are set in internal CP scripts: RUN_ID - pipeline run ID. PIPELINE_NAME - pipeline name. COMMON_DIR - directory where pipeline common data (parameter with \"type\": \"common\" ) will be stored. ANALYSIS_DIR - directory where output data of the pipeline (parameter with \"type\": \"output\" ) will be stored. INPUT_DIR - directory where input data of the pipeline (parameter with \"type\": \"input\" ) will be stored. SCRIPTS_DIR - directory where all pipeline scripts and config.json file will be stored.","title":"Default environment variables"},{"location":"manual/07_Manage_Detached_configuration/7.1._Create_and_customize_Detached_configuration/","text":"7.1. Create and customize Detached configuration Create Detached configuration Customize Detached configuration Edit detached configuration name and description Edit detached configuration permissions Add run configuration to detached configuration Edit run configuration in the Detached configuration Set a root entity and map configuration parameters Delete run configuration from the Detached configuration Create Detached configuration To create Detached configuration in a Folder you need to have WRITE permission for that folder and the ROLE_CONFIGURATION_MANAGER role. For more information see 13. Permissions . Note : you can create a specific type of a run configuration which could be used only for a specific type of data. In such type of a run configuration, you can link type of data (e.g. Sample, Participant, etc) and the algorithm - a pipeline. To do that, you shall create your run configuration in a Project folder. Learn how to create a project here . To create a Detached configuration: Navigate to the folder where you want to create. Click + Create \u2192 Configuration . Enter Configuration name and Configuration description in pop-up window. Click Create . The configuration will be shown in the Library. Customize Detached configuration To edit Detached configuration you need WRITE permissions for it. For more information see 13. Permissions . Edit detached configuration name and description Navigate to the Folder where the Detached configuration is stored. Click icon. The \"Edit configuration info\" pop-up window will be open. Change Detached configuration name and description. Click Save . Edit detached configuration permissions Navigate to the Detached configuration and click icon. Note: Also you can navigate to the Folder where the Detached configuration is stored and click \"Pencil\" icon. Go to Permissions tab. Click Add user or Add user group . In the example screenshots, we grant permissions to a user. Enter user's name. Auto-filling will help you. A user will be added to the list. Click User's name to manage user's permissions. Tick appropriate permissions. For more details see 13. Permissions . Add run configuration to detached configuration Select Detached configuration in the Library . Add new Run configuration via + ADD button. Enter the name, description of the new Run configuration . If the Detached configuration already has more than one Run configuration , select the template for the new one. The new configuration will be based on the template . Click Create . New Run Configuration will be represented at Detached configuration details pane. Edit run configuration in the Detached configuration Select Detached configuration in the Library . Go to the tab with the Run configuration that you want to change. Change parameters of the Run configuration. Click Save . Set a root entity and map configuration parameters If your configuration stored in a folder with a Project type, when, to set a \"Root entity\" field, you shall add metadata to your project. After that, you'll be able to select metadata entity type from the drop-down list. Note : learn how to create a project here and about managing metadata here . Click \" Root entity \" combo-box. Choose the object from default values: Participants, Samples, Pairs, Sets of Participants, Sets of Samples, Sets of Pairs. When you select \" Root entity \", you'll be able to map configuration parameters to the root entity metadata attributes. You can set it using expansion expressions. Click an empty parameter value field. Enter \" this. \". \" this \" means that value is attributed to the selected Root entity type . In the drop-down list select the metadata value. Delete run configuration from the Detached configuration Select Detached configuration in the Library . Go to the tab with the Run configuration that you want to delete. Click Remove . Confirm removal.","title":"7.1. Create and customize Detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7.1._Create_and_customize_Detached_configuration/#71-create-and-customize-detached-configuration","text":"Create Detached configuration Customize Detached configuration Edit detached configuration name and description Edit detached configuration permissions Add run configuration to detached configuration Edit run configuration in the Detached configuration Set a root entity and map configuration parameters Delete run configuration from the Detached configuration","title":"7.1. Create and customize Detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7.1._Create_and_customize_Detached_configuration/#create-detached-configuration","text":"To create Detached configuration in a Folder you need to have WRITE permission for that folder and the ROLE_CONFIGURATION_MANAGER role. For more information see 13. Permissions . Note : you can create a specific type of a run configuration which could be used only for a specific type of data. In such type of a run configuration, you can link type of data (e.g. Sample, Participant, etc) and the algorithm - a pipeline. To do that, you shall create your run configuration in a Project folder. Learn how to create a project here . To create a Detached configuration: Navigate to the folder where you want to create. Click + Create \u2192 Configuration . Enter Configuration name and Configuration description in pop-up window. Click Create . The configuration will be shown in the Library.","title":"Create Detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7.1._Create_and_customize_Detached_configuration/#customize-detached-configuration","text":"To edit Detached configuration you need WRITE permissions for it. For more information see 13. Permissions .","title":"Customize Detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7.1._Create_and_customize_Detached_configuration/#edit-detached-configuration-name-and-description","text":"Navigate to the Folder where the Detached configuration is stored. Click icon. The \"Edit configuration info\" pop-up window will be open. Change Detached configuration name and description. Click Save .","title":"Edit\u00a0detached configuration name and description"},{"location":"manual/07_Manage_Detached_configuration/7.1._Create_and_customize_Detached_configuration/#edit-detached-configuration-permissions","text":"Navigate to the Detached configuration and click icon. Note: Also you can navigate to the Folder where the Detached configuration is stored and click \"Pencil\" icon. Go to Permissions tab. Click Add user or Add user group . In the example screenshots, we grant permissions to a user. Enter user's name. Auto-filling will help you. A user will be added to the list. Click User's name to manage user's permissions. Tick appropriate permissions. For more details see 13. Permissions .","title":"Edit\u00a0detached configuration permissions"},{"location":"manual/07_Manage_Detached_configuration/7.1._Create_and_customize_Detached_configuration/#add-run-configuration-to-detached-configuration","text":"Select Detached configuration in the Library . Add new Run configuration via + ADD button. Enter the name, description of the new Run configuration . If the Detached configuration already has more than one Run configuration , select the template for the new one. The new configuration will be based on the template . Click Create . New Run Configuration will be represented at Detached configuration details pane.","title":"Add\u00a0run configuration\u00a0to detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7.1._Create_and_customize_Detached_configuration/#edit-run-configuration-in-the-detached-configuration","text":"Select Detached configuration in the Library . Go to the tab with the Run configuration that you want to change. Change parameters of the Run configuration. Click Save .","title":"Edit run configuration in the Detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7.1._Create_and_customize_Detached_configuration/#set-a-root-entity-and-map-configuration-parameters","text":"If your configuration stored in a folder with a Project type, when, to set a \"Root entity\" field, you shall add metadata to your project. After that, you'll be able to select metadata entity type from the drop-down list. Note : learn how to create a project here and about managing metadata here . Click \" Root entity \" combo-box. Choose the object from default values: Participants, Samples, Pairs, Sets of Participants, Sets of Samples, Sets of Pairs. When you select \" Root entity \", you'll be able to map configuration parameters to the root entity metadata attributes. You can set it using expansion expressions. Click an empty parameter value field. Enter \" this. \". \" this \" means that value is attributed to the selected Root entity type . In the drop-down list select the metadata value.","title":"Set a root entity and map configuration parameters"},{"location":"manual/07_Manage_Detached_configuration/7.1._Create_and_customize_Detached_configuration/#delete-run-configuration-from-the-detached-configuration","text":"Select Detached configuration in the Library . Go to the tab with the Run configuration that you want to delete. Click Remove . Confirm removal.","title":"Delete\u00a0run configuration\u00a0from the Detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7.2._Launch_Detached_Configuration/","text":"7.2. Launch Detached Configuration Launch detached cluster configuration as a cluster Schedule a launch from the detached configuration Create and configure a schedule rule Delete a schedule rule To launch a Detached configuration you need to have EXECUTE permissions for it. For more information see 13. Permissions . Detached configuration represents a configuration for running instances as a cluster. For example, when you need different instances running at one task: master machine and one or several worker machines are configured. They may or may not use one docker image and run different scripts. There are different options to run the cluster in Cloud Pipeline : To use Configure cluster button at Launch pipeline page or at Detached configuration page. Click the button, configure cluster - you will be offered to start current configuration at several machines (\"working nodes\"). I.e. there will start several identically configured machines having network file system within each. It is impossible to run all configurations for the pipeline at once as a cluster. For more details see 6. Manage Pipeline . To use Detached configuration . Cluster configuration is a set of configurations for nodes that may or may not start some pipeline. Configurations in Detached configuration page are typically different. From Detached configuration page, you can launch all configurations at once as a cluster or launch configurations one by one as via Launch pipeline tab. Launch detached cluster configuration as a cluster Navigate to Detached Configuration details page. Click Run \u2192 Run cluster . All the Run configurations of the Detached configuration will start execution. Note : Select Run selected to launch only the opened Run configuration. Please note, that the current user can launch a detached configuration only if he/his group has corresponding permissions on that configuration (for more information see 13. Permissions ), but the Run button may be disabled also for one of the following reasons: execution isn't allowed for specified docker image; read operations aren't allowed for specified input or common path parameters; write operations aren't allowed for specified output path parameters. In such cases, hover over the Run button to view warning notification with a reason of a run forbiddance, e.g.: In case of launching Root entity configuration , a pop-up window emerges. Select an appropriate metadata in correspondence with root entity . If root entity is an attribute of the selected metadata, use expansion expression in the Define expression field . Click OK . Schedule a launch from the detached configuration In some cases, the ability to configure a schedule for detached configuration running is beneficial. User is able to set a schedule for launch a run from the detached configuration: Schedule is defined as a list of rules (user is able to specify any number of them) For each rule in the list user is able to set the recurrence: Daily : every N days, time or Weekly : every weekday(s) , time Conflicting rules are not allowed (i.e. rules that are configured on the same launching time) If any schedule rule is configured for the detached configuration - a corresponding job (a pipeline (if specified) or a plain container) will be started accordingly in the scheduled day and time Configuration will be run from the user who created or updated the corresponding schedule If detached configuration has several entries (configs) only default one will be launched by the schedule Create and configure a schedule rule Open the detached configuration Click the Run schedule button in the right upper corner: The \"Run schedule\" popup will appear: Click the Add rule button. The first rule in the list will appear: Let's configure the launch of the detached configuration every 3 days at 12:30. For that: click the field next to \"Every\" label and specify the corresponding number in it click the time field, select hours in the left dropdown list and minutes in the right Let's add another rule and configure the launch of the detached configuration every monday and wednesday at 15:00. For that: click the Add rule button for the appeared new rule select the \"Weekly\" item in the recurrence dropdown list click the field next to the recurrence dropdown and select the corresponding weekdays click the time field, select the corresponding time values Click the OK button to save created rules: From now, the run from that detached configuration will be automatically launching according to the created schedule rules. Delete a schedule rule Click the Run schedule button in the right upper corner of the detached configuration: The \"Run schedule\" popup will appear. Click the Delete button in the row of the rule you want to remove, e.g.: The row with removing rule will become pink: Note : you may easily revert the removal by button Click the OK button to save performed changes and permanently remove the rule","title":"7.2. Launch Detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7.2._Launch_Detached_Configuration/#72-launch-detached-configuration","text":"Launch detached cluster configuration as a cluster Schedule a launch from the detached configuration Create and configure a schedule rule Delete a schedule rule To launch a Detached configuration you need to have EXECUTE permissions for it. For more information see 13. Permissions . Detached configuration represents a configuration for running instances as a cluster. For example, when you need different instances running at one task: master machine and one or several worker machines are configured. They may or may not use one docker image and run different scripts. There are different options to run the cluster in Cloud Pipeline : To use Configure cluster button at Launch pipeline page or at Detached configuration page. Click the button, configure cluster - you will be offered to start current configuration at several machines (\"working nodes\"). I.e. there will start several identically configured machines having network file system within each. It is impossible to run all configurations for the pipeline at once as a cluster. For more details see 6. Manage Pipeline . To use Detached configuration . Cluster configuration is a set of configurations for nodes that may or may not start some pipeline. Configurations in Detached configuration page are typically different. From Detached configuration page, you can launch all configurations at once as a cluster or launch configurations one by one as via Launch pipeline tab.","title":"7.2. Launch Detached Configuration"},{"location":"manual/07_Manage_Detached_configuration/7.2._Launch_Detached_Configuration/#launch-detached-cluster-configuration-as-a-cluster","text":"Navigate to Detached Configuration details page. Click Run \u2192 Run cluster . All the Run configurations of the Detached configuration will start execution. Note : Select Run selected to launch only the opened Run configuration. Please note, that the current user can launch a detached configuration only if he/his group has corresponding permissions on that configuration (for more information see 13. Permissions ), but the Run button may be disabled also for one of the following reasons: execution isn't allowed for specified docker image; read operations aren't allowed for specified input or common path parameters; write operations aren't allowed for specified output path parameters. In such cases, hover over the Run button to view warning notification with a reason of a run forbiddance, e.g.: In case of launching Root entity configuration , a pop-up window emerges. Select an appropriate metadata in correspondence with root entity . If root entity is an attribute of the selected metadata, use expansion expression in the Define expression field . Click OK .","title":"Launch detached\u00a0cluster\u00a0configuration as a cluster"},{"location":"manual/07_Manage_Detached_configuration/7.2._Launch_Detached_Configuration/#schedule-a-launch-from-the-detached-configuration","text":"In some cases, the ability to configure a schedule for detached configuration running is beneficial. User is able to set a schedule for launch a run from the detached configuration: Schedule is defined as a list of rules (user is able to specify any number of them) For each rule in the list user is able to set the recurrence: Daily : every N days, time or Weekly : every weekday(s) , time Conflicting rules are not allowed (i.e. rules that are configured on the same launching time) If any schedule rule is configured for the detached configuration - a corresponding job (a pipeline (if specified) or a plain container) will be started accordingly in the scheduled day and time Configuration will be run from the user who created or updated the corresponding schedule If detached configuration has several entries (configs) only default one will be launched by the schedule","title":"Schedule a launch from the detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7.2._Launch_Detached_Configuration/#create-and-configure-a-schedule-rule","text":"Open the detached configuration Click the Run schedule button in the right upper corner: The \"Run schedule\" popup will appear: Click the Add rule button. The first rule in the list will appear: Let's configure the launch of the detached configuration every 3 days at 12:30. For that: click the field next to \"Every\" label and specify the corresponding number in it click the time field, select hours in the left dropdown list and minutes in the right Let's add another rule and configure the launch of the detached configuration every monday and wednesday at 15:00. For that: click the Add rule button for the appeared new rule select the \"Weekly\" item in the recurrence dropdown list click the field next to the recurrence dropdown and select the corresponding weekdays click the time field, select the corresponding time values Click the OK button to save created rules: From now, the run from that detached configuration will be automatically launching according to the created schedule rules.","title":"Create and configure a schedule rule"},{"location":"manual/07_Manage_Detached_configuration/7.2._Launch_Detached_Configuration/#delete-a-schedule-rule","text":"Click the Run schedule button in the right upper corner of the detached configuration: The \"Run schedule\" popup will appear. Click the Delete button in the row of the rule you want to remove, e.g.: The row with removing rule will become pink: Note : you may easily revert the removal by button Click the OK button to save performed changes and permanently remove the rule","title":"Delete a schedule rule"},{"location":"manual/07_Manage_Detached_configuration/7.3._Expansion_Expressions/","text":"7.3. Expansion Expressions As the Root entity has a number of attributes and the entity is connected with other entities and their attributes, we need a special way to define them as parameters of configuration or be able to launch one entity metadata under a configuration with a different type of root entity. For this purpose we use expansion expressions : \"this. ...\" - \"this\" keyword references a specific instance of an entity. It shall be followed by an attribute of an instance. Additionally, set of attributes of corresponding entities can be chained (see examples below). \"this.attribute\" - \"this.fastq\" \"this.attribute.attribute. ...\" - chaining allows creating more complex pipeline runs. In this case, each \"attribute\" keyword will be expanded and used as an input for the next \"attribute\". For example, \" this.control_sample.r1_fastq \". \"this.attribute\" construction is also useful at Root entity configuration launching step when a user selects metadata: Example 1 . The root entity for the configuration is \" Samples \", but user selects a \" Pair \" in the pop-up window. The \" Pair \" has a link to two samples as attributes . A user here should define the desired attribute ( sample ) to launch pipeline with the specified root entity. For example, type \" this.control_sample \" in Define expression field. Example 2 . The root entity is \" Samples \", but user selects \" Sample Set \" and wants to run the analysis for all the samples in the set . A user here should type the following expression \" this.Samples \" in Define expression field.","title":"7.3. Expansion expressions"},{"location":"manual/07_Manage_Detached_configuration/7.3._Expansion_Expressions/#73-expansion-expressions","text":"As the Root entity has a number of attributes and the entity is connected with other entities and their attributes, we need a special way to define them as parameters of configuration or be able to launch one entity metadata under a configuration with a different type of root entity. For this purpose we use expansion expressions : \"this. ...\" - \"this\" keyword references a specific instance of an entity. It shall be followed by an attribute of an instance. Additionally, set of attributes of corresponding entities can be chained (see examples below). \"this.attribute\" - \"this.fastq\" \"this.attribute.attribute. ...\" - chaining allows creating more complex pipeline runs. In this case, each \"attribute\" keyword will be expanded and used as an input for the next \"attribute\". For example, \" this.control_sample.r1_fastq \". \"this.attribute\" construction is also useful at Root entity configuration launching step when a user selects metadata: Example 1 . The root entity for the configuration is \" Samples \", but user selects a \" Pair \" in the pop-up window. The \" Pair \" has a link to two samples as attributes . A user here should define the desired attribute ( sample ) to launch pipeline with the specified root entity. For example, type \" this.control_sample \" in Define expression field. Example 2 . The root entity is \" Samples \", but user selects \" Sample Set \" and wants to run the analysis for all the samples in the set . A user here should type the following expression \" this.Samples \" in Define expression field.","title":"7.3. Expansion Expressions"},{"location":"manual/07_Manage_Detached_configuration/7.4._Remove_Detached_configuration/","text":"7.4. Remove Detached configuration To delete Detached configuration you need to have WRITE permission for that configuration and the ROLE_CONFIGURATION_MANAGER . For more information see 13. Permissions . You can remove the Detached configuration . Navigate to the Folder where Cluster configuration is stored. Click icon in the detached configuration line to delete. Click Delete . Confirm the action.","title":"7.4. Remove Detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7.4._Remove_Detached_configuration/#74-remove-detached-configuration","text":"To delete Detached configuration you need to have WRITE permission for that configuration and the ROLE_CONFIGURATION_MANAGER . For more information see 13. Permissions . You can remove the Detached configuration . Navigate to the Folder where Cluster configuration is stored. Click icon in the detached configuration line to delete. Click Delete . Confirm the action.","title":"7.4. Remove Detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7._Manage_Detached_configuration/","text":"7. Manage Detached configuration \"Details\" view pane Controls Run schedule \"Gear\" icon Add Run Remove Save Detached configuration is a run configuration or a set of run configurations that allows running tools and pipelines. Note : In comparison with pipeline configurations, detached configurations do not require a pipeline. \"Details\" view pane At the \" Details view \" pane you can find: Section Control Description Name Detach configuration (a) and its Run configuration (b) names Estimated price per hour Control shows machine hours prices. If you navigate mouse to \" info \" icon (c) , you'll see the maximum, minimum and average price for a particular pipeline version run as well as the price per hour. Exec environment This section lists execution environment parameters. Pipeline (d) A name of the pipeline to be executed (optional). Click on the field to select a pipeline in the pop-up. Execution environment (e) An environment platform for execution the pipeline. Click for select from the list. Docker image (f) A name of a Docker image to use for a pipeline execution (e.g. \"base-generic-centos7\"). Click on the field to select an image in the pop-up. Node type (g) An instance type in terms of Cloud Provider with specifying amounts of CPU, RAM and GPU (e.g. 4 CPU cores, 16 Gb RAM and 0 GPU cores). Disk (h) Size of a disk, that will be attached to the instance in Gb. Configure cluster button (i) By clicking on this button you can configure cluster or auto-scaled cluster. Cluster is a collection of instances which are connected so that they can be used together on a task. See here and here for more information. Cloud Region (j) A specific region for a compute node placement. Please note, if a non-default region is selected - certain CP features may be unavailable: FS mounts usage from the another region (e.g. \" EU West \" region cannot use FS mounts from the \" US East \"). Regular storages will be still available If a specific tool, used for a run, requires an on-premise license server (e.g. monolix, matlab, schrodinger, etc.) - such instances shall be run in a region, that hosts those license servers. Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - in that control Cloud Provider auxiliary icons also will be displayed, e.g.: For a single-Provider deployments only Cloud Region icons are displayed. Total resources (q) Information about total resources that will be used for running pipeline with specified parameters (depends on node type and cluster configuration). See here for more details. Advanced Price type (k) Choose spot or on-demand type of instance. The \"Info\" icon can give you additional information, which helps you to make choice. Timeout (min) (l) After this time pipeline will shut down (optional). Limit mounts (m) Restricts available storages for the tools or pipelines. See here . Cmd template (n) A shell command that will be executed on the running node. \"Start idle\" The flag sets Cmd template to \"sleep infinity\". For more information about starting a job in this mode refer to 15. Interactive services . Parameters This section lists pipeline specific parameters that can be used during a pipeline run. Pipeline parameters can be assigned the following types (p) : String - generic scalar value (e.g. Sample name). Boolean - boolean value. Path - path in a data storage hierarchy. Input - path in a data storage hierarchy. During pipeline initialization, this path will be used to get data from a storage. Output - path in a data storage hierarchy. During pipeline finalization, this path will be used to upload resulting data to a storage. Common - path in a data storage hierarchy. Similar to \"Input\" type, but this data will not be erased from a calculation node, when a pipeline is finished (this is useful for reference data, as it can be reused by further pipeline runs that share the same reference). Note : You can use Project attribute values as parameters for the Run : Click an empty parameter value field. Enter \"project.\" In the drop-down list select the Project attribute value. Add parameter (o) This control helps to add an additional parameter to a configuration. Root entity type Note : This parameter is only available for configurations that are stored in Project type of the Folder and the Project has to store metadata object(s) within. See 7.1. Create and customize Detached configuration . It defines an entity which metadata will be used to process data. Default values: Participants, Samples, Pairs, Sets of Participants, Sets of Samples, Sets of Pairs. Controls There are buttons at the top of the \"Details\" view: Run schedule Allows creating of schedule rules to launch runs from the default configuration in the scheduled day and time in automatic regimen (a) . See 7.2. Launch Detached Configuration . \"Gear\" icon Allows changing a name, description of the configuration and permissions for it (b) . See 7.1. Create and customize Detached configuration . Add Allows adding machine configuration (c) . See 7.1. Create and customize Detached configuration . Run Allows launching one machine or all machines as a cluster (d) . See 7.2. Launch Detached Configuration . Remove Allows removing machine configuration (e) . See 7.4. Remove Detached configuration . Save Allow saving machine configuration (f) . See 7.1. Create and customize Detached configuration .","title":"7.0. Overview"},{"location":"manual/07_Manage_Detached_configuration/7._Manage_Detached_configuration/#7-manage-detached-configuration","text":"\"Details\" view pane Controls Run schedule \"Gear\" icon Add Run Remove Save Detached configuration is a run configuration or a set of run configurations that allows running tools and pipelines. Note : In comparison with pipeline configurations, detached configurations do not require a pipeline.","title":"7. Manage Detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7._Manage_Detached_configuration/#details-view-pane","text":"At the \" Details view \" pane you can find: Section Control Description Name Detach configuration (a) and its Run configuration (b) names Estimated price per hour Control shows machine hours prices. If you navigate mouse to \" info \" icon (c) , you'll see the maximum, minimum and average price for a particular pipeline version run as well as the price per hour. Exec environment This section lists execution environment parameters. Pipeline (d) A name of the pipeline to be executed (optional). Click on the field to select a pipeline in the pop-up. Execution environment (e) An environment platform for execution the pipeline. Click for select from the list. Docker image (f) A name of a Docker image to use for a pipeline execution (e.g. \"base-generic-centos7\"). Click on the field to select an image in the pop-up. Node type (g) An instance type in terms of Cloud Provider with specifying amounts of CPU, RAM and GPU (e.g. 4 CPU cores, 16 Gb RAM and 0 GPU cores). Disk (h) Size of a disk, that will be attached to the instance in Gb. Configure cluster button (i) By clicking on this button you can configure cluster or auto-scaled cluster. Cluster is a collection of instances which are connected so that they can be used together on a task. See here and here for more information. Cloud Region (j) A specific region for a compute node placement. Please note, if a non-default region is selected - certain CP features may be unavailable: FS mounts usage from the another region (e.g. \" EU West \" region cannot use FS mounts from the \" US East \"). Regular storages will be still available If a specific tool, used for a run, requires an on-premise license server (e.g. monolix, matlab, schrodinger, etc.) - such instances shall be run in a region, that hosts those license servers. Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - in that control Cloud Provider auxiliary icons also will be displayed, e.g.: For a single-Provider deployments only Cloud Region icons are displayed. Total resources (q) Information about total resources that will be used for running pipeline with specified parameters (depends on node type and cluster configuration). See here for more details. Advanced Price type (k) Choose spot or on-demand type of instance. The \"Info\" icon can give you additional information, which helps you to make choice. Timeout (min) (l) After this time pipeline will shut down (optional). Limit mounts (m) Restricts available storages for the tools or pipelines. See here . Cmd template (n) A shell command that will be executed on the running node. \"Start idle\" The flag sets Cmd template to \"sleep infinity\". For more information about starting a job in this mode refer to 15. Interactive services . Parameters This section lists pipeline specific parameters that can be used during a pipeline run. Pipeline parameters can be assigned the following types (p) : String - generic scalar value (e.g. Sample name). Boolean - boolean value. Path - path in a data storage hierarchy. Input - path in a data storage hierarchy. During pipeline initialization, this path will be used to get data from a storage. Output - path in a data storage hierarchy. During pipeline finalization, this path will be used to upload resulting data to a storage. Common - path in a data storage hierarchy. Similar to \"Input\" type, but this data will not be erased from a calculation node, when a pipeline is finished (this is useful for reference data, as it can be reused by further pipeline runs that share the same reference). Note : You can use Project attribute values as parameters for the Run : Click an empty parameter value field. Enter \"project.\" In the drop-down list select the Project attribute value. Add parameter (o) This control helps to add an additional parameter to a configuration. Root entity type Note : This parameter is only available for configurations that are stored in Project type of the Folder and the Project has to store metadata object(s) within. See 7.1. Create and customize Detached configuration . It defines an entity which metadata will be used to process data. Default values: Participants, Samples, Pairs, Sets of Participants, Sets of Samples, Sets of Pairs.","title":"\"Details\" view pane"},{"location":"manual/07_Manage_Detached_configuration/7._Manage_Detached_configuration/#controls","text":"There are buttons at the top of the \"Details\" view:","title":"Controls"},{"location":"manual/07_Manage_Detached_configuration/7._Manage_Detached_configuration/#run-schedule","text":"Allows creating of schedule rules to launch runs from the default configuration in the scheduled day and time in automatic regimen (a) . See 7.2. Launch Detached Configuration .","title":"Run schedule"},{"location":"manual/07_Manage_Detached_configuration/7._Manage_Detached_configuration/#gear-icon","text":"Allows changing a name, description of the configuration and permissions for it (b) . See 7.1. Create and customize Detached configuration .","title":"\"Gear\" icon"},{"location":"manual/07_Manage_Detached_configuration/7._Manage_Detached_configuration/#add","text":"Allows adding machine configuration (c) . See 7.1. Create and customize Detached configuration .","title":"Add"},{"location":"manual/07_Manage_Detached_configuration/7._Manage_Detached_configuration/#run","text":"Allows launching one machine or all machines as a cluster (d) . See 7.2. Launch Detached Configuration .","title":"Run"},{"location":"manual/07_Manage_Detached_configuration/7._Manage_Detached_configuration/#remove","text":"Allows removing machine configuration (e) . See 7.4. Remove Detached configuration .","title":"Remove"},{"location":"manual/07_Manage_Detached_configuration/7._Manage_Detached_configuration/#save","text":"Allow saving machine configuration (f) . See 7.1. Create and customize Detached configuration .","title":"Save"},{"location":"manual/08_Manage_Data_Storage/8.1._Create_and_edit_storage/","text":"8.1. Create and edit storage Create object storage Edit storage To create a Storage in a Folder you need to have WRITE permission for that folder and the ROLE_STORAGE_MANAGER role. For more information see 13. Permissions . You also can create Storage via CLI . See 14.3. Manage Storage via CLI . Create object storage Navigate to the folder where you want to create data storage. Click + Create \u2192 Storage \u2192 Create new object storage . Note : choose Add existing object storage to use an already existing cloud storage for this data storage. Note : how to create FS mount see here . Fill in the \" Info \" form: Storage path - path to access the storage (cloud storage name). If on Data storage tab in Preferences section of system-level settings storage.object.prefix (see v.0.14 - 12.10. Manage system-level settings ) is set - all new storages will be created with this prefix (e.g. \" ds \"): Alias - object storage name (if not specified, it is set equal to the Storage path ). Cloud region - location region of a data storage. This select allows to decrease time of data movement for huge data volumes by choosing the nearest region. Please note, if a non-default region is selected - certain CP features may be unavailable: FS mounts usage from the another region (e.g. \" EU West \" region cannot use FS mounts from the \" US East \"). Regular storages will be still available. If a specific tool, used for a run, requires an on-premise license server (e.g. monolix, matlab, schrodinger, etc.) - such instances shall be run in a region, that hosts those license servers. Also note, if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - in that case auxiliary Cloud Provider icons are additionally displayed, e.g.: Description - description of the data storage and comments. STS duration - short-term storage support (in days). Allows to set duration of data store period in a \"usual\" object storage. After this period, the data will be transferred to the long-term storage. LTS duration - long-term storage support (in days). Allows to set duration of the data store period in a long-term storage (e.g. S3 Glacier for AWS ), where the data is transferred after the STS duration expiration. After LTS period expiration, the data will be removed permanently. Long-term storage is a secure, durable, and low-cost storage compared to a \"usual\" object storage, but using of such storage intends not so fast and frequently access to the data. So, long-term storages are ideal for archives where data is regularly retrieved and some of the data may be needed in minutes. Enable versioning box - allows to enable versioning for the storage files. Backup duration - how long backup is stored (days). After this period expiration all backup-versions \"older\" than specified period will be removed. This setting is available only if versioning is enabled. Note : If you want to store data permanently, leave fields empty. Mount-point - specific mount-point. Mount options - specific mount options. Enable sharing box - allows to share storage content with other users (see here ). Click \"Create\" button. Edit storage You also can edit Storage via CLI . See 14.3. Manage Storage via CLI . You may change Alias , Description , STS and LTS duration , Mount-point and Mount options . Example: Select storage. Click icon. Change number of days in STS and LTS duration fields. If you want to store data permanently, leave fields empty. Click \"Save\" button.","title":"8.1. Create and edit Data Storage"},{"location":"manual/08_Manage_Data_Storage/8.1._Create_and_edit_storage/#81-create-and-edit-storage","text":"Create object storage Edit storage To create a Storage in a Folder you need to have WRITE permission for that folder and the ROLE_STORAGE_MANAGER role. For more information see 13. Permissions . You also can create Storage via CLI . See 14.3. Manage Storage via CLI .","title":"8.1. Create and edit storage"},{"location":"manual/08_Manage_Data_Storage/8.1._Create_and_edit_storage/#create-object-storage","text":"Navigate to the folder where you want to create data storage. Click + Create \u2192 Storage \u2192 Create new object storage . Note : choose Add existing object storage to use an already existing cloud storage for this data storage. Note : how to create FS mount see here . Fill in the \" Info \" form: Storage path - path to access the storage (cloud storage name). If on Data storage tab in Preferences section of system-level settings storage.object.prefix (see v.0.14 - 12.10. Manage system-level settings ) is set - all new storages will be created with this prefix (e.g. \" ds \"): Alias - object storage name (if not specified, it is set equal to the Storage path ). Cloud region - location region of a data storage. This select allows to decrease time of data movement for huge data volumes by choosing the nearest region. Please note, if a non-default region is selected - certain CP features may be unavailable: FS mounts usage from the another region (e.g. \" EU West \" region cannot use FS mounts from the \" US East \"). Regular storages will be still available. If a specific tool, used for a run, requires an on-premise license server (e.g. monolix, matlab, schrodinger, etc.) - such instances shall be run in a region, that hosts those license servers. Also note, if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - in that case auxiliary Cloud Provider icons are additionally displayed, e.g.: Description - description of the data storage and comments. STS duration - short-term storage support (in days). Allows to set duration of data store period in a \"usual\" object storage. After this period, the data will be transferred to the long-term storage. LTS duration - long-term storage support (in days). Allows to set duration of the data store period in a long-term storage (e.g. S3 Glacier for AWS ), where the data is transferred after the STS duration expiration. After LTS period expiration, the data will be removed permanently. Long-term storage is a secure, durable, and low-cost storage compared to a \"usual\" object storage, but using of such storage intends not so fast and frequently access to the data. So, long-term storages are ideal for archives where data is regularly retrieved and some of the data may be needed in minutes. Enable versioning box - allows to enable versioning for the storage files. Backup duration - how long backup is stored (days). After this period expiration all backup-versions \"older\" than specified period will be removed. This setting is available only if versioning is enabled. Note : If you want to store data permanently, leave fields empty. Mount-point - specific mount-point. Mount options - specific mount options. Enable sharing box - allows to share storage content with other users (see here ). Click \"Create\" button.","title":"Create object storage"},{"location":"manual/08_Manage_Data_Storage/8.1._Create_and_edit_storage/#edit-storage","text":"You also can edit Storage via CLI . See 14.3. Manage Storage via CLI . You may change Alias , Description , STS and LTS duration , Mount-point and Mount options . Example: Select storage. Click icon. Change number of days in STS and LTS duration fields. If you want to store data permanently, leave fields empty. Click \"Save\" button.","title":"Edit storage"},{"location":"manual/08_Manage_Data_Storage/8.2._Upload_Download_data/","text":"8.2. Upload/Download data Upload data Download data Generate URL To edit a Storage you need to have WRITE permission for the Storage . For more information see 13. Permissions . You also can upload and download data via CLI. See 14.3. Manage Storage via CLI . Upload data Click Upload button in the storage and folder of your choice: Browse file(s) to upload. Note : make sure size doesn't exceed 5 Gb. Note : you can cancel upload process by clicking the \"Cancel\" button. As a result, the file will be uploaded to the CP system. Note: the uploaded file will be tagged with auto-created attribute: CP_OWNER . The value of the attribute will be set as a user ID. The exception is that the storage is based on FS mount. Files in such data storage don't have attributes at all. Download data Click the Download button next to a file name. Specify where to download in the pop-up window. As a result, the file will be downloaded via your browser to the specified location. Generate URL You can use this to generate URLs for a number of files and then download them manually one by one or via scripts. Select files using a checkbox. Click the Generate URL button. A list of URLs (one for each file) will be generated.","title":"8.2. Upload/Download data"},{"location":"manual/08_Manage_Data_Storage/8.2._Upload_Download_data/#82-uploaddownload-data","text":"Upload data Download data Generate URL To edit a Storage you need to have WRITE permission for the Storage . For more information see 13. Permissions . You also can upload and download data via CLI. See 14.3. Manage Storage via CLI .","title":"8.2. Upload/Download data"},{"location":"manual/08_Manage_Data_Storage/8.2._Upload_Download_data/#upload-data","text":"Click Upload button in the storage and folder of your choice: Browse file(s) to upload. Note : make sure size doesn't exceed 5 Gb. Note : you can cancel upload process by clicking the \"Cancel\" button. As a result, the file will be uploaded to the CP system. Note: the uploaded file will be tagged with auto-created attribute: CP_OWNER . The value of the attribute will be set as a user ID. The exception is that the storage is based on FS mount. Files in such data storage don't have attributes at all.","title":"Upload data"},{"location":"manual/08_Manage_Data_Storage/8.2._Upload_Download_data/#download-data","text":"Click the Download button next to a file name. Specify where to download in the pop-up window. As a result, the file will be downloaded via your browser to the specified location.","title":"Download data"},{"location":"manual/08_Manage_Data_Storage/8.2._Upload_Download_data/#generate-url","text":"You can use this to generate URLs for a number of files and then download them manually one by one or via scripts. Select files using a checkbox. Click the Generate URL button. A list of URLs (one for each file) will be generated.","title":"Generate URL"},{"location":"manual/08_Manage_Data_Storage/8.3._Create_and_Edit_text_files/","text":"8.3. Create and Edit text files You can create and edit files via GUI. It may be useful when you add some description or metadata about files in the storage. Create and rename a file View and edit text file's contents View and edit tabular file's contents To edit a Storage you need to have WRITE permission for the Storage . For more information see 13. Permissions . Create and rename a file To create a file: Click + Create \u2192 File . Enter file's name. Enter file's contents (optionally). Click OK . As a result, a new file will be created. Note: the new file will be tagged with auto-created attribute: CP_OWNER . The value of the attribute will be set as a user ID. The exception is that the storage is based on FS mount. Files in such data storage don't have attributes at all. To rename a file: Click the button in the desired file line. Rename it. Click OK . View and edit text file's contents Click on the file. You will see file preview on the right. In case of the file is large you will see only a part of it. Click \"Expand\" icon at the upper right of the preview window. A pop-up text editor window appears. Click Edit and change the file. Click Save to save the changes. View and edit tabular file's contents Click on the tabular file. You will see file preview on the right. In case of the file is large you will see only a part of it. Click \"Expand\" icon at the upper right of the preview window. A pop-up text editor window appears. Click Edit and change file in tabular view or in the text view. Click Save to save the changes.","title":"8.3. Create and edit text files"},{"location":"manual/08_Manage_Data_Storage/8.3._Create_and_Edit_text_files/#83-create-and-edit-text-files","text":"You can create and edit files via GUI. It may be useful when you add some description or metadata about files in the storage. Create and rename a file View and edit text file's contents View and edit tabular file's contents To edit a Storage you need to have WRITE permission for the Storage . For more information see 13. Permissions .","title":"8.3. Create and Edit text files"},{"location":"manual/08_Manage_Data_Storage/8.3._Create_and_Edit_text_files/#create-and-rename-a-file","text":"To create a file: Click + Create \u2192 File . Enter file's name. Enter file's contents (optionally). Click OK . As a result, a new file will be created. Note: the new file will be tagged with auto-created attribute: CP_OWNER . The value of the attribute will be set as a user ID. The exception is that the storage is based on FS mount. Files in such data storage don't have attributes at all. To rename a file: Click the button in the desired file line. Rename it. Click OK .","title":"Create and rename a file"},{"location":"manual/08_Manage_Data_Storage/8.3._Create_and_Edit_text_files/#view-and-edit-text-files-contents","text":"Click on the file. You will see file preview on the right. In case of the file is large you will see only a part of it. Click \"Expand\" icon at the upper right of the preview window. A pop-up text editor window appears. Click Edit and change the file. Click Save to save the changes.","title":"View and edit text file's contents"},{"location":"manual/08_Manage_Data_Storage/8.3._Create_and_Edit_text_files/#view-and-edit-tabular-files-contents","text":"Click on the tabular file. You will see file preview on the right. In case of the file is large you will see only a part of it. Click \"Expand\" icon at the upper right of the preview window. A pop-up text editor window appears. Click Edit and change file in tabular view or in the text view. Click Save to save the changes.","title":"View and edit tabular file's contents"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/","text":"8.4. Control File versions Note : This feature is available not for all Cloud Providers. Currently, it is supported by AWS and GCP . You can also control file versions via CLI. See 14.3. Manage Storage via CLI . Versioned files management Management of deleted files Management of existing files Object group level (not a version) Delete a file from a data storage Rename a file Download a file Latest version Delete the latest file version Rename the latest file version Download the latest file version Previous version(s) Set a file version as the latest Download one of the previous file versions Change backup duration File versioning management system prevents users from accident deletion of files and loss of data. It allows restoring specific versions of a file. Versioned files management To manage file versions in GUI: In the Library tab choose an appropriate data storage. Choose the Show files versions option. Note : an OWNER or a user with ROLE_ADMIN role are able to turn on files versioning view, other users will not be able to see/use that option. After that, you'll be able to see file versions and files marked as deleted (but they are not actually deleted from the data storage yet). Deleted files (i.e. objects where the latest file version is set to a \" Delete marker \") are highlighted in red (\" file2.txt \" in the example below). To expand a list of versions for each file press the \" Plus \" icon. Management of deleted files Deleted files are kept in a data storage for some time. See more details about backup duration later in this document. Available operations for deleted versioned files: on the object group level (not a version) : To delete a file completely from a data storage press the \" Delete \" icon. with the latest version: To delete a file completely press the \" Delete \" icon of its latest version (expand file version list to do it). Works the same way as for object group level. with the previous version(s): These operations are the same as listed in the next section. Management of existing files Available operations for existing versioned files: Object group level (not a version) Delete a file from a data storage To delete an existing file from a data storage press the \" Delete \" icon. If \" Show files versions \" is ON - you will have the following options: \" Set deletion marker \" - \" Delete marker \" will be set and the latest version of the file will NOT be really deleted from a data storage. Users (except \" ADMIN \" and \" OWNER \") will not be able to view this file after that. \" Delete from bucket \" - delete a file from a data storage. Note : if you don't have ROLE_ADMIN or OWNER rights, you won't have these options and you won't be able to delete a file from the data storage completely (\" Delete marker \" will be set). If \" Show files versions \" is OFF - \" Set deletion marker \" option will be used by default. Rename a file To rename an existing files press the \"Rename\" icon . In this example, we will rename \"1.txt\" to \"5.txt\". To confirm the action press OK . After that, the latest version of \"1.txt\" is set to \" Delete marker \". New file appears in the folder. Download a file To download the latest version of an existing file press the \" Download \" button . Latest version All operations below work in the same way as for object group level. Delete the latest file version To delete a file press the \" Delete \" button of its latest version. Rename the latest file version To rename the latest version of an object press the \" Rename \" button of its latest version. Download the latest file version To download the latest version of an object press the \" Download \" button of its latest version. Previous version(s) Set a file version as the latest To set a chosen version as the latest press the \" Restore \" button: Download one of the previous file versions To download an appropriate previous version of the file press the \" Download \" button. Change backup duration To change the backup time in GUI: Press the \" Edit \" button . Turn on \" Enable versioning \" option. Choose Backup duration in days. If not explicitly defined, the global default value will be applied (defined in service configuration). Note : setting such rule for a Data Storage (from GUI or CLI) will tell Cloud Provider to delete previous versions of an object that are older than a specified number of days. This configuration option will be available to users with ROLE_ADMIN or OWNER rights.","title":"8.4. Control file versions"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#84-control-file-versions","text":"Note : This feature is available not for all Cloud Providers. Currently, it is supported by AWS and GCP . You can also control file versions via CLI. See 14.3. Manage Storage via CLI . Versioned files management Management of deleted files Management of existing files Object group level (not a version) Delete a file from a data storage Rename a file Download a file Latest version Delete the latest file version Rename the latest file version Download the latest file version Previous version(s) Set a file version as the latest Download one of the previous file versions Change backup duration File versioning management system prevents users from accident deletion of files and loss of data. It allows restoring specific versions of a file.","title":"8.4. Control File versions"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#versioned-files-management","text":"To manage file versions in GUI: In the Library tab choose an appropriate data storage. Choose the Show files versions option. Note : an OWNER or a user with ROLE_ADMIN role are able to turn on files versioning view, other users will not be able to see/use that option. After that, you'll be able to see file versions and files marked as deleted (but they are not actually deleted from the data storage yet). Deleted files (i.e. objects where the latest file version is set to a \" Delete marker \") are highlighted in red (\" file2.txt \" in the example below). To expand a list of versions for each file press the \" Plus \" icon.","title":"Versioned files management"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#management-of-deleted-files","text":"Deleted files are kept in a data storage for some time. See more details about backup duration later in this document. Available operations for deleted versioned files: on the object group level (not a version) : To delete a file completely from a data storage press the \" Delete \" icon. with the latest version: To delete a file completely press the \" Delete \" icon of its latest version (expand file version list to do it). Works the same way as for object group level. with the previous version(s): These operations are the same as listed in the next section.","title":"Management of deleted files"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#management-of-existing-files","text":"Available operations for existing versioned files:","title":"Management of existing files"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#object-group-level-not-a-version","text":"","title":"Object group level (not a version)"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#delete-a-file-from-a-data-storage","text":"To delete an existing file from a data storage press the \" Delete \" icon. If \" Show files versions \" is ON - you will have the following options: \" Set deletion marker \" - \" Delete marker \" will be set and the latest version of the file will NOT be really deleted from a data storage. Users (except \" ADMIN \" and \" OWNER \") will not be able to view this file after that. \" Delete from bucket \" - delete a file from a data storage. Note : if you don't have ROLE_ADMIN or OWNER rights, you won't have these options and you won't be able to delete a file from the data storage completely (\" Delete marker \" will be set). If \" Show files versions \" is OFF - \" Set deletion marker \" option will be used by default.","title":"Delete a file from a data storage"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#rename-a-file","text":"To rename an existing files press the \"Rename\" icon . In this example, we will rename \"1.txt\" to \"5.txt\". To confirm the action press OK . After that, the latest version of \"1.txt\" is set to \" Delete marker \". New file appears in the folder.","title":"Rename a file"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#download-a-file","text":"To download the latest version of an existing file press the \" Download \" button .","title":"Download a file"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#latest-version","text":"All operations below work in the same way as for object group level.","title":"Latest version"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#delete-the-latest-file-version","text":"To delete a file press the \" Delete \" button of its latest version.","title":"Delete the latest file version"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#rename-the-latest-file-version","text":"To rename the latest version of an object press the \" Rename \" button of its latest version.","title":"Rename the latest file version"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#download-the-latest-file-version","text":"To download the latest version of an object press the \" Download \" button of its latest version.","title":"Download the latest file version"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#previous-versions","text":"","title":"Previous version(s)"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#set-a-file-version-as-the-latest","text":"To set a chosen version as the latest press the \" Restore \" button:","title":"Set a file version as the latest"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#download-one-of-the-previous-file-versions","text":"To download an appropriate previous version of the file press the \" Download \" button.","title":"Download one of the previous file versions"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#change-backup-duration","text":"To change the backup time in GUI: Press the \" Edit \" button . Turn on \" Enable versioning \" option. Choose Backup duration in days. If not explicitly defined, the global default value will be applied (defined in service configuration). Note : setting such rule for a Data Storage (from GUI or CLI) will tell Cloud Provider to delete previous versions of an object that are older than a specified number of days. This configuration option will be available to users with ROLE_ADMIN or OWNER rights.","title":"Change backup duration"},{"location":"manual/08_Manage_Data_Storage/8.5._Delete_and_unregister_Data_Storage/","text":"8.5. Delete and unregister Data Storage Delete storage Unregister storage To delete a Storage you need to have WRITE permission for that storage and the ROLE_STORAGE_MANAGER role. For more details see 13. Permissions . You can also delete and unregister Storage via CLI . See 14.3. Manage Storage via CLI . Delete storage Note : If storage contains only metadata, it will not prevent deletion. Select a Data storage. Click Edit . Choose Delete . You will be offered to unregister or delete a storage. Click Delete . Unregister storage A user can unregister storage. Cloud bucket with data neither will be deleted nor will be accessible in Cloud Pipeline. Do the same actions as in Delete storage but choose to Unregister in step 4 .","title":"8.5. Delete and unregister Data Storage"},{"location":"manual/08_Manage_Data_Storage/8.5._Delete_and_unregister_Data_Storage/#85-delete-and-unregister-data-storage","text":"Delete storage Unregister storage To delete a Storage you need to have WRITE permission for that storage and the ROLE_STORAGE_MANAGER role. For more details see 13. Permissions . You can also delete and unregister Storage via CLI . See 14.3. Manage Storage via CLI .","title":"8.5. Delete and unregister Data Storage"},{"location":"manual/08_Manage_Data_Storage/8.5._Delete_and_unregister_Data_Storage/#delete-storage","text":"Note : If storage contains only metadata, it will not prevent deletion. Select a Data storage. Click Edit . Choose Delete . You will be offered to unregister or delete a storage. Click Delete .","title":"Delete storage"},{"location":"manual/08_Manage_Data_Storage/8.5._Delete_and_unregister_Data_Storage/#unregister-storage","text":"A user can unregister storage. Cloud bucket with data neither will be deleted nor will be accessible in Cloud Pipeline. Do the same actions as in Delete storage but choose to Unregister in step 4 .","title":"Unregister storage"},{"location":"manual/08_Manage_Data_Storage/8.6._Delete_Files_and_Folders_from_Storage/","text":"8.6. Delete Files and Folders from Storage Note : when you delete a folder, all child folders and files will be removed as well. Click the Delete button in the right side of the row with file or folder you want to delete. Confirm the action.","title":"8.6. Delete Files and Folders from Data Storage"},{"location":"manual/08_Manage_Data_Storage/8.6._Delete_Files_and_Folders_from_Storage/#86-delete-files-and-folders-from-storage","text":"Note : when you delete a folder, all child folders and files will be removed as well. Click the Delete button in the right side of the row with file or folder you want to delete. Confirm the action.","title":"8.6. Delete Files and Folders from Storage"},{"location":"manual/08_Manage_Data_Storage/8.7._Create_shared_file_system/","text":"8.7. Create shared file system Create FS mount FS storage features User shall have ROLE_ADMIN to mount FS to the Cloud Pipeline. For more information see 13. Permissions . A shared file system is a data storage based on network file system. It has several advantages over regular data storages and local file system: While regular data storage is a great option for a long-term storage, it cannot be used as a shared file system for high-performance computing jobs as it does not support network-like interface. A local disk cannot be shared across several nodes. A user needs to specify local disk size when scheduling a run. If a user specifies a size that is not enough to finish a job - it will fail. Cloud-based shared file system could be used to workaround this issue. Create FS mount Navigate to a desired folder and click + Create \u2192 Storages \u2192 Create new FS mount . Note : For FS mounts - \"Add existing\" option is not available. Note : For the correct FS mount creation, at least one mount point shall be registered in the System Preferences for any Cloud Region. If no - the corresponding button of the FS mount creation becomes invisible: Specify Storage path and other optional parameters. Note : Storage path parameter contains FS mount path and the name of the storage to be created. Note : FS storages are just subdirectories of the mounted FS. One FS mount can have multiple FS storages. When deleted from GUI, FS storage is unmounted from the Cloud Pipeline. FS storage features For FS storages GUI doesn't display the following features typical for regular data storages: STS LTS Versioning and Backup duration. When a user selects Input/Common/Output path parameter for a pipeline run - it is impossible to set FS storage: FS storages aren't displayed in the \"Browse...\" dialog for Input/Common/Output path parameters; Value of Input/Common/Output path parameters is validated so that user is not able to specify a path to FS storage manually. The content of files stored in FS data storage could be previewed as well as in regular data storages: Since FS isn't an object data storage, it isn't possible to add metadata tags to files in the FS storage. Use FS storage between cluster nodes. If pipeline Tools contain FS client, FS storage(s) will be mounted automatically.","title":"8.7. Create shared file system"},{"location":"manual/08_Manage_Data_Storage/8.7._Create_shared_file_system/#87-create-shared-file-system","text":"Create FS mount FS storage features User shall have ROLE_ADMIN to mount FS to the Cloud Pipeline. For more information see 13. Permissions . A shared file system is a data storage based on network file system. It has several advantages over regular data storages and local file system: While regular data storage is a great option for a long-term storage, it cannot be used as a shared file system for high-performance computing jobs as it does not support network-like interface. A local disk cannot be shared across several nodes. A user needs to specify local disk size when scheduling a run. If a user specifies a size that is not enough to finish a job - it will fail. Cloud-based shared file system could be used to workaround this issue.","title":"8.7. Create shared file system"},{"location":"manual/08_Manage_Data_Storage/8.7._Create_shared_file_system/#create-fs-mount","text":"Navigate to a desired folder and click + Create \u2192 Storages \u2192 Create new FS mount . Note : For FS mounts - \"Add existing\" option is not available. Note : For the correct FS mount creation, at least one mount point shall be registered in the System Preferences for any Cloud Region. If no - the corresponding button of the FS mount creation becomes invisible: Specify Storage path and other optional parameters. Note : Storage path parameter contains FS mount path and the name of the storage to be created. Note : FS storages are just subdirectories of the mounted FS. One FS mount can have multiple FS storages. When deleted from GUI, FS storage is unmounted from the Cloud Pipeline.","title":"Create FS mount"},{"location":"manual/08_Manage_Data_Storage/8.7._Create_shared_file_system/#fs-storage-features","text":"For FS storages GUI doesn't display the following features typical for regular data storages: STS LTS Versioning and Backup duration. When a user selects Input/Common/Output path parameter for a pipeline run - it is impossible to set FS storage: FS storages aren't displayed in the \"Browse...\" dialog for Input/Common/Output path parameters; Value of Input/Common/Output path parameters is validated so that user is not able to specify a path to FS storage manually. The content of files stored in FS data storage could be previewed as well as in regular data storages: Since FS isn't an object data storage, it isn't possible to add metadata tags to files in the FS storage. Use FS storage between cluster nodes. If pipeline Tools contain FS client, FS storage(s) will be mounted automatically.","title":"FS storage features"},{"location":"manual/08_Manage_Data_Storage/8.8._Data_sharing/","text":"8.8. Data sharing Create shared storage Upload data to shared storage Download data from shared storage To create a Shared storage in a Folder you need to have WRITE permission for that folder and the ROLE_STORAGE_MANAGER role. For more information see 13. Permissions . Users can share data storages within a Cloud Platform for enabling of getting data files by the external partners for processing. Create shared storage For the ability of getting data files by the external partners, users should be considered, that external partner has own CP account and R/W permissions for a storage. Start creating a new object storage (for more details see here ), fill Info items. Set Enable sharing . Click Create button: Open created storage by clicking on it in the folder tree ( 1 ). Click icon in upper right corner ( 2 ): Choose Permissions tab in opened pop-up window: Click on button, enter user, for whom you want to share created storage. Confirm by clicking \" Ok \" button: Click on user name. If you want your partner can only download data from creating shared space, set Allow checkbox for READ permission, set Deny checkbox for WRITE permission: If you want your partner can download data from creating shared spaced and upload on it, set Allow checkbox both for READ and WRITE permissions: Close pop-up window. Click button. In the pop-up window generated URL will be appeared. It can be shared with the external collaborator. Upload data to shared storage Shared storage's collaboration space can be used to exchange large data files (up to 5Tb per one file). For storage owner : Uploading data to shared storage is doing in the same way as on a regular. For more details see here . For external partner : Note : for uploading to shared storage, user account shall be registered within CP users catalog and granted READ WRITE permissions for that storage. Open new browser window and insert URL, that you receive from the partner. In appeared login page enter user credentials and sign in. The following page will be opened: Click button. In opened pop-up window browse file(s) to upload. Confirm uploading. Note : make sure size doesn't exceed 5 Tb. Note : you can cancel upload process by clicking the \"Cancel\" button: As a result, the file(s) will be uploaded to the shared storage: Download data from shared storage For storage owner : Downloading data from shared storage is doing in the same way as from a regular. For more details see here . For external partner : Note : for downloading from shared storage, user account shall be registered within CP users catalog and granted READ permission for that storage. Open new browser window and insert URL, that you receive from the partner. In appeared login page enter user credentials and sign in. The following page will be opened: Click to download required file. Specify where to download in the pop-up window. As a result, the file will be downloaded via your browser to the specified location.","title":"8.8. Data sharing"},{"location":"manual/08_Manage_Data_Storage/8.8._Data_sharing/#88-data-sharing","text":"Create shared storage Upload data to shared storage Download data from shared storage To create a Shared storage in a Folder you need to have WRITE permission for that folder and the ROLE_STORAGE_MANAGER role. For more information see 13. Permissions . Users can share data storages within a Cloud Platform for enabling of getting data files by the external partners for processing.","title":"8.8. Data sharing"},{"location":"manual/08_Manage_Data_Storage/8.8._Data_sharing/#create-shared-storage","text":"For the ability of getting data files by the external partners, users should be considered, that external partner has own CP account and R/W permissions for a storage. Start creating a new object storage (for more details see here ), fill Info items. Set Enable sharing . Click Create button: Open created storage by clicking on it in the folder tree ( 1 ). Click icon in upper right corner ( 2 ): Choose Permissions tab in opened pop-up window: Click on button, enter user, for whom you want to share created storage. Confirm by clicking \" Ok \" button: Click on user name. If you want your partner can only download data from creating shared space, set Allow checkbox for READ permission, set Deny checkbox for WRITE permission: If you want your partner can download data from creating shared spaced and upload on it, set Allow checkbox both for READ and WRITE permissions: Close pop-up window. Click button. In the pop-up window generated URL will be appeared. It can be shared with the external collaborator.","title":"Create shared storage"},{"location":"manual/08_Manage_Data_Storage/8.8._Data_sharing/#upload-data-to-shared-storage","text":"Shared storage's collaboration space can be used to exchange large data files (up to 5Tb per one file). For storage owner : Uploading data to shared storage is doing in the same way as on a regular. For more details see here . For external partner : Note : for uploading to shared storage, user account shall be registered within CP users catalog and granted READ WRITE permissions for that storage. Open new browser window and insert URL, that you receive from the partner. In appeared login page enter user credentials and sign in. The following page will be opened: Click button. In opened pop-up window browse file(s) to upload. Confirm uploading. Note : make sure size doesn't exceed 5 Tb. Note : you can cancel upload process by clicking the \"Cancel\" button: As a result, the file(s) will be uploaded to the shared storage:","title":"Upload data to shared storage"},{"location":"manual/08_Manage_Data_Storage/8.8._Data_sharing/#download-data-from-shared-storage","text":"For storage owner : Downloading data from shared storage is doing in the same way as from a regular. For more details see here . For external partner : Note : for downloading from shared storage, user account shall be registered within CP users catalog and granted READ permission for that storage. Open new browser window and insert URL, that you receive from the partner. In appeared login page enter user credentials and sign in. The following page will be opened: Click to download required file. Specify where to download in the pop-up window. As a result, the file will be downloaded via your browser to the specified location.","title":"Download data from shared storage"},{"location":"manual/08_Manage_Data_Storage/8.9._Mapping_storages/","text":"8.9. Mount Cloud Data Storage to the local workstation Introduction Cloud Pipeline platform allows to mount data storages, located in the Cloud, to the local workstation as a network drive. This provides an easy way to manage files, that shall be processed in the Cloud or downloaded locally. Authenticate within Cloud Pipeline platform and obtain mapping token Map a storage as the network drive to the Windows workstation using built-in tools Map a storage using CyberDuck tool Please consider the following limitations and prerequisites to map a network drive: 1. Network drive mapping is only supported for Windows workstations. 2. Internet Explorer 11 shall be used for Authentication section of this manual. Once authentication is done - Chrome or other web-browser can be used to further work. 3. Only NFS data storages (e.g. EFS/NFS/SMB) can be mapped to the local workstation. S3/AZ/GCP storages are not supported. 4. The following limits are applied to the data transfer: - Max 500 Mb per file - Max transfer duration: 30 min 5. The following requirements shall be set for the user account: - User shall be granted write permissions to any of the NFS data storages Authenticate within Cloud Pipeline platform and obtain mapping token Open Internet Explorer web-browser Navigate to the Cloud Pipeline GUI - https:// cloud pipeline adress /pipeline/ Click the Settings button in the left menu Navigate to the CLI tab and select Drive mapping section in the left menu: Authentication prompt will be shown: Click the Authenticate button If authentication succeeds you will see a popup window with a link to map a network drive. Copy the URL shown in the popup, it will be used in the workstation configuration section: Map a storage as the network drive to the Windows workstation using built-in tools Open This PC ( My Computer ) view Click the Map network drive button: Map Network Drive dialog will be shown: Select a Drive letter (it will be used to address a drive, e.g. Z:\\ ) or keep the default value for the \"Drive\" field Paste a mapping URL (obtained from the Cloud Pipeline authentication dialog) into the \"Folder\" field Tick \" Reconnect at logon \" checkbox Click the Finish button: Drive will be mapped and opened. Further you can address it using a Drive letter set above (e.g. Z:\\ ): Map a storage using CyberDuck tool Prerequisites : get a mapping URL by the way described above . Launch CyberDuck tool. Click the Open Connection button: In the appeared popup, click the upper dropdown list with connection types: Select the item \" WebDAV (HTTPS) \": Paste a mapping URL (from the prerequisites) into the \"Server\" field. Fields \"URL\", \"Port\", \"Path\" will be filled in automatically: Specify your Cloud Pipeline credentials - username into the \"Username\" field, access key into the \"Password\" field: Note : to get your access token - open System Settings , then CLI tab, and in the sub-tab Pipe CLI , click the \" Generate access key \" button: Click the Connect button to confirm. After authentication will be done, all available to the user FS mounts will appear: They are displayed as folders. Double-click the folder you wish - FS storage content will appear, e.g.: In this window, you can manage files/folders as in any file commander, according to your permissions on that FS mount. More about Cyberduck functionality you may see here . After all, close the connection:","title":"8.9. Mapping storages"},{"location":"manual/08_Manage_Data_Storage/8.9._Mapping_storages/#89-mount-cloud-data-storage-to-the-local-workstation","text":"","title":"8.9. Mount Cloud Data Storage to the local workstation"},{"location":"manual/08_Manage_Data_Storage/8.9._Mapping_storages/#introduction","text":"Cloud Pipeline platform allows to mount data storages, located in the Cloud, to the local workstation as a network drive. This provides an easy way to manage files, that shall be processed in the Cloud or downloaded locally. Authenticate within Cloud Pipeline platform and obtain mapping token Map a storage as the network drive to the Windows workstation using built-in tools Map a storage using CyberDuck tool Please consider the following limitations and prerequisites to map a network drive: 1. Network drive mapping is only supported for Windows workstations. 2. Internet Explorer 11 shall be used for Authentication section of this manual. Once authentication is done - Chrome or other web-browser can be used to further work. 3. Only NFS data storages (e.g. EFS/NFS/SMB) can be mapped to the local workstation. S3/AZ/GCP storages are not supported. 4. The following limits are applied to the data transfer: - Max 500 Mb per file - Max transfer duration: 30 min 5. The following requirements shall be set for the user account: - User shall be granted write permissions to any of the NFS data storages","title":"Introduction"},{"location":"manual/08_Manage_Data_Storage/8.9._Mapping_storages/#authenticate-within-cloud-pipeline-platform-and-obtain-mapping-token","text":"Open Internet Explorer web-browser Navigate to the Cloud Pipeline GUI - https:// cloud pipeline adress /pipeline/ Click the Settings button in the left menu Navigate to the CLI tab and select Drive mapping section in the left menu: Authentication prompt will be shown: Click the Authenticate button If authentication succeeds you will see a popup window with a link to map a network drive. Copy the URL shown in the popup, it will be used in the workstation configuration section:","title":"Authenticate within Cloud Pipeline platform and obtain mapping token"},{"location":"manual/08_Manage_Data_Storage/8.9._Mapping_storages/#map-a-storage-as-the-network-drive-to-the-windows-workstation-using-built-in-tools","text":"Open This PC ( My Computer ) view Click the Map network drive button: Map Network Drive dialog will be shown: Select a Drive letter (it will be used to address a drive, e.g. Z:\\ ) or keep the default value for the \"Drive\" field Paste a mapping URL (obtained from the Cloud Pipeline authentication dialog) into the \"Folder\" field Tick \" Reconnect at logon \" checkbox Click the Finish button: Drive will be mapped and opened. Further you can address it using a Drive letter set above (e.g. Z:\\ ):","title":"Map a storage as the network drive to the Windows workstation using built-in tools"},{"location":"manual/08_Manage_Data_Storage/8.9._Mapping_storages/#map-a-storage-using-cyberduck-tool","text":"Prerequisites : get a mapping URL by the way described above . Launch CyberDuck tool. Click the Open Connection button: In the appeared popup, click the upper dropdown list with connection types: Select the item \" WebDAV (HTTPS) \": Paste a mapping URL (from the prerequisites) into the \"Server\" field. Fields \"URL\", \"Port\", \"Path\" will be filled in automatically: Specify your Cloud Pipeline credentials - username into the \"Username\" field, access key into the \"Password\" field: Note : to get your access token - open System Settings , then CLI tab, and in the sub-tab Pipe CLI , click the \" Generate access key \" button: Click the Connect button to confirm. After authentication will be done, all available to the user FS mounts will appear: They are displayed as folders. Double-click the folder you wish - FS storage content will appear, e.g.: In this window, you can manage files/folders as in any file commander, according to your permissions on that FS mount. More about Cyberduck functionality you may see here . After all, close the connection:","title":"Map a storage using CyberDuck tool"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/","text":"8. Manage Data Storage Data storage is a Cloud Pipeline object that represents cloud storage and its content in a folder hierarchy. Controls Select page Show file versions Remove all selected Generate URL Show attributes/Hide attributes \"Gear\" icon Refresh + Create Upload Each-line controls View and edit a text file CLI Storage options Permissions management for a storage is described here . \"Details view\" lists content of the storage: files that may be organized into folders. Clicking on the inside folder will open its content in the \"Details view\" . Note : storage's folders hierarchy will not be represented in the \"Hierarchy view\" panel. Note : you can move Storage to a new parent folder using drag and drop approach. In \"Library tree view\" and \"Details view\" data storages are tagged with region flag to visually distinguish storage locations. Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - in that case auxiliary Cloud Provider icons are additionally displayed, e.g.: Figure 1 Another option for navigation in the storage is to use \"breadcrumbs\" control at the top of the \"Details\" view (see picture above, 1 ): Clicking an item will navigate to that folder. Editing a path will allow to copy/paste a path and navigate to any custom location. Controls At the top of the \"Details\" view there are buttons: Select page Clicking this control (see picture Figure 1 above, item 2 ), the whole file and folders on the current page will be selected. It allows to perform bulk operations like deleting. Show file versions This feature is available not for all Cloud Providers. Currently, it is supported by AWS and GCP . Tick this checkbox (see picture Figure 1 above, item 3 ) and the view of a page will changed: the all file versions will be displayed. You can expand each version's list by clicking \" + \" in desired line. Note : the last version will be marked by \" (latest) \". Remove all selected This is a bulk operation control. It is visible, if at least one of the data storage item (folder or file) is selected. Generate URL This control helps to generate URLs for a number of files and then download them manually one by one or via scripts. See details here . Note : the control is available, if only files are selected. Show attributes/Hide attributes Allows see or edit a list of key=value attributes of the data storage (see picture Figure 1 above, item 4 ). Note : If selected storage has any defined attribute, Attributes pane is shown by default. See 17. CP objects tagging by additional attributes . \"Gear\" icon Allows to edit the path, alias, description of the storage, manage its STS and LTS durations and enable versions control (see picture Figure 1 above, item 5 ). The delete option is also here (if storage contains only metadata, it will be deleted anyway). See 8.1. Create and edit storage . Refresh Allows updating representation of storage's contents (see picture Figure 1 above, item 6 ). + Create You can also create new folders and files via this button (see picture Figure 1 above, item 7 ). See 8.3. Create and Edit text files . Upload This control allows uploading files to the storage (see picture Figure 1 above, item 8 ). See 8.2. Upload/Download data . Each-line controls Control Description Download This control calls downloading of selected file. Edit Helps to rename a file or a folder. Delete Delete a file or a folder. View and edit a text file You can view and edit text files. For more details see here . CLI Storage options There are also several options that are only implemented in CLI but not in GUI: To move files and folders from one storage to another or between local file system and storage. To copy files from one storage to another. See here for more details.","title":"8.0. Overview"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#8-manage-data-storage","text":"Data storage is a Cloud Pipeline object that represents cloud storage and its content in a folder hierarchy. Controls Select page Show file versions Remove all selected Generate URL Show attributes/Hide attributes \"Gear\" icon Refresh + Create Upload Each-line controls View and edit a text file CLI Storage options Permissions management for a storage is described here . \"Details view\" lists content of the storage: files that may be organized into folders. Clicking on the inside folder will open its content in the \"Details view\" . Note : storage's folders hierarchy will not be represented in the \"Hierarchy view\" panel. Note : you can move Storage to a new parent folder using drag and drop approach. In \"Library tree view\" and \"Details view\" data storages are tagged with region flag to visually distinguish storage locations. Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - in that case auxiliary Cloud Provider icons are additionally displayed, e.g.: Figure 1 Another option for navigation in the storage is to use \"breadcrumbs\" control at the top of the \"Details\" view (see picture above, 1 ): Clicking an item will navigate to that folder. Editing a path will allow to copy/paste a path and navigate to any custom location.","title":"8. Manage Data Storage"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#controls","text":"At the top of the \"Details\" view there are buttons:","title":"Controls"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#select-page","text":"Clicking this control (see picture Figure 1 above, item 2 ), the whole file and folders on the current page will be selected. It allows to perform bulk operations like deleting.","title":"Select page"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#show-file-versions","text":"This feature is available not for all Cloud Providers. Currently, it is supported by AWS and GCP . Tick this checkbox (see picture Figure 1 above, item 3 ) and the view of a page will changed: the all file versions will be displayed. You can expand each version's list by clicking \" + \" in desired line. Note : the last version will be marked by \" (latest) \".","title":"Show file versions"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#remove-all-selected","text":"This is a bulk operation control. It is visible, if at least one of the data storage item (folder or file) is selected.","title":"Remove all selected"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#generate-url","text":"This control helps to generate URLs for a number of files and then download them manually one by one or via scripts. See details here . Note : the control is available, if only files are selected.","title":"Generate URL"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#show-attributeshide-attributes","text":"Allows see or edit a list of key=value attributes of the data storage (see picture Figure 1 above, item 4 ). Note : If selected storage has any defined attribute, Attributes pane is shown by default. See 17. CP objects tagging by additional attributes .","title":"Show attributes/Hide attributes"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#gear-icon","text":"Allows to edit the path, alias, description of the storage, manage its STS and LTS durations and enable versions control (see picture Figure 1 above, item 5 ). The delete option is also here (if storage contains only metadata, it will be deleted anyway). See 8.1. Create and edit storage .","title":"\"Gear\" icon"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#refresh","text":"Allows updating representation of storage's contents (see picture Figure 1 above, item 6 ).","title":"Refresh"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#create","text":"You can also create new folders and files via this button (see picture Figure 1 above, item 7 ). See 8.3. Create and Edit text files .","title":"+ Create"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#upload","text":"This control allows uploading files to the storage (see picture Figure 1 above, item 8 ). See 8.2. Upload/Download data .","title":"Upload"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#each-line-controls","text":"Control Description Download This control calls downloading of selected file. Edit Helps to rename a file or a folder. Delete Delete a file or a folder.","title":"Each-line controls"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#view-and-edit-a-text-file","text":"You can view and edit text files. For more details see here .","title":"View and edit a text file"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#cli-storage-options","text":"There are also several options that are only implemented in CLI but not in GUI: To move files and folders from one storage to another or between local file system and storage. To copy files from one storage to another. See here for more details.","title":"CLI Storage options"},{"location":"manual/09_Manage_Cluster_nodes/9._Manage_Cluster_nodes/","text":"9. Manage Cluster nodes \" Cluster nodes \" provides a list of working nodes. You can get information on nodes usage and terminate them in this tab. Overview Controls Node information page GENERAL INFO JOBS MONITOR Filters Zoom and scroll Export data Note : Nodes remain for the time that is already paid for, even if all runs at the node finished execution. So if you restart pipeline, new nodes will not be initialized saving time and money. Overview This tab shows Active nodes table that has information about: Name - a name of the node. Pipeline - a currently assigned run on the node. Labels of the node - characteristics extracted from the parameters of the node. There are common labels: RUN ID - ID of currently assigned run, MASTER/EDGE - service labels, nodes with this labels may be viewed only by ADMIN users. Addresses - node addresses. Created - a date of creation. Controls Control Description Terminate This control terminates node. Refresh To get currently active nodes list. Note : You can also terminate a node via CLI. For more details see here . Node information page Note : You can also view node information via CLI. See 14.6. View cluster nodes via CLI . Clicking on the row of the table will redirect you to detailed node information page. This page has three tabs. GENERAL INFO This tab allows seeing general info about the node, including: System information ; Addresses of internal network and domain name; Labels of the node automatically generated in accordance with system information; Node type - amounts of available and total memory, number of jobs and CPUs. JOBS \"JOBS\" tab lists jobs being processed at the moment. Name of the job; clicking \" + \" icon next to the name expands a list of containers needed for the job. Namespace for a job to be executed at; Status of the job; Requests and Limits of resources for the job. MONITOR \"MONITOR\" tab displays a dashboard with following diagrams: Diagram Description CPU usage A diagram represents CPU usage (cores) - time graph. The usage is displayed in fractions according to left vertical axis. Memory usage A diagram represents memory usage - time graph. Blue graph represents usage in MB according to left vertical axis. Red graph represents usage in % of available amounts of memory according to right vertical axis. Network connection speed A diagram represents connection speed (bytes) - time graph. Blue graph ( TX ) represents \"transceiving\" speed. Red graph ( RX ) represents \"receive\" speed. Drop-down at the top of the section allows changing connection protocol. File system load Represents all the disks of the machine and their loading. The current state of the resources utilization is available for all active runs. The maximum storage period for this data is set by the system preference system.resource.monitoring.stats.retention.period (in days, by default - 5). So, any utilization data older than that period is unavailable for users (no matter the run duration). The historical resources utilization is also available for completed runs (during the specified time storage period). It can be useful for debugging/optimization purposes. To view the monitor of resources utilization for the completed run: Open the COMPLETED RUNS page. Click the run you wish to view the resources utilization data, e.g.: At the opened Run logs page expand the \"Instance\" section: Click the node IP hyperlink. The monitor of the node resources utilization will appear: Please note, the resources utilization data for the completed run is available during system.resource.monitoring.stats.retention.period days. If you'll try to view monitor of the completed run after the specified period is over - the monitor will be empty. Filters User can manage plots date configurations. For this purpose the system has number of filters: Common range for all charts (1) User can synchronize the time period for all charts. To do so user should mark this filter. If this filter is unmarked, user can zoom or scroll any plot without any change for others. Live update (2) If this checkbox is marked the charts data will be updated every 5 seconds in a real-time manner. The fields with dates will be updated as well. This filter can be marked only in pair with the Common range for all charts filter. If both checkboxes were unmarked and user set the Live update filter active, the system would mark both checkboxes. This feature is available only for active runs. Set range (3) User can select the predefined time range for all plots from the list: Whole range Last week Last day Last hour This filter works in pair with the Common range for all charts filter. If user sets the date range, the system will mark the Common range for all charts checkbox, if it wasn't. So the data in all charts will be filtered by the selected range. Date filter (4) User can specify the Start and the End dates for plots using this filter. By default, the Start date (the left field of the filter) is the node creating datetime, the End date (the right field of the filter) is the current datetime. To change the Start \\/ End date the user should: click the corresponding date field, e.g.: the calendar will be displayed: The dates before the node creation and after today will be unavailable to select: click the specific available date in the calendar. Selected date will appear in the date field. Charts will be automatically redrawn according to the new set period. If the user focuses on the calendar icon or the whole field at any of date fields the \"Cross\" button will be displayed: If click this button in the Start date field - the node creation date will be substituted into that filter. If click this button in the End date field - the date in that field will be erased and the system will interpret it as the current datetime. Zooming and scrolling features In addition, user can scroll plots. To do so: focus on the plot, hold the left mouse button and move the mouse in the desired direction (left or right) Note : if Common range for all charts filter is on, all charts will be moving simultaneously Another feature is chart zooming . To zoom a chart: hold the Shift key and scroll the plot via mouse. The area will be highlighted: Then release the Shift key and the highlighted area will be automatically zoomed. another way of zooming plot - using the right panel. There are Plus and Minus buttons for such purpose on it: Click the desired button and the chart will be zoomed. Export utilization data Users have the ability to export the utilization information into a .csv file. This can be useful, if the user wants to keep locally the information for a longer period of time than defined by the preference system.resource.monitoring.stats.retention.period . To export resources utilization data at the Monitor page of the node: Hover over the Export button in the right upper corner of the page: In the appeared list (the list size depends on the run duration) select the interval of resources utilization statistics you wish to export, e.g.: The corresponding .csv file will be downloaded automatically. Example of such file:","title":"9. Manage cluster nodes"},{"location":"manual/09_Manage_Cluster_nodes/9._Manage_Cluster_nodes/#9-manage-cluster-nodes","text":"\" Cluster nodes \" provides a list of working nodes. You can get information on nodes usage and terminate them in this tab. Overview Controls Node information page GENERAL INFO JOBS MONITOR Filters Zoom and scroll Export data Note : Nodes remain for the time that is already paid for, even if all runs at the node finished execution. So if you restart pipeline, new nodes will not be initialized saving time and money.","title":"9. Manage Cluster nodes"},{"location":"manual/09_Manage_Cluster_nodes/9._Manage_Cluster_nodes/#overview","text":"This tab shows Active nodes table that has information about: Name - a name of the node. Pipeline - a currently assigned run on the node. Labels of the node - characteristics extracted from the parameters of the node. There are common labels: RUN ID - ID of currently assigned run, MASTER/EDGE - service labels, nodes with this labels may be viewed only by ADMIN users. Addresses - node addresses. Created - a date of creation.","title":"Overview"},{"location":"manual/09_Manage_Cluster_nodes/9._Manage_Cluster_nodes/#controls","text":"Control Description Terminate This control terminates node. Refresh To get currently active nodes list. Note : You can also terminate a node via CLI. For more details see here .","title":"Controls"},{"location":"manual/09_Manage_Cluster_nodes/9._Manage_Cluster_nodes/#node-information-page","text":"Note : You can also view node information via CLI. See 14.6. View cluster nodes via CLI . Clicking on the row of the table will redirect you to detailed node information page. This page has three tabs.","title":"Node information page"},{"location":"manual/09_Manage_Cluster_nodes/9._Manage_Cluster_nodes/#general-info","text":"This tab allows seeing general info about the node, including: System information ; Addresses of internal network and domain name; Labels of the node automatically generated in accordance with system information; Node type - amounts of available and total memory, number of jobs and CPUs.","title":"GENERAL INFO"},{"location":"manual/09_Manage_Cluster_nodes/9._Manage_Cluster_nodes/#jobs","text":"\"JOBS\" tab lists jobs being processed at the moment. Name of the job; clicking \" + \" icon next to the name expands a list of containers needed for the job. Namespace for a job to be executed at; Status of the job; Requests and Limits of resources for the job.","title":"JOBS"},{"location":"manual/09_Manage_Cluster_nodes/9._Manage_Cluster_nodes/#monitor","text":"\"MONITOR\" tab displays a dashboard with following diagrams: Diagram Description CPU usage A diagram represents CPU usage (cores) - time graph. The usage is displayed in fractions according to left vertical axis. Memory usage A diagram represents memory usage - time graph. Blue graph represents usage in MB according to left vertical axis. Red graph represents usage in % of available amounts of memory according to right vertical axis. Network connection speed A diagram represents connection speed (bytes) - time graph. Blue graph ( TX ) represents \"transceiving\" speed. Red graph ( RX ) represents \"receive\" speed. Drop-down at the top of the section allows changing connection protocol. File system load Represents all the disks of the machine and their loading. The current state of the resources utilization is available for all active runs. The maximum storage period for this data is set by the system preference system.resource.monitoring.stats.retention.period (in days, by default - 5). So, any utilization data older than that period is unavailable for users (no matter the run duration). The historical resources utilization is also available for completed runs (during the specified time storage period). It can be useful for debugging/optimization purposes. To view the monitor of resources utilization for the completed run: Open the COMPLETED RUNS page. Click the run you wish to view the resources utilization data, e.g.: At the opened Run logs page expand the \"Instance\" section: Click the node IP hyperlink. The monitor of the node resources utilization will appear: Please note, the resources utilization data for the completed run is available during system.resource.monitoring.stats.retention.period days. If you'll try to view monitor of the completed run after the specified period is over - the monitor will be empty.","title":"MONITOR"},{"location":"manual/09_Manage_Cluster_nodes/9._Manage_Cluster_nodes/#filters","text":"User can manage plots date configurations. For this purpose the system has number of filters: Common range for all charts (1) User can synchronize the time period for all charts. To do so user should mark this filter. If this filter is unmarked, user can zoom or scroll any plot without any change for others. Live update (2) If this checkbox is marked the charts data will be updated every 5 seconds in a real-time manner. The fields with dates will be updated as well. This filter can be marked only in pair with the Common range for all charts filter. If both checkboxes were unmarked and user set the Live update filter active, the system would mark both checkboxes. This feature is available only for active runs. Set range (3) User can select the predefined time range for all plots from the list: Whole range Last week Last day Last hour This filter works in pair with the Common range for all charts filter. If user sets the date range, the system will mark the Common range for all charts checkbox, if it wasn't. So the data in all charts will be filtered by the selected range. Date filter (4) User can specify the Start and the End dates for plots using this filter. By default, the Start date (the left field of the filter) is the node creating datetime, the End date (the right field of the filter) is the current datetime. To change the Start \\/ End date the user should: click the corresponding date field, e.g.: the calendar will be displayed: The dates before the node creation and after today will be unavailable to select: click the specific available date in the calendar. Selected date will appear in the date field. Charts will be automatically redrawn according to the new set period. If the user focuses on the calendar icon or the whole field at any of date fields the \"Cross\" button will be displayed: If click this button in the Start date field - the node creation date will be substituted into that filter. If click this button in the End date field - the date in that field will be erased and the system will interpret it as the current datetime.","title":"Filters"},{"location":"manual/09_Manage_Cluster_nodes/9._Manage_Cluster_nodes/#zooming-and-scrolling-features","text":"In addition, user can scroll plots. To do so: focus on the plot, hold the left mouse button and move the mouse in the desired direction (left or right) Note : if Common range for all charts filter is on, all charts will be moving simultaneously Another feature is chart zooming . To zoom a chart: hold the Shift key and scroll the plot via mouse. The area will be highlighted: Then release the Shift key and the highlighted area will be automatically zoomed. another way of zooming plot - using the right panel. There are Plus and Minus buttons for such purpose on it: Click the desired button and the chart will be zoomed.","title":"Zooming and scrolling features"},{"location":"manual/09_Manage_Cluster_nodes/9._Manage_Cluster_nodes/#export-utilization-data","text":"Users have the ability to export the utilization information into a .csv file. This can be useful, if the user wants to keep locally the information for a longer period of time than defined by the preference system.resource.monitoring.stats.retention.period . To export resources utilization data at the Monitor page of the node: Hover over the Export button in the right upper corner of the page: In the appeared list (the list size depends on the run duration) select the interval of resources utilization statistics you wish to export, e.g.: The corresponding .csv file will be downloaded automatically. Example of such file:","title":"Export utilization data"},{"location":"manual/10_Manage_Tools/10.1._Add_Edit_a_Docker_registry/","text":"10.1. Add/Edit a Docker registry Add registry Edit/delete registry Customize registry permissions A docker registry is a storage and content delivery system, holding named Docker images, available in different tagged versions. This page describes the process of adding and editing Docker registries to the Cloud Pipeline . Also here you will find information about permission management for Docker registries. Only administrators can create Docker registries. To edit its parameters you need to have WRITE permissions. For more information see 13. Permissions . Add registry Configured Docker registry can be added to the Cloud Pipeline with the following commands: In the Tools tab click the Gear icon \u2192 Registry \u2192 + Create . Then set the registry parameters: Path (mandatory field) - IP address/domain name of the machine with configured Docker registry. Description - registry description. Require security scanning - tick the box to allow scheduled security scanning procedure for the Tools in the registry. For more information see 10.6. Tool security check . User name * and Password * (optional field) - if the registry is closed, you have to set a username and a password. Certificate * - if Docker registry uses a self-signed certificate, upload it with the Choose file button to set HTTPS access to the registry. External URL * - URL that can be exploited to push/pull Docker images to/from a registry. Pipeline authentication * - tick to make registry use Cloud Pipeline authentication system. * click Edit credentials to get these fields. You'll see a new registry in the registry list. Edit/delete registry To edit/view registry attributes - see 17. CP objects tagging by additional attributes . Choose registry from the registry list and click the Gear icon \u2192 Registry \u2192 Edit . You'll be able to modify registry parameters in the Info tab ( a ). Note : you can modify all registry parameters except Path . If you wish to detach registry from the Cloud Pipeline - click the Delete button ( b ). Customize registry permissions Users with ROLE_ADMIN or OWNER rights can modify permissions for a registry. For detailed instruction refer to this document .","title":"10.1. Add and edit a Docker registry"},{"location":"manual/10_Manage_Tools/10.1._Add_Edit_a_Docker_registry/#101-addedit-a-docker-registry","text":"Add registry Edit/delete registry Customize registry permissions A docker registry is a storage and content delivery system, holding named Docker images, available in different tagged versions. This page describes the process of adding and editing Docker registries to the Cloud Pipeline . Also here you will find information about permission management for Docker registries. Only administrators can create Docker registries. To edit its parameters you need to have WRITE permissions. For more information see 13. Permissions .","title":"10.1. Add/Edit a Docker registry"},{"location":"manual/10_Manage_Tools/10.1._Add_Edit_a_Docker_registry/#add-registry","text":"Configured Docker registry can be added to the Cloud Pipeline with the following commands: In the Tools tab click the Gear icon \u2192 Registry \u2192 + Create . Then set the registry parameters: Path (mandatory field) - IP address/domain name of the machine with configured Docker registry. Description - registry description. Require security scanning - tick the box to allow scheduled security scanning procedure for the Tools in the registry. For more information see 10.6. Tool security check . User name * and Password * (optional field) - if the registry is closed, you have to set a username and a password. Certificate * - if Docker registry uses a self-signed certificate, upload it with the Choose file button to set HTTPS access to the registry. External URL * - URL that can be exploited to push/pull Docker images to/from a registry. Pipeline authentication * - tick to make registry use Cloud Pipeline authentication system. * click Edit credentials to get these fields. You'll see a new registry in the registry list.","title":"Add registry"},{"location":"manual/10_Manage_Tools/10.1._Add_Edit_a_Docker_registry/#editdelete-registry","text":"To edit/view registry attributes - see 17. CP objects tagging by additional attributes . Choose registry from the registry list and click the Gear icon \u2192 Registry \u2192 Edit . You'll be able to modify registry parameters in the Info tab ( a ). Note : you can modify all registry parameters except Path . If you wish to detach registry from the Cloud Pipeline - click the Delete button ( b ).","title":"Edit/delete registry"},{"location":"manual/10_Manage_Tools/10.1._Add_Edit_a_Docker_registry/#customize-registry-permissions","text":"Users with ROLE_ADMIN or OWNER rights can modify permissions for a registry. For detailed instruction refer to this document .","title":"Customize registry permissions"},{"location":"manual/10_Manage_Tools/10.2._Add_Edit_a_Tool_group/","text":"10.2. Add/Edit a Tool group Add a generic Tool group Add a personal Tool group Edit/delete a Tool group Customize Tool group permissions A Tool group is an object of the Cloud Pipeline that allows to organize Tools into groups. This page describes the process of adding and editing Tool groups. Also here you will find information about permission management for Tool groups. To create a Tool group, user need to have WRITE permission for a Docker registry and the ROLE_TOOL_GROUP_MANAGER role. To edit Tool group parameters you need to have WRITE permissions for it. For more information see 13. Permissions . Add a generic Tool group A Tool group can be added to a Docker registry in the following way: In the Tools tab click the Gear icon \u2192 Group \u2192 + Create . Give your Tool group a name and an optional description : Click the CREATE button to confirm. You'll be automatically redirected to the new Tool group in the current Docker registry. Add a personal Tool group In the Tools tab click the Gear icon \u2192 Group \u2192 + Create personal . Another way to do that is to select personal Tool group from the Tool group list And then press the Create personal tool group button. After that you'll be able to upload Tools to your personal Tool group. Edit/delete a Tool group Choose a Tool group and click the Gear icon \u2192 Group \u2192 Edit . You'll be able to modify Tool group description in the Info tab. If you wish to delete a Tool group: click the Gear icon \u2192 Group \u2192 Delete : set the \"Delete child tools\" checkbox if the group is not empty and click the Delete button to confirm: Customize Tool group permissions Users with ROLE_ADMIN or OWNER rights can modify permissions for this Tool group. It is convenient when you want to manage access for the whole Tool group and not for the individual Tools. For more details see here .","title":"10.2. Add/edit a Tool group"},{"location":"manual/10_Manage_Tools/10.2._Add_Edit_a_Tool_group/#102-addedit-a-tool-group","text":"Add a generic Tool group Add a personal Tool group Edit/delete a Tool group Customize Tool group permissions A Tool group is an object of the Cloud Pipeline that allows to organize Tools into groups. This page describes the process of adding and editing Tool groups. Also here you will find information about permission management for Tool groups. To create a Tool group, user need to have WRITE permission for a Docker registry and the ROLE_TOOL_GROUP_MANAGER role. To edit Tool group parameters you need to have WRITE permissions for it. For more information see 13. Permissions .","title":"10.2. Add/Edit a Tool group"},{"location":"manual/10_Manage_Tools/10.2._Add_Edit_a_Tool_group/#add-a-generic-tool-group","text":"A Tool group can be added to a Docker registry in the following way: In the Tools tab click the Gear icon \u2192 Group \u2192 + Create . Give your Tool group a name and an optional description : Click the CREATE button to confirm. You'll be automatically redirected to the new Tool group in the current Docker registry.","title":"Add a generic Tool group"},{"location":"manual/10_Manage_Tools/10.2._Add_Edit_a_Tool_group/#add-a-personal-tool-group","text":"In the Tools tab click the Gear icon \u2192 Group \u2192 + Create personal . Another way to do that is to select personal Tool group from the Tool group list And then press the Create personal tool group button. After that you'll be able to upload Tools to your personal Tool group.","title":"Add a personal Tool group"},{"location":"manual/10_Manage_Tools/10.2._Add_Edit_a_Tool_group/#editdelete-a-tool-group","text":"Choose a Tool group and click the Gear icon \u2192 Group \u2192 Edit . You'll be able to modify Tool group description in the Info tab. If you wish to delete a Tool group: click the Gear icon \u2192 Group \u2192 Delete : set the \"Delete child tools\" checkbox if the group is not empty and click the Delete button to confirm:","title":"Edit/delete a Tool group"},{"location":"manual/10_Manage_Tools/10.2._Add_Edit_a_Tool_group/#customize-tool-group-permissions","text":"Users with ROLE_ADMIN or OWNER rights can modify permissions for this Tool group. It is convenient when you want to manage access for the whole Tool group and not for the individual Tools. For more details see here .","title":"Customize Tool group permissions"},{"location":"manual/10_Manage_Tools/10.3._Add_a_Tool/","text":"10.3. Add a Tool This page describes the process of adding new Docker image to the Docker registry. Docker CLI has to be installed. A registry has to be configured to use Cloud Pipeline (CP) authentication system (see 10.1. Add/Edit a Docker registry ). In case of registry doesn't use CP authentication system - contact your system administrators for registry access details. Go to the Tools tab and choose a registry. Click the Gear icon \u2192 How to configure? Copy and paste a command from the Login into cloud registry section ( a ) into the Terminal, then run it. This command installs registry certificate and uses docker login command to access the registry. Note : this command requires \"root\" rights. Copy and run instructions from the Push a local docker image to the cloud registry section ( b ) consequentially to add the Tool to the registry: Create environment variable ( MY_LOCAL_DOCKER_IMAGE ) that holds the name of the Docker image; Tag the image you want to push with the domain name or IP address and the port of the Docker registry; Push the image to the registry. Example : here we push \" hello-world \" image to the registry \" 18.195.69.178:5000 \", \" library \" Tool group. Troubleshooting section ( c ) contains information about fixing common problems that may appear during the execution of docker login or docker pull/push commands. Make sure that image was pushed to the registry and enabled in the Tools tab. Note : Tool will be automatically enabled only if CP authentication is configured for the registry. Note : If registry doesn't use CP authentication system, after the image was pushed do the following: Navigate to the Tools tab. Choose a Registry and a Tool group . Click the Gear icon \u2192 + Enable Tool . Write the name of the pushed image. Registry and Tool group will be already written for you.","title":"10.3. Add a Tool"},{"location":"manual/10_Manage_Tools/10.3._Add_a_Tool/#103-add-a-tool","text":"This page describes the process of adding new Docker image to the Docker registry. Docker CLI has to be installed. A registry has to be configured to use Cloud Pipeline (CP) authentication system (see 10.1. Add/Edit a Docker registry ). In case of registry doesn't use CP authentication system - contact your system administrators for registry access details. Go to the Tools tab and choose a registry. Click the Gear icon \u2192 How to configure? Copy and paste a command from the Login into cloud registry section ( a ) into the Terminal, then run it. This command installs registry certificate and uses docker login command to access the registry. Note : this command requires \"root\" rights. Copy and run instructions from the Push a local docker image to the cloud registry section ( b ) consequentially to add the Tool to the registry: Create environment variable ( MY_LOCAL_DOCKER_IMAGE ) that holds the name of the Docker image; Tag the image you want to push with the domain name or IP address and the port of the Docker registry; Push the image to the registry. Example : here we push \" hello-world \" image to the registry \" 18.195.69.178:5000 \", \" library \" Tool group. Troubleshooting section ( c ) contains information about fixing common problems that may appear during the execution of docker login or docker pull/push commands. Make sure that image was pushed to the registry and enabled in the Tools tab. Note : Tool will be automatically enabled only if CP authentication is configured for the registry. Note : If registry doesn't use CP authentication system, after the image was pushed do the following: Navigate to the Tools tab. Choose a Registry and a Tool group . Click the Gear icon \u2192 + Enable Tool . Write the name of the pushed image. Registry and Tool group will be already written for you.","title":"10.3. Add a Tool"},{"location":"manual/10_Manage_Tools/10.4._Edit_a_Tool/","text":"10.4. Edit a Tool Edit a Tool description Edit a Tool version Run/Delete a Tool version Commit a Tool Edit a Tool settings Delete a Tool To view and edit tool's attributes see 17. CP objects tagging by additional attributes . Edit a Tool description Select a Tool and click its name. Navigate to the Description tab of a Tool. Click the Edit buttons on the right side of the screen to modify the Short description and Full description of the Tool. Edit a Tool version In this tab, you can run a Tool version with custom settings or delete a Tool version. To see an example of a launching a Tool with custom settings see here . Run/Delete a Tool version Navigate to the Versions tab of a Tool. Click on the arrow near the Run button. Select Custom settings option to configure run parameters. If you want to delete a Tool version, click the Delete button. Commit a Tool Commit function allows modifying existing Tools. Launch a Tool in the sleep infinity mode. See an example here . SSH into it via the SSH button at the \" Run logs \" page. Change something. Example : here we install a biopython package into this Docker image. Wait for the installation to complete! Go back to the Logs tab and click the COMMIT button. Choose a Docker registry and a Tool group. Change a name for the modified image or add a new version to the current Tool by typing the version name in a separate box. Note : to add a new version to the existing Tool don't change the original name of the Tool! Note : image name and a version name should be written according to the following rules: May contain lowercase letters, digits, and separators. A separator is defined as a period, one or two underscores, or one or more dashes. A name component may not start or end with a separator. Tick boxes if needed: Delete runtime files box - to delete all files from /runs/[pipeline name] folder before committing. Stop pipeline box - to stop the current run after committing. In this example, we will change \"base-generic-centos7\" to \"base-generic-centos7-biopython\". Committing may take some time: When it is complete COMMITING status on the right side of the screen will change to COMMIT SUCCEEDED . In round brackets the date/time of the latest commit is shown: Find a modified Tool in the registry. Committing features In certain use-cases, extra steps shall be executed before/after running the commit command in the container. For example, to avoid warning messages about terminating the previous session (which was committed) of the tool application in a non-graceful manner. Some applications may require extra cleanup to be performed before the termination. To workaround such issues in the Cloud Pipeline an approach of \" pre/post-commit hooks \" is implemented. That allows to perform some graceful cleanup/restore before/after performing the commit itself. Note : Those hooks are valid only for the specific images and therefore shall be contained within those images. Cloud Pipeline itself only performs the calls to the hooks if they exist. There are two preferences from system-level settings that determine a behavior of the \" pre/post-commit hooks \" approach: commit.pre.command.path : specified a path to a script within a docker image, that will be executed in a currently running container, before docker commit occurs (default value: /root/pre_commit.sh ). This option is useful if any operations shall be performed with the running processes (e.g. send a signal), because in the subsequent post operation - only filesystem operations will be available. Note : any changes done at this stage will affect the running container. commit.post.command.path : specified a path to a script within a docker image, that will be executed in a committed image, after docker commit occurs (default value: /root/post_commit.sh ). This hook can be used to perform any filesystem cleanup or other operations, that shall not affect the currently running processes. Note : User shall have ROLE_ADMIN to read and update system-level settings. So, when you try to commit some tool, the Cloud Pipeline will check preferences described above and execute scripts with the specified names, if that scripts files exist in a docker image. If a corresponding pre/post script is not found in the docker image - it will not be executed. Consider an example with RStudio tool, that Cloud Pipeline provides \"out of the box\". RStudio Docker image contains a post-commit script that cleans the session after the commit: Open the Tools page, launch RStudio . SSH to the launched run via the SSH control in the upper-right corner of the run logs page. Check that post_commit.sh script exists in the Docker image - use the command ls /root/ : View the contents of that script. It looks like that: Note : /etc/cp_env.sh is a special Cloud Pipeline script, that sets all environment variables of the current docker container. So, as you can see from the code, as the result of that script launch will be a cleanup (removing) of the RStudio active session from the user home directory. Close SSH tab and try to do a commit - click the COMMIT control in the upper-right corner of the run logs page. Specify a new name for the committed tool, e.g. rstudio-test : While image is being committed, you can see the follow-up log for the CommitPipelineRun task: Here: ( 1 ) pre-commit script is not found in the docker image - nothing will be executed before commit ( 2 ) post-commit script is found at the specified path in the docker image - and it is being executed ( 3 ) post-commit script was performed successfully Edit a Tool settings Settings in this tab are applied to all Tool versions (i.e. these settings will be a default for all Tool version). Navigate to the Settings tab. Specify Endpoints for a Tool by click \" Add endpoint \" button: In the example below: the port is 8788 and the endpoint name is rstudio-s3fs : Let's look at the endpoint closer: \"nginx\" - type of the endpoints (only nginx is currently supported) \"port\": XXXX - an application will be deployed on this port on the pipeline node. You can specify additional nginx configuration for that endpoint in the text field bottom in JSON format. Note : optional path parameter may be required in case your application starts on host:port: / path . Note : optional additional parameter may be required in case you need to specify nginx location settings. See more information here . \"name\" - this value will be visible as a hyperlink in the UI. It is especially convenient when user sets more than one endpoint configuration for an interactive tool (learn more about interactive services - 15. Interactive services ). In the example below, we name one endpoint as \"rstudio-s3fs\" and another one as \"shiny\" . This is how everything will look in the Run log window: Click the + New Label button and add a label to the Tool (e.g. \"Awesome Tool\"). Specify \" Execution defaults \": Instance type (e.g. \"m4.large\") Price type (e.g. \"Spot\") Disk size in Gb (e.g. \"20\") select available storages configure cluster, if it's necessary for your Tool write the Default command for the Tool execution (e.g. echo \"Hello world!\" ) If it's necessary for your Tool - add system or custom parameters. For more details see 6.1. Create and configure pipeline . Click Save button to save these settings. Delete a Tool To detach a Tool from the Cloud Pipeline: Open the Tool you wish to detach Hover over the gear icon in the right upper corner and click the \"Delete tool\" item in the appeared list: Click the Delete button to confirm: Note : if you try to delete a tool/version, but there is/are active job(s) using it - the corresponding warning notification will be shown, e.g.:","title":"10.4. Edit a Tool"},{"location":"manual/10_Manage_Tools/10.4._Edit_a_Tool/#104-edit-a-tool","text":"Edit a Tool description Edit a Tool version Run/Delete a Tool version Commit a Tool Edit a Tool settings Delete a Tool To view and edit tool's attributes see 17. CP objects tagging by additional attributes .","title":"10.4. Edit a Tool"},{"location":"manual/10_Manage_Tools/10.4._Edit_a_Tool/#edit-a-tool-description","text":"Select a Tool and click its name. Navigate to the Description tab of a Tool. Click the Edit buttons on the right side of the screen to modify the Short description and Full description of the Tool.","title":"Edit a Tool description"},{"location":"manual/10_Manage_Tools/10.4._Edit_a_Tool/#edit-a-tool-version","text":"In this tab, you can run a Tool version with custom settings or delete a Tool version. To see an example of a launching a Tool with custom settings see here .","title":"Edit a Tool version"},{"location":"manual/10_Manage_Tools/10.4._Edit_a_Tool/#rundelete-a-tool-version","text":"Navigate to the Versions tab of a Tool. Click on the arrow near the Run button. Select Custom settings option to configure run parameters. If you want to delete a Tool version, click the Delete button.","title":"Run/Delete a Tool version"},{"location":"manual/10_Manage_Tools/10.4._Edit_a_Tool/#commit-a-tool","text":"Commit function allows modifying existing Tools. Launch a Tool in the sleep infinity mode. See an example here . SSH into it via the SSH button at the \" Run logs \" page. Change something. Example : here we install a biopython package into this Docker image. Wait for the installation to complete! Go back to the Logs tab and click the COMMIT button. Choose a Docker registry and a Tool group. Change a name for the modified image or add a new version to the current Tool by typing the version name in a separate box. Note : to add a new version to the existing Tool don't change the original name of the Tool! Note : image name and a version name should be written according to the following rules: May contain lowercase letters, digits, and separators. A separator is defined as a period, one or two underscores, or one or more dashes. A name component may not start or end with a separator. Tick boxes if needed: Delete runtime files box - to delete all files from /runs/[pipeline name] folder before committing. Stop pipeline box - to stop the current run after committing. In this example, we will change \"base-generic-centos7\" to \"base-generic-centos7-biopython\". Committing may take some time: When it is complete COMMITING status on the right side of the screen will change to COMMIT SUCCEEDED . In round brackets the date/time of the latest commit is shown: Find a modified Tool in the registry.","title":"Commit a Tool"},{"location":"manual/10_Manage_Tools/10.4._Edit_a_Tool/#committing-features","text":"In certain use-cases, extra steps shall be executed before/after running the commit command in the container. For example, to avoid warning messages about terminating the previous session (which was committed) of the tool application in a non-graceful manner. Some applications may require extra cleanup to be performed before the termination. To workaround such issues in the Cloud Pipeline an approach of \" pre/post-commit hooks \" is implemented. That allows to perform some graceful cleanup/restore before/after performing the commit itself. Note : Those hooks are valid only for the specific images and therefore shall be contained within those images. Cloud Pipeline itself only performs the calls to the hooks if they exist. There are two preferences from system-level settings that determine a behavior of the \" pre/post-commit hooks \" approach: commit.pre.command.path : specified a path to a script within a docker image, that will be executed in a currently running container, before docker commit occurs (default value: /root/pre_commit.sh ). This option is useful if any operations shall be performed with the running processes (e.g. send a signal), because in the subsequent post operation - only filesystem operations will be available. Note : any changes done at this stage will affect the running container. commit.post.command.path : specified a path to a script within a docker image, that will be executed in a committed image, after docker commit occurs (default value: /root/post_commit.sh ). This hook can be used to perform any filesystem cleanup or other operations, that shall not affect the currently running processes. Note : User shall have ROLE_ADMIN to read and update system-level settings. So, when you try to commit some tool, the Cloud Pipeline will check preferences described above and execute scripts with the specified names, if that scripts files exist in a docker image. If a corresponding pre/post script is not found in the docker image - it will not be executed. Consider an example with RStudio tool, that Cloud Pipeline provides \"out of the box\". RStudio Docker image contains a post-commit script that cleans the session after the commit: Open the Tools page, launch RStudio . SSH to the launched run via the SSH control in the upper-right corner of the run logs page. Check that post_commit.sh script exists in the Docker image - use the command ls /root/ : View the contents of that script. It looks like that: Note : /etc/cp_env.sh is a special Cloud Pipeline script, that sets all environment variables of the current docker container. So, as you can see from the code, as the result of that script launch will be a cleanup (removing) of the RStudio active session from the user home directory. Close SSH tab and try to do a commit - click the COMMIT control in the upper-right corner of the run logs page. Specify a new name for the committed tool, e.g. rstudio-test : While image is being committed, you can see the follow-up log for the CommitPipelineRun task: Here: ( 1 ) pre-commit script is not found in the docker image - nothing will be executed before commit ( 2 ) post-commit script is found at the specified path in the docker image - and it is being executed ( 3 ) post-commit script was performed successfully","title":"Committing features"},{"location":"manual/10_Manage_Tools/10.4._Edit_a_Tool/#edit-a-tool-settings","text":"Settings in this tab are applied to all Tool versions (i.e. these settings will be a default for all Tool version). Navigate to the Settings tab. Specify Endpoints for a Tool by click \" Add endpoint \" button: In the example below: the port is 8788 and the endpoint name is rstudio-s3fs : Let's look at the endpoint closer: \"nginx\" - type of the endpoints (only nginx is currently supported) \"port\": XXXX - an application will be deployed on this port on the pipeline node. You can specify additional nginx configuration for that endpoint in the text field bottom in JSON format. Note : optional path parameter may be required in case your application starts on host:port: / path . Note : optional additional parameter may be required in case you need to specify nginx location settings. See more information here . \"name\" - this value will be visible as a hyperlink in the UI. It is especially convenient when user sets more than one endpoint configuration for an interactive tool (learn more about interactive services - 15. Interactive services ). In the example below, we name one endpoint as \"rstudio-s3fs\" and another one as \"shiny\" . This is how everything will look in the Run log window: Click the + New Label button and add a label to the Tool (e.g. \"Awesome Tool\"). Specify \" Execution defaults \": Instance type (e.g. \"m4.large\") Price type (e.g. \"Spot\") Disk size in Gb (e.g. \"20\") select available storages configure cluster, if it's necessary for your Tool write the Default command for the Tool execution (e.g. echo \"Hello world!\" ) If it's necessary for your Tool - add system or custom parameters. For more details see 6.1. Create and configure pipeline . Click Save button to save these settings.","title":"Edit a Tool settings"},{"location":"manual/10_Manage_Tools/10.4._Edit_a_Tool/#delete-a-tool","text":"To detach a Tool from the Cloud Pipeline: Open the Tool you wish to detach Hover over the gear icon in the right upper corner and click the \"Delete tool\" item in the appeared list: Click the Delete button to confirm: Note : if you try to delete a tool/version, but there is/are active job(s) using it - the corresponding warning notification will be shown, e.g.:","title":"Delete a Tool"},{"location":"manual/10_Manage_Tools/10.5._Launch_a_Tool/","text":"10.5. Launch a Tool Launch the latest version Launch particular Tool version Launch a Tool with \"friendly\" URL Instance management To launch a Tool you need to have EXECUTE permissions for it. For more information see 13. Permissions . You also can launch a tool via CLI. See here . Launch the latest version To run an instance with a selected Tool navigate to the Tools tab and click the Tool name . Click the Run button in the top-right corner of the screen and the latest version with default settings will be launched (these are defined for Cloud Pipeline globally). If you want to change settings, you shall click the arrow near the Run button \u2192 Custom settings . Launch tool page will be opened. If the Price type is set as \" On-demand \" - at the Launch page , an additional checkbox Auto pause appears: This checkbox allows to enable automatic pausing on-demand instance if it is not used. Such behavior could be controlled by Administrators using a set of parameters at System Preferences (see here ). Please note, this checkbox will not be displayed if any cluster is configured (\"Static\" or \"Autoscaled\"). If the Price type is set as \" On-demand \" - at the Launch page , an additional control Maintenance appears. It allows to configure schedule for automatical pause/resume a tool run: It could be useful when the tool is launched for a long time (several days/weeks) but it shall not stand idle, just increasing costs, in weekends and holidays, for example. For more details, how to configure the automatically schedule for a run see 6.2. Launch a pipeline (item 5). Please note, the Maintenance control will not be displayed if any cluster is configured (\"Static\" or \"Autoscaled\"). Users (who have permissions to pause/resume a run) can create/view/modify/delete schedule rules anytime launched run is active via the Run logs page - for more details see 11. Manage runs . Define the parameters in the Exec environment , Advanced and Parameters sections. Click the Launch button in the top-right corner of the screen. Please note, that the current user can launch a tool only if he/his group has corresponding permissions on that tool (for more information see 13. Permissions ), but the Launch button may be disabled also for one of the following reasons: execution isn't allowed for specified docker image; read operations aren't allowed for specified input or common path parameters; write operations aren't allowed for specified output path parameters. In such cases, hover over the Launch button to view warning notification with a reason of a run forbiddance, e.g.: Note : you can also launch a tool with the same settings via the CLI command or API request. To generate the corresponding command/request click the button near the \"Launch\" button. For more details see here . Launch particular Tool version To run a particular version click the Versions section. Select a version and click the Run button. The selected version with default settings will be launched (these are defined for Cloud Pipeline globally). If you want to change settings, you shall click the arrow near the Run button \u2192 Custom settings . Launch a tool page will be opened. Define the parameters. Click the \" Launch \" button. Example 1 In this example, we will run the \" Ubuntu \" Tool with custom settings: 30 Gb hard drive, 4 CPU cores, and 16 Gb RAM. Note : \"Start idle\" box is ticked to allow SSH access to the running Tool. To learn more about interactive services see 15. Interactive services . Click the Launch button in the top-right corner of the screen when all parameters are set. After the Tool is launched you will be redirected to the Runs tab: Click the Log button to see run details after instance finishes initialization. Wait until the SSH button will appear at the Run logs page, click it You will be redirected to the page with interactive shell session inside the Docker container. For example, we can list \"/\" directory content inside the container. Launch a Tool with \"friendly\" URL User can specify a \"friendly\"-format endpoint URL for persistent services. This produces the endpoint URL for the selected interactive service in a more friendly/descriptive format instead of the default general host /pipeline- RunID - port . \"Friendly\" format can be configured before the specific service launch in the \" Advanced \" section of the Launch page. This format has the following structure - custom-host / friendly_url , where: custom-host - valid existing domain name. If it's not specified, the default host name will be used friendly_url - valid endpoint name. If it's not specified but the custom-host is specified - the final endpoint URL will be equal the custom-host only Note : specified endpoint URL shall be unique among all active runs. Example 2 In this example we will configure a pretty URL for RStudio Tool. Note : for do that, user account shall be registered within CP users catalog and granted READ EXECUTE permissions for the rstudio Tool. Navigate to the Tools tab. Select/find RStudio Tool in the list: At the opened page hover the \" Run \" button and click the appeared \" Custom settings \" point: Click the \" Advanced \" control ( a ), input desired \" Friendly URL \" ( b ) (name shall be unique) and then click the \" Launch \" button ( c ), e.g.: Open the Run logs page of RStudio Tool, wait until the tool successfully starts. Click the hyperlink opposite \" Endpoint \" label: In a new tab RStudio will be opened. Check that the URL is in the \"pretty\" format you specified at step 4: Instance management Instance management allows to set restrictions on instance types and price types for tool runs. User shall have ROLE_ADMIN or to be an OWNER of the Tool to launch Instance management panel. For more information see 13. Permissions . To open Instance management panel: Click button in the left upper corner of the main tool page. Click \"Instance management\": Such panel will be shown: On this panel you can specify some restrictions on allowed instance types and price types for launching tool. Here you can specify: Field Description Example Allowed tool instance types mask This mask restrict for a tool allowed instance types. If you want for that tool only some of \"large m5...\" instances types will be able, mask would be m5*.large* In that case, before launching tool, dropdown list of available node types will be look like this: Allowed price types In this field you may restrict, what price types will be allowed for a user. If you want \"On-demand\" runs only for that tool will be able, select it in the dropdown list: In that case, before launching tool, dropdown list of price types will be look like this: To apply set restrictions for a tool click button. Setting restrictions on allowed instance types/price types is a convenient way to minimize a number of invalid configurations runs. Such restrictions could be set not only for a tool, but on another levels too. In CP platform next hierarchy is set for applying of inputted allowed instance types (sorted by priority): User level (specified for a user on \"User management\" tab) (see v.0.14 - 12.4. Edit/delete a user ) User group level (specified for a group (role) on \"User management\" tab. If a user is a member of several groups - list of allowed instances will be summarized across all the groups) (see v.0.14 - 12.6. Edit a group/role ) Tool level (specified for a tool on \"Instance management\" panel) (see above ) (global) cluster.allowed.instance.types.docker (specified on \"Cluster\" tab in \"Preferences\" section of system-level settings) (see v.0.14 - 12.10. Manage system-level settings ) (global) cluster.allowed.instance.types (specified on \"Cluster\" tab in \"Preferences\" section of system-level settings) (see v.0.14 - 12.10. Manage system-level settings )","title":"10.5. Launch a Tool"},{"location":"manual/10_Manage_Tools/10.5._Launch_a_Tool/#105-launch-a-tool","text":"Launch the latest version Launch particular Tool version Launch a Tool with \"friendly\" URL Instance management To launch a Tool you need to have EXECUTE permissions for it. For more information see 13. Permissions . You also can launch a tool via CLI. See here .","title":"10.5. Launch a Tool"},{"location":"manual/10_Manage_Tools/10.5._Launch_a_Tool/#launch-the-latest-version","text":"To run an instance with a selected Tool navigate to the Tools tab and click the Tool name . Click the Run button in the top-right corner of the screen and the latest version with default settings will be launched (these are defined for Cloud Pipeline globally). If you want to change settings, you shall click the arrow near the Run button \u2192 Custom settings . Launch tool page will be opened. If the Price type is set as \" On-demand \" - at the Launch page , an additional checkbox Auto pause appears: This checkbox allows to enable automatic pausing on-demand instance if it is not used. Such behavior could be controlled by Administrators using a set of parameters at System Preferences (see here ). Please note, this checkbox will not be displayed if any cluster is configured (\"Static\" or \"Autoscaled\"). If the Price type is set as \" On-demand \" - at the Launch page , an additional control Maintenance appears. It allows to configure schedule for automatical pause/resume a tool run: It could be useful when the tool is launched for a long time (several days/weeks) but it shall not stand idle, just increasing costs, in weekends and holidays, for example. For more details, how to configure the automatically schedule for a run see 6.2. Launch a pipeline (item 5). Please note, the Maintenance control will not be displayed if any cluster is configured (\"Static\" or \"Autoscaled\"). Users (who have permissions to pause/resume a run) can create/view/modify/delete schedule rules anytime launched run is active via the Run logs page - for more details see 11. Manage runs . Define the parameters in the Exec environment , Advanced and Parameters sections. Click the Launch button in the top-right corner of the screen. Please note, that the current user can launch a tool only if he/his group has corresponding permissions on that tool (for more information see 13. Permissions ), but the Launch button may be disabled also for one of the following reasons: execution isn't allowed for specified docker image; read operations aren't allowed for specified input or common path parameters; write operations aren't allowed for specified output path parameters. In such cases, hover over the Launch button to view warning notification with a reason of a run forbiddance, e.g.: Note : you can also launch a tool with the same settings via the CLI command or API request. To generate the corresponding command/request click the button near the \"Launch\" button. For more details see here .","title":"Launch the latest version"},{"location":"manual/10_Manage_Tools/10.5._Launch_a_Tool/#launch-particular-tool-version","text":"To run a particular version click the Versions section. Select a version and click the Run button. The selected version with default settings will be launched (these are defined for Cloud Pipeline globally). If you want to change settings, you shall click the arrow near the Run button \u2192 Custom settings . Launch a tool page will be opened. Define the parameters. Click the \" Launch \" button.","title":"Launch\u00a0particular Tool version"},{"location":"manual/10_Manage_Tools/10.5._Launch_a_Tool/#example-1","text":"In this example, we will run the \" Ubuntu \" Tool with custom settings: 30 Gb hard drive, 4 CPU cores, and 16 Gb RAM. Note : \"Start idle\" box is ticked to allow SSH access to the running Tool. To learn more about interactive services see 15. Interactive services . Click the Launch button in the top-right corner of the screen when all parameters are set. After the Tool is launched you will be redirected to the Runs tab: Click the Log button to see run details after instance finishes initialization. Wait until the SSH button will appear at the Run logs page, click it You will be redirected to the page with interactive shell session inside the Docker container. For example, we can list \"/\" directory content inside the container.","title":"Example\u00a01"},{"location":"manual/10_Manage_Tools/10.5._Launch_a_Tool/#launch-a-tool-with-friendly-url","text":"User can specify a \"friendly\"-format endpoint URL for persistent services. This produces the endpoint URL for the selected interactive service in a more friendly/descriptive format instead of the default general host /pipeline- RunID - port . \"Friendly\" format can be configured before the specific service launch in the \" Advanced \" section of the Launch page. This format has the following structure - custom-host / friendly_url , where: custom-host - valid existing domain name. If it's not specified, the default host name will be used friendly_url - valid endpoint name. If it's not specified but the custom-host is specified - the final endpoint URL will be equal the custom-host only Note : specified endpoint URL shall be unique among all active runs.","title":"Launch a Tool with \"friendly\" URL"},{"location":"manual/10_Manage_Tools/10.5._Launch_a_Tool/#example-2","text":"In this example we will configure a pretty URL for RStudio Tool. Note : for do that, user account shall be registered within CP users catalog and granted READ EXECUTE permissions for the rstudio Tool. Navigate to the Tools tab. Select/find RStudio Tool in the list: At the opened page hover the \" Run \" button and click the appeared \" Custom settings \" point: Click the \" Advanced \" control ( a ), input desired \" Friendly URL \" ( b ) (name shall be unique) and then click the \" Launch \" button ( c ), e.g.: Open the Run logs page of RStudio Tool, wait until the tool successfully starts. Click the hyperlink opposite \" Endpoint \" label: In a new tab RStudio will be opened. Check that the URL is in the \"pretty\" format you specified at step 4:","title":"Example 2"},{"location":"manual/10_Manage_Tools/10.5._Launch_a_Tool/#instance-management","text":"Instance management allows to set restrictions on instance types and price types for tool runs. User shall have ROLE_ADMIN or to be an OWNER of the Tool to launch Instance management panel. For more information see 13. Permissions . To open Instance management panel: Click button in the left upper corner of the main tool page. Click \"Instance management\": Such panel will be shown: On this panel you can specify some restrictions on allowed instance types and price types for launching tool. Here you can specify: Field Description Example Allowed tool instance types mask This mask restrict for a tool allowed instance types. If you want for that tool only some of \"large m5...\" instances types will be able, mask would be m5*.large* In that case, before launching tool, dropdown list of available node types will be look like this: Allowed price types In this field you may restrict, what price types will be allowed for a user. If you want \"On-demand\" runs only for that tool will be able, select it in the dropdown list: In that case, before launching tool, dropdown list of price types will be look like this: To apply set restrictions for a tool click button. Setting restrictions on allowed instance types/price types is a convenient way to minimize a number of invalid configurations runs. Such restrictions could be set not only for a tool, but on another levels too. In CP platform next hierarchy is set for applying of inputted allowed instance types (sorted by priority): User level (specified for a user on \"User management\" tab) (see v.0.14 - 12.4. Edit/delete a user ) User group level (specified for a group (role) on \"User management\" tab. If a user is a member of several groups - list of allowed instances will be summarized across all the groups) (see v.0.14 - 12.6. Edit a group/role ) Tool level (specified for a tool on \"Instance management\" panel) (see above ) (global) cluster.allowed.instance.types.docker (specified on \"Cluster\" tab in \"Preferences\" section of system-level settings) (see v.0.14 - 12.10. Manage system-level settings ) (global) cluster.allowed.instance.types (specified on \"Cluster\" tab in \"Preferences\" section of system-level settings) (see v.0.14 - 12.10. Manage system-level settings )","title":"Instance management"},{"location":"manual/10_Manage_Tools/10.6._Tool_security_check/","text":"10.6. Tool security check Force a security scanning procedure Show/hide unscanned Tool versions \"White list\" for Tool versions A registry may potentially contain tools with vulnerable software which may cause damage. To prevent this issue, Cloud Pipeline performs Security scanning feature of the tools that are provided by users and restrict usage of not secure Tools. User shall have ROLE_ADMIN to force a security scanning procedure or to run unscanned Tools/Tools with critical number of vulnerabilities. For more information see 13. Permissions . Force a security scanning procedure In the Cloud Pipeline security scanning is performed on a scheduled basis - every N minutes (configurable parameter). However, a user with an administrator role can manually run a security scanning procedure: Select a Tool and navigate to the Versions tab. Click the SCAN button of a Tool version. Note : If scan results for version are against the security politics (i.e. vulnerabilities number exceeds the threshold) the Run button for version will be deactivated. Only users with ROLE_ADMIN role will be able to run this Tool. Generic users can run this version of Tool only if it is checked with a \"white list\" flag (see below ) or if its \"grace\" period is not elapsed yet (see security.tools.grace.hours setting here v.0.14 - 12.10. Manage system-level settings ). Note : If scan results for the latest version are against the security politics (i.e. vulnerabilities number exceeds the threshold) the Run button for Tool will be deactivated. Only users with ROLE_ADMIN role will be able to run this Tool. Generic users can run this version of Tool only if it is checked with a \"white list\" flag (see below ) or if its \"grace\" period is not elapsed yet (see security.tools.grace.hours setting here v.0.14 - 12.10. Manage system-level settings ). Show/hide unscanned Tool versions Select a Tool and navigate to the Versions tab. Press the View unscanned versions control. Unscanned version(s) will appear in the list of Tool versions. Note : for users with ADMIN role Run button of unscanned version contains \"warning\" sign. Hover over that button and you'll see a suggestion that the version shall be scanned for vulnerabilities. Generic users won't be able to run unscanned Tools (except when they're checked with a \"white list\" flag - see below or if \"grace\" period for this version of Tool is not elapsed yet (see security.tools.grace.hours setting here v.0.14 - 12.10. Manage system-level settings )). If you are user with ADMIN role and press the Run button anyway, you'll see another warning: Hide unscanned Tool versions by clicking Hide unscanned versions control. \"White list\" for Tool versions Generic users can't run tools with vulnerable software or unscanned tools because it may cause damage. But admin may allow users to run certain docker versions even if they are vulnerable/unscanned. For do that admin has to check a such docker version with a special \"white list\" flag. User shall have ROLE_ADMIN to set a \"white list\" flag for an unscanned Tools or Tools with critical number of vulnerabilities. To \"add\" a Tool version into the \"white list\": Open Versions tab on main tool's page. Click Add to white list button in the row of the version you want allow users to run in spite of possible damage: The version with a \"white list\" flag will be shown in green: Now, if user will try to launch this version of docker image, it will be run without any errors during launch time and viewing. If user will try to launch any other \"danger\" version of this tool without a \"white list\" flag, error message will be shown, tool won't be run: To \"delete\" a Tool version from the \"white list\": Open Versions tab on the main tool's page. Click Remove from white list button in the row of the version with a set \"white list\" flag:","title":"10.6. Tool security check"},{"location":"manual/10_Manage_Tools/10.6._Tool_security_check/#106-tool-security-check","text":"Force a security scanning procedure Show/hide unscanned Tool versions \"White list\" for Tool versions A registry may potentially contain tools with vulnerable software which may cause damage. To prevent this issue, Cloud Pipeline performs Security scanning feature of the tools that are provided by users and restrict usage of not secure Tools. User shall have ROLE_ADMIN to force a security scanning procedure or to run unscanned Tools/Tools with critical number of vulnerabilities. For more information see 13. Permissions .","title":"10.6. Tool security check"},{"location":"manual/10_Manage_Tools/10.6._Tool_security_check/#force-a-security-scanning-procedure","text":"In the Cloud Pipeline security scanning is performed on a scheduled basis - every N minutes (configurable parameter). However, a user with an administrator role can manually run a security scanning procedure: Select a Tool and navigate to the Versions tab. Click the SCAN button of a Tool version. Note : If scan results for version are against the security politics (i.e. vulnerabilities number exceeds the threshold) the Run button for version will be deactivated. Only users with ROLE_ADMIN role will be able to run this Tool. Generic users can run this version of Tool only if it is checked with a \"white list\" flag (see below ) or if its \"grace\" period is not elapsed yet (see security.tools.grace.hours setting here v.0.14 - 12.10. Manage system-level settings ). Note : If scan results for the latest version are against the security politics (i.e. vulnerabilities number exceeds the threshold) the Run button for Tool will be deactivated. Only users with ROLE_ADMIN role will be able to run this Tool. Generic users can run this version of Tool only if it is checked with a \"white list\" flag (see below ) or if its \"grace\" period is not elapsed yet (see security.tools.grace.hours setting here v.0.14 - 12.10. Manage system-level settings ).","title":"Force a security scanning procedure"},{"location":"manual/10_Manage_Tools/10.6._Tool_security_check/#showhide-unscanned-tool-versions","text":"Select a Tool and navigate to the Versions tab. Press the View unscanned versions control. Unscanned version(s) will appear in the list of Tool versions. Note : for users with ADMIN role Run button of unscanned version contains \"warning\" sign. Hover over that button and you'll see a suggestion that the version shall be scanned for vulnerabilities. Generic users won't be able to run unscanned Tools (except when they're checked with a \"white list\" flag - see below or if \"grace\" period for this version of Tool is not elapsed yet (see security.tools.grace.hours setting here v.0.14 - 12.10. Manage system-level settings )). If you are user with ADMIN role and press the Run button anyway, you'll see another warning: Hide unscanned Tool versions by clicking Hide unscanned versions control.","title":"Show/hide unscanned Tool versions"},{"location":"manual/10_Manage_Tools/10.6._Tool_security_check/#white-list-for-tool-versions","text":"Generic users can't run tools with vulnerable software or unscanned tools because it may cause damage. But admin may allow users to run certain docker versions even if they are vulnerable/unscanned. For do that admin has to check a such docker version with a special \"white list\" flag. User shall have ROLE_ADMIN to set a \"white list\" flag for an unscanned Tools or Tools with critical number of vulnerabilities. To \"add\" a Tool version into the \"white list\": Open Versions tab on main tool's page. Click Add to white list button in the row of the version you want allow users to run in spite of possible damage: The version with a \"white list\" flag will be shown in green: Now, if user will try to launch this version of docker image, it will be run without any errors during launch time and viewing. If user will try to launch any other \"danger\" version of this tool without a \"white list\" flag, error message will be shown, tool won't be run: To \"delete\" a Tool version from the \"white list\": Open Versions tab on the main tool's page. Click Remove from white list button in the row of the version with a set \"white list\" flag:","title":"\"White list\" for Tool versions"},{"location":"manual/10_Manage_Tools/10.7._Tool_version_menu/","text":"10.7. Tool version menu Vulnerabilities report Version settings Version packages To see the detailed info of Docker image version: Click any Tool version : Tool version menu will be shown: Vulnerabilities report This tab contains the detailed info of the Docker image's scanning results. Here you can see vulnerable components ( a ). Each component has severity estimation ( b ): Expand each component details by clicking the \" Plus \" icon to see more information about it: a ) link to a page of the vulnerability description b ) the component version in which this vulnerability was fixed c ) severity level d ) short description of the vulnerability (it appears when hovering mouse pointer over the link a ) Sort components alphabetically ( 1 ), or by their severity ( 2 ): Version settings On this tab version-level settings are defined. If these settings are specified - they will be applied to each run of the docker image version. If version-level settings are not defined: docker-level settings will be applied for launch. If docker-level settings are not defined: global defaults will be applied. There are 3 groups of parameters that user can specify (they are analogical to the \"Execution environment\" of tool settings, for more details see here ): Execution defaults System parameters Custom parameters For change version-level settings, e.g.: Select an Instance type . Set the Price type . Input the Disk size. Click Save button: Click button to return into the tool menu. Click Run \u2192 Custom settings for the changed tool version. Check that version-level settings are applied: Note : via the Cloud Region field at the Version Settings tab, the admin/tool owner can select a specific Cloud Provider / Region to enforce users to run that tool version in it: By default, it has Not configured value. This means, that a tool version will be launched in a Default region (configured in the global settings) or a user can set any allowed Cloud Region / Provider manually. Version packages On this tab user can see the full list software packages installed into a specific Docker image. List of packages is generated from the docker version together with vulnerabilities scanning. This occurs nightly (all dockers are scanned) or if admin explicitly requests scanning by clicking SCAN button for a specific version. Currently the following types of software packages can be scanned: System package manager's database (i.e. yum , apt ) R packages Python packages Software packages are combined into groups named \" Ecosystems \". To view content of any ecosystem, select it in the dropdown list: Information about each package contains: Package name Package description ( if available ) For filter/search packages type some text into a search-query field. Search will be done automatically across all ecosystems :","title":"10.7. Tool version menu"},{"location":"manual/10_Manage_Tools/10.7._Tool_version_menu/#107-tool-version-menu","text":"Vulnerabilities report Version settings Version packages To see the detailed info of Docker image version: Click any Tool version : Tool version menu will be shown:","title":"10.7. Tool version menu"},{"location":"manual/10_Manage_Tools/10.7._Tool_version_menu/#vulnerabilities-report","text":"This tab contains the detailed info of the Docker image's scanning results. Here you can see vulnerable components ( a ). Each component has severity estimation ( b ): Expand each component details by clicking the \" Plus \" icon to see more information about it: a ) link to a page of the vulnerability description b ) the component version in which this vulnerability was fixed c ) severity level d ) short description of the vulnerability (it appears when hovering mouse pointer over the link a ) Sort components alphabetically ( 1 ), or by their severity ( 2 ):","title":"Vulnerabilities report"},{"location":"manual/10_Manage_Tools/10.7._Tool_version_menu/#version-settings","text":"On this tab version-level settings are defined. If these settings are specified - they will be applied to each run of the docker image version. If version-level settings are not defined: docker-level settings will be applied for launch. If docker-level settings are not defined: global defaults will be applied. There are 3 groups of parameters that user can specify (they are analogical to the \"Execution environment\" of tool settings, for more details see here ): Execution defaults System parameters Custom parameters For change version-level settings, e.g.: Select an Instance type . Set the Price type . Input the Disk size. Click Save button: Click button to return into the tool menu. Click Run \u2192 Custom settings for the changed tool version. Check that version-level settings are applied: Note : via the Cloud Region field at the Version Settings tab, the admin/tool owner can select a specific Cloud Provider / Region to enforce users to run that tool version in it: By default, it has Not configured value. This means, that a tool version will be launched in a Default region (configured in the global settings) or a user can set any allowed Cloud Region / Provider manually.","title":"Version settings"},{"location":"manual/10_Manage_Tools/10.7._Tool_version_menu/#version-packages","text":"On this tab user can see the full list software packages installed into a specific Docker image. List of packages is generated from the docker version together with vulnerabilities scanning. This occurs nightly (all dockers are scanned) or if admin explicitly requests scanning by clicking SCAN button for a specific version. Currently the following types of software packages can be scanned: System package manager's database (i.e. yum , apt ) R packages Python packages Software packages are combined into groups named \" Ecosystems \". To view content of any ecosystem, select it in the dropdown list: Information about each package contains: Package name Package description ( if available ) For filter/search packages type some text into a search-query field. Search will be done automatically across all ecosystems :","title":"Version packages"},{"location":"manual/10_Manage_Tools/10.8._Symlinks_between_tools/","text":"10.8. \"Symlinked\" tools The majority of the tools for general users are managed by the administrators and are available via the library tool group. But for some of the users it is convenient to have separate tool groups, which contain a mix of the custom tools (managed by the users themselves) and the library tools (managed by the admins). For the latter ones the ability to create \" symlinks \" into the other tool groups exists. \"Symlinked\" tools are displayed in that users' tool groups as the original tools but can't be edited/updated. When a run is started with \"symlinked\" tool as docker image it is being replaced with original image for Kubernetes pod spec. The following behavior is implemented: to create a \"symlink\" to the tool, the user shall have READ access to the source tool and WRITE access to the destination tool group for the \"symlinked\" tool all the same description, icon, settings as in the source image are displayed. It isn't possible to make any changes to the \"symlink\" data (description, icon, settings, attributes, issues, etc.), even for the admins image owners and admins are able to manage the permissions for the \"symlinked\" tools. Permissions on the \"symlinked\" tools are configured separately from the original tool two levels of \"symlinks\" is not possible (\"symlink\" to the \"symlinked\" tool can't be created) it isn't possible to \"push\" into the \"symlinked\" tool Creation and usage of a \"symlink\" to the tool A \"symlink\" to the tool can be created by any user with the ROLE_TOOL_GROUP_MANAGER role, that has read access to the original (source) tool and write access to the destination tool group. To create a \"symlink\" to the tool: Open the Tools page. Open the tool you wish to create the \"symlink\". Click the link icon in the upper-right corner: In the opened popup select the destination Registry and Tool group where you wish to create the \"symlink\", e.g.: Click the Create Link button to confirm. Just-created \"symlinked\" tool will be open automatically: The link icon before the tool name indicates a \"symlink\". In the tool group, \"symlinked\" tool is displayed with the same icon: Please note, that all \"edit\" functions are unavailable even for the \"symlink\" OWNER . All settings, descriptions, icon, attributes, issues are loaded from the original tool: Version management (update/delete versions/attributes/settings/scan/vulnerabilities) is unavailable. Necessary data is also loaded from the original tool, e.g.: Run of the \"symlinked\" tool/its version is being performed as for usual tool. Commit of the launched \"symlinked\" tool can be performed only to the non-\"symlinked\" tool (no matter - new or existing): Management of a \"symlinked\" tool \"Symlinked\" tool can't be edited. Possible operations for OWNERs/admins: set the permissions on the \"symlink\" delete the \"symlink\" Set permissions Permissions are being set by the way as for general tools: Click the gear icon in the right-upper corner of the \"symlinked\" tool page. Click the Permissions item: In the opened popup configure desired permissions: Note : configured permissions are valid only for the \"symlink\". Permissions for the original tool shall be configured separately at the original tool page Remove a \"symlinked\" tool To delete a \"symlinked\" tool: Click the gear icon in the right-upper corner of the \"symlinked\" tool page. Click the Delete tool link item: Confirm the deletion:","title":"10.8. \"Symlinked\" tools"},{"location":"manual/10_Manage_Tools/10.8._Symlinks_between_tools/#108-symlinked-tools","text":"The majority of the tools for general users are managed by the administrators and are available via the library tool group. But for some of the users it is convenient to have separate tool groups, which contain a mix of the custom tools (managed by the users themselves) and the library tools (managed by the admins). For the latter ones the ability to create \" symlinks \" into the other tool groups exists. \"Symlinked\" tools are displayed in that users' tool groups as the original tools but can't be edited/updated. When a run is started with \"symlinked\" tool as docker image it is being replaced with original image for Kubernetes pod spec. The following behavior is implemented: to create a \"symlink\" to the tool, the user shall have READ access to the source tool and WRITE access to the destination tool group for the \"symlinked\" tool all the same description, icon, settings as in the source image are displayed. It isn't possible to make any changes to the \"symlink\" data (description, icon, settings, attributes, issues, etc.), even for the admins image owners and admins are able to manage the permissions for the \"symlinked\" tools. Permissions on the \"symlinked\" tools are configured separately from the original tool two levels of \"symlinks\" is not possible (\"symlink\" to the \"symlinked\" tool can't be created) it isn't possible to \"push\" into the \"symlinked\" tool","title":"10.8. \"Symlinked\" tools"},{"location":"manual/10_Manage_Tools/10.8._Symlinks_between_tools/#creation-and-usage-of-a-symlink-to-the-tool","text":"A \"symlink\" to the tool can be created by any user with the ROLE_TOOL_GROUP_MANAGER role, that has read access to the original (source) tool and write access to the destination tool group. To create a \"symlink\" to the tool: Open the Tools page. Open the tool you wish to create the \"symlink\". Click the link icon in the upper-right corner: In the opened popup select the destination Registry and Tool group where you wish to create the \"symlink\", e.g.: Click the Create Link button to confirm. Just-created \"symlinked\" tool will be open automatically: The link icon before the tool name indicates a \"symlink\". In the tool group, \"symlinked\" tool is displayed with the same icon: Please note, that all \"edit\" functions are unavailable even for the \"symlink\" OWNER . All settings, descriptions, icon, attributes, issues are loaded from the original tool: Version management (update/delete versions/attributes/settings/scan/vulnerabilities) is unavailable. Necessary data is also loaded from the original tool, e.g.: Run of the \"symlinked\" tool/its version is being performed as for usual tool. Commit of the launched \"symlinked\" tool can be performed only to the non-\"symlinked\" tool (no matter - new or existing):","title":"Creation and usage of a \"symlink\" to the tool"},{"location":"manual/10_Manage_Tools/10.8._Symlinks_between_tools/#management-of-a-symlinked-tool","text":"\"Symlinked\" tool can't be edited. Possible operations for OWNERs/admins: set the permissions on the \"symlink\" delete the \"symlink\"","title":"Management of a \"symlinked\" tool"},{"location":"manual/10_Manage_Tools/10.8._Symlinks_between_tools/#set-permissions","text":"Permissions are being set by the way as for general tools: Click the gear icon in the right-upper corner of the \"symlinked\" tool page. Click the Permissions item: In the opened popup configure desired permissions: Note : configured permissions are valid only for the \"symlink\". Permissions for the original tool shall be configured separately at the original tool page","title":"Set permissions"},{"location":"manual/10_Manage_Tools/10.8._Symlinks_between_tools/#remove-a-symlinked-tool","text":"To delete a \"symlinked\" tool: Click the gear icon in the right-upper corner of the \"symlinked\" tool page. Click the Delete tool link item: Confirm the deletion:","title":"Remove a \"symlinked\" tool"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/","text":"10. Manage Tools Overview \"Details\" view pane Registry Tool group Search Show attributes/Hide attributes \"Gear\" icon Personal Docker repository (Tool group) List of Tools Tool information page Description tab Versions tab Settings tab For more information about Docker container lifecycle and instance lifecycle see at Appendix A. Instance and Docker container lifecycles . You also can view tools details via the CLI . See 14.8. View tools definitions . Overview The Tools tab represents a list of available docker registries and docker images that contain tools. Using Docker images allows you to configure the same processing environment on each node regardless of the node type. Every Tool object in the Cloud Pipeline is a representation of Docker image. \"Details\" view pane At the top of the page, you'll see the basic objects of the \"Tools\" space. Registry Click on the registry name to see a drop-down list of available Docker registries and choose one. Tool group Press under the arrow to see a list of available Tool groups or to search Tool group by the name. Only the lower-case alphanumeric string is allowed for a Tool group name. Note : when you navigate to the Docker registry, the Tool group shown by default will be chosen based on the following conditions: If a user is included into some group (e.g. \"cancer\") and a Tool group with the same name exists, it will be shown by default. See more about user groups here . If the first condition isn't met, \" library \" group will be shown by default. If \" library \" group doesn't exist, \" default \" group will be shown by default. If the \" default \" group doesn't exist either, \" personal \" group will be shown. If none of the above groups doesn't exist/user doesn't have access to them, the first Tool group from the list will be shown by default. Search field This field helps to find a Tool by name in particular Tool group in a registry. Show attributes/Hide attributes Show attributes/Hide attributes opens the Attributes pane, where you can see and edit a list of key=value attributes of the tool group. See 17. CP objects tagging by additional attributes . \"Gear\" icon The following options are available: Option Description Registry This button allows to create a new registry or Edit/Delete current. Group Allows creating new Tool group or Edit/Delete current. + Enable Tool Enables a Tool in a registry. How to configure Configures the Docker client to push/pull images to/from a registry. Note : Docker client needs to be installed. For installation instructions refer to https://docs.docker.com/install/ . Personal Docker repository (Tool group) All tools within such repository are named as user_name / tool_name . Note : If a user loads a registry and there is no \"personal\" group in it, it shall be checked whether he has WRITE access to the registry. If No - do not display \"personal\" section (1) in the registry. If Yes - a user will see the message \"Personal tool group was not found in registry\" (2) . On top of that, you'll see a suggestion to explore library Tool group. See how to create personal Tool group here . List of Tools Tools list will be shown after you select group and registry. Tool information page Click on a Tool's name to open Tool information page. In the top right corner you can find the following buttons: Control Description \"Displays\" icon This icon includes: Show attributes/Hide attributes This button is used to show attributes of a Tool. Note : If selected Tool has any defined attribute, attributes pane is shown by default. For more details see 17. CP objects tagging by additional attributes . \"Show issues/Hide issues\" shows/hides the issues of the current Tool to discuss. To learn more see here . \"Gear\" icon Manage Tool permissions or delete a Tool. Run Run an instance with this Tool. Tool information page is divided into 3 tabs. Description tab This tab shows Tool description. For registries with Pipeline authentication option, you'll also see the Docker pull command on this tab if you have READ access to a Tool. Versions tab Choose this tab to see a list of Tool versions. About internal Tool version menu see 10.7. Tool version menu . Each version has the following icons and controls: Control/Label Description Name Name of version Scanning status If the security scanning is forced, you'll see the status \" in progress \" of security scanning. If the security scanning is failed, you'll see the status \" failed \". Last successful scan: The label shows if a version is successfully scanned at any time. The label contains date and time of the last successful attempt. Last scan date The label shows if a version scanning is failed. The label contains date and time of the last scanning attempt. Colored bars Hover over the colored bars to see scan status - a number of vulnerabilities grouped by severity (e.g. Critical, High, Medium, ...). Digest The label shows unique identifier of docker image. Corresponding aliases The label shows aliases of docker image (e.g. if some digest has more than one alias). Image size The label shows the size of docker image. Note : this value is provided for the \"gzipped\" docker image. When pulled to the local workstation or the cloud instance - the size of the image will be greater. Modified date The label shows modified date of docker image. SCAN Control forces the security scanning process. Available only for users with ROLE_ADMIN role. Run Allow to run the particular Tool version with default settings or customize it. Delete Delete the particular Tool version. In addition, the Version tab contains View unscanned version control. The control is visible if unscanned versions exist. More about Security scan feature you could learn here . Settings tab Navigate to this tab to see Tool attributes and execution defaults: Tool endpoints - specify an endpoint for the service launched in a Tool. Tool attributes - tool labels to briefly describe the Tool. Execution defaults - execution environment settings with which tool will be run by default: Instance type - an instance type in terms of Cloud Provider with specifying amounts of CPU, RAM and GPU. Price type - spot or on-demand type of instance. Disk - instance disk size in Gb. Limit mounts - available storages for Tool execution. Configure cluster - by clicking on this button cluster can be configured (for more details see here ). Cloud Region - the field where the admin/tool owner could select a specific Cloud Provider / Region to enforce users to run that tool in it. By default, has Not configured value. This means, that a tool will be launched in a Default region (configured in the global settings) or a user can set any allowed Cloud Region / Provider manually. Cmd template - default command for Tool execution. System parameters - system parameters that can be used during Tool execution. Custom parameters - specific parameters that can be used during Tool execution. Navigate back to the Tools group page from the Tool description with the arrow button on the top-left corner.","title":"10.0. Overview"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#10-manage-tools","text":"Overview \"Details\" view pane Registry Tool group Search Show attributes/Hide attributes \"Gear\" icon Personal Docker repository (Tool group) List of Tools Tool information page Description tab Versions tab Settings tab For more information about Docker container lifecycle and instance lifecycle see at Appendix A. Instance and Docker container lifecycles . You also can view tools details via the CLI . See 14.8. View tools definitions .","title":"10. Manage Tools"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#overview","text":"The Tools tab represents a list of available docker registries and docker images that contain tools. Using Docker images allows you to configure the same processing environment on each node regardless of the node type. Every Tool object in the Cloud Pipeline is a representation of Docker image.","title":"Overview"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#details-view-pane","text":"At the top of the page, you'll see the basic objects of the \"Tools\" space.","title":"\"Details\" view pane"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#registry","text":"Click on the registry name to see a drop-down list of available Docker registries and choose one.","title":"Registry"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#tool-group","text":"Press under the arrow to see a list of available Tool groups or to search Tool group by the name. Only the lower-case alphanumeric string is allowed for a Tool group name. Note : when you navigate to the Docker registry, the Tool group shown by default will be chosen based on the following conditions: If a user is included into some group (e.g. \"cancer\") and a Tool group with the same name exists, it will be shown by default. See more about user groups here . If the first condition isn't met, \" library \" group will be shown by default. If \" library \" group doesn't exist, \" default \" group will be shown by default. If the \" default \" group doesn't exist either, \" personal \" group will be shown. If none of the above groups doesn't exist/user doesn't have access to them, the first Tool group from the list will be shown by default.","title":"Tool group"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#search-field","text":"This field helps to find a Tool by name in particular Tool group in a registry.","title":"Search field"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#show-attributeshide-attributes","text":"Show attributes/Hide attributes opens the Attributes pane, where you can see and edit a list of key=value attributes of the tool group. See 17. CP objects tagging by additional attributes .","title":"Show attributes/Hide attributes"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#gear-icon","text":"The following options are available: Option Description Registry This button allows to create a new registry or Edit/Delete current. Group Allows creating new Tool group or Edit/Delete current. + Enable Tool Enables a Tool in a registry. How to configure Configures the Docker client to push/pull images to/from a registry. Note : Docker client needs to be installed. For installation instructions refer to https://docs.docker.com/install/ .","title":"\"Gear\" icon"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#personal-docker-repository-tool-group","text":"All tools within such repository are named as user_name / tool_name . Note : If a user loads a registry and there is no \"personal\" group in it, it shall be checked whether he has WRITE access to the registry. If No - do not display \"personal\" section (1) in the registry. If Yes - a user will see the message \"Personal tool group was not found in registry\" (2) . On top of that, you'll see a suggestion to explore library Tool group. See how to create personal Tool group here .","title":"Personal Docker repository (Tool group)"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#list-of-tools","text":"Tools list will be shown after you select group and registry.","title":"List of Tools"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#tool-information-page","text":"Click on a Tool's name to open Tool information page. In the top right corner you can find the following buttons: Control Description \"Displays\" icon This icon includes: Show attributes/Hide attributes This button is used to show attributes of a Tool. Note : If selected Tool has any defined attribute, attributes pane is shown by default. For more details see 17. CP objects tagging by additional attributes . \"Show issues/Hide issues\" shows/hides the issues of the current Tool to discuss. To learn more see here . \"Gear\" icon Manage Tool permissions or delete a Tool. Run Run an instance with this Tool. Tool information page is divided into 3 tabs.","title":"Tool information page"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#description-tab","text":"This tab shows Tool description. For registries with Pipeline authentication option, you'll also see the Docker pull command on this tab if you have READ access to a Tool.","title":"Description\u00a0tab"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#versions-tab","text":"Choose this tab to see a list of Tool versions. About internal Tool version menu see 10.7. Tool version menu . Each version has the following icons and controls: Control/Label Description Name Name of version Scanning status If the security scanning is forced, you'll see the status \" in progress \" of security scanning. If the security scanning is failed, you'll see the status \" failed \". Last successful scan: The label shows if a version is successfully scanned at any time. The label contains date and time of the last successful attempt. Last scan date The label shows if a version scanning is failed. The label contains date and time of the last scanning attempt. Colored bars Hover over the colored bars to see scan status - a number of vulnerabilities grouped by severity (e.g. Critical, High, Medium, ...). Digest The label shows unique identifier of docker image. Corresponding aliases The label shows aliases of docker image (e.g. if some digest has more than one alias). Image size The label shows the size of docker image. Note : this value is provided for the \"gzipped\" docker image. When pulled to the local workstation or the cloud instance - the size of the image will be greater. Modified date The label shows modified date of docker image. SCAN Control forces the security scanning process. Available only for users with ROLE_ADMIN role. Run Allow to run the particular Tool version with default settings or customize it. Delete Delete the particular Tool version. In addition, the Version tab contains View unscanned version control. The control is visible if unscanned versions exist. More about Security scan feature you could learn here .","title":"Versions\u00a0tab"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#settings-tab","text":"Navigate to this tab to see Tool attributes and execution defaults: Tool endpoints - specify an endpoint for the service launched in a Tool. Tool attributes - tool labels to briefly describe the Tool. Execution defaults - execution environment settings with which tool will be run by default: Instance type - an instance type in terms of Cloud Provider with specifying amounts of CPU, RAM and GPU. Price type - spot or on-demand type of instance. Disk - instance disk size in Gb. Limit mounts - available storages for Tool execution. Configure cluster - by clicking on this button cluster can be configured (for more details see here ). Cloud Region - the field where the admin/tool owner could select a specific Cloud Provider / Region to enforce users to run that tool in it. By default, has Not configured value. This means, that a tool will be launched in a Default region (configured in the global settings) or a user can set any allowed Cloud Region / Provider manually. Cmd template - default command for Tool execution. System parameters - system parameters that can be used during Tool execution. Custom parameters - specific parameters that can be used during Tool execution. Navigate back to the Tools group page from the Tool description with the arrow button on the top-left corner.","title":"Settings\u00a0tab"},{"location":"manual/11_Manage_Runs/11.1._Manage_runs_lifecycles/","text":"11.1. Manage runs lifecycles Only users with ROLE_ADMIN or OWNERS can manage runs lifecycles (pause/resume/stop/terminate). Cloud Platform currently provides functionality to launch and access services on Cloud hosted calculation nodes. Launching a service takes up to several minutes depending on multiple factors. When work with service is done, instance is terminated and all the local data and environment (installed tools, settings) are completely lost. In order to store the data it should be uploaded to Cloud data storage before service termination, to save service environment user may user COMMIT option to update a service or create a new one, but for some use cases, e.g. script development in RStudio , these options may be inconvenient. PAUSE and RESUME options allow to reduce time to start a service, have an option to store service state and to reduce expenses for idle services. Stopped instances cost less than running instances. Note : pause/resume options are available only for on-demand instances. Price type can be set during Run configuration in the Advanced tab. Note : you can't pause/resume cluster runs even with On-demand price type. Pause/resume run Find a run you want to pause in the Active runs tab and press Pause . Confirm pausing. A Run will have status PAUSING for a short period of time. Then RESUME option will appear. To resume the Run press the Resume button and confirm this action. A Run will have status RESUMING for a short period of time. Then a Run will continue working again. Note : Not always paused run could be resumed. E.g. user may hit a situation of resource limits - instance type was available when run was initially launched, but at the moment of resume operation provider has no sufficient capacity for this type. In such cases, run will be returned to the Paused state. User will be notified about that - by the hint message near the RESUME button: Or at the Run information page : Or at the ACTIVE RUNS panel of the main Dashboard: Stop/terminate run STOP option allows to stop a run execution forcibly. Once a run is stopped - all its local data will be deleted, this action couldn't be undone. Note : This option is available only for initializing/executing runs, not for paused ones. Note : This action only stops run and doesn't terminate cluster node. Find a run you want to stop in the Active runs tab and press STOP . Confirm action by click \" STOP \" button. After that, run execution will be stopped and run with \"Stopped\" state will appear at \"COMPLETED RUNS\" tab. Note : also you can stop a run via CLI. For more details see here . Some of the jobs, that were paused (either manually, or by the automated service), may be not needed anymore. In that case, user is more convenient to use TERMINATE option. TERMINATE option allows to terminate compute node of a paused run without its resuming. Note : This option is shown only for paused runs. Note : This action terminates cluster node and marks run as \"Stopped\". Find a paused run you want to terminate in the Active runs tab and press TERMINATE . Confirm terminating. After that, cluster node of that run will be terminated and run with \"Stopped\" state will appear at \"COMPLETED RUNS\" tab.","title":"11.1. Manage runs lifecycles"},{"location":"manual/11_Manage_Runs/11.1._Manage_runs_lifecycles/#111-manage-runs-lifecycles","text":"Only users with ROLE_ADMIN or OWNERS can manage runs lifecycles (pause/resume/stop/terminate). Cloud Platform currently provides functionality to launch and access services on Cloud hosted calculation nodes. Launching a service takes up to several minutes depending on multiple factors. When work with service is done, instance is terminated and all the local data and environment (installed tools, settings) are completely lost. In order to store the data it should be uploaded to Cloud data storage before service termination, to save service environment user may user COMMIT option to update a service or create a new one, but for some use cases, e.g. script development in RStudio , these options may be inconvenient. PAUSE and RESUME options allow to reduce time to start a service, have an option to store service state and to reduce expenses for idle services. Stopped instances cost less than running instances. Note : pause/resume options are available only for on-demand instances. Price type can be set during Run configuration in the Advanced tab. Note : you can't pause/resume cluster runs even with On-demand price type.","title":"11.1. Manage runs lifecycles"},{"location":"manual/11_Manage_Runs/11.1._Manage_runs_lifecycles/#pauseresume-run","text":"Find a run you want to pause in the Active runs tab and press Pause . Confirm pausing. A Run will have status PAUSING for a short period of time. Then RESUME option will appear. To resume the Run press the Resume button and confirm this action. A Run will have status RESUMING for a short period of time. Then a Run will continue working again. Note : Not always paused run could be resumed. E.g. user may hit a situation of resource limits - instance type was available when run was initially launched, but at the moment of resume operation provider has no sufficient capacity for this type. In such cases, run will be returned to the Paused state. User will be notified about that - by the hint message near the RESUME button: Or at the Run information page : Or at the ACTIVE RUNS panel of the main Dashboard:","title":"Pause/resume run"},{"location":"manual/11_Manage_Runs/11.1._Manage_runs_lifecycles/#stopterminate-run","text":"STOP option allows to stop a run execution forcibly. Once a run is stopped - all its local data will be deleted, this action couldn't be undone. Note : This option is available only for initializing/executing runs, not for paused ones. Note : This action only stops run and doesn't terminate cluster node. Find a run you want to stop in the Active runs tab and press STOP . Confirm action by click \" STOP \" button. After that, run execution will be stopped and run with \"Stopped\" state will appear at \"COMPLETED RUNS\" tab. Note : also you can stop a run via CLI. For more details see here . Some of the jobs, that were paused (either manually, or by the automated service), may be not needed anymore. In that case, user is more convenient to use TERMINATE option. TERMINATE option allows to terminate compute node of a paused run without its resuming. Note : This option is shown only for paused runs. Note : This action terminates cluster node and marks run as \"Stopped\". Find a paused run you want to terminate in the Active runs tab and press TERMINATE . Confirm terminating. After that, cluster node of that run will be terminated and run with \"Stopped\" state will appear at \"COMPLETED RUNS\" tab.","title":"Stop/terminate run"},{"location":"manual/11_Manage_Runs/11.2._Auto-commit_Docker_image/","text":"11.2. Auto-commit Docker image User shall have ROLE_ADMIN role or be an OWNER of the Run to stop it and auto-commit Docker image. Auto-committing is a useful Cloud Pipeline option that allows to save current Docker image state before stopping a Run. In the Active runs tab select a Run and press STOP Select the checkbox Persist current docker image state , give that Docker image a name and a version (optional), e.g. auto-committed-version ). Press STOP . After that a Run will have a COMMITTING status for a short period of time. And then it will be stopped.","title":"11.2. Auto-commit Docker images"},{"location":"manual/11_Manage_Runs/11.2._Auto-commit_Docker_image/#112-auto-commit-docker-image","text":"User shall have ROLE_ADMIN role or be an OWNER of the Run to stop it and auto-commit Docker image. Auto-committing is a useful Cloud Pipeline option that allows to save current Docker image state before stopping a Run. In the Active runs tab select a Run and press STOP Select the checkbox Persist current docker image state , give that Docker image a name and a version (optional), e.g. auto-committed-version ). Press STOP . After that a Run will have a COMMITTING status for a short period of time. And then it will be stopped.","title":"11.2. Auto-commit Docker image"},{"location":"manual/11_Manage_Runs/11.3._Sharing_with_other_users_or_groups_of_users/","text":"11.3. Sharing with other users or groups of users Overview Sharing a run with user(s) Sharing a run with users group(s) Work with a sharing running instance (for not owners/admins) Sharing runs with the anonymous users Overview For certain use cases it is beneficial to be able to share applications with other users/groups. Cloud platform allows ability when runs environments will be accessed for several users, not only for the user, who launched the run ( OWNER ). Sharing of a run - allows to share as the interactive tools endpoints (e.g. rstudio , jupyter , nomachine , etc.) and SSH sessions too. Sharing a run with user(s) In this example we will share a run with other user(s). Note : for do that, user account shall be registered within CP users catalog and granted READ EXECUTE for the pipeline/tool. User must be an OWNER of the running instance. Note : in the example we will share a run of the RStudio tool (for more information about launching Tools see 10.5. Launch a Tool ). Open run logs of the instance: Click the link near the label \" Share with \": In the opened pop-up window click button. In the appeared window enter user name, for whom you want to share running instance. Confirm selected user by clicking the OK button: If necessary, add more users. If you want to give a SSH-access to the instance - set the Enable SSH connection checkbox. If this checkbox is not set, only the interactive tools endpoints will be shared. When finished, click the Ok button: In the run logs, users names for whom you shared running instance will appear near the label Share with : Copy the link near the label Endpoint , send it to users, for whom you shared the instance: To share the SSH-access to a non-interactive run for the user - use the similar steps. For a non-interactive run the \"share\" pop-up looks slightly different (\" Enable SSH connection \" checkboxes are missed): Sharing a run with users group(s) In this example we will share a run with other users group(s). Note : for do that, user account shall be registered within CP users catalog and granted READ EXECUTE for the pipeline/tool. User must be an OWNER of the running instance. Note : in the example we will share a run of the RStudio tool (for more information about launching Tools see 10.5. Launch a Tool ). Open run logs of the instance: Click the link near the label Share with : In the opened pop-up window click button. In the appeared window enter user group's name, for which you want to share running instance. Confirm selected user group by click the OK button: If necessary, add more groups. If you want to give a SSH-access to the instance - set the Enable SSH connection checkbox. If this checkbox is not set, only the interactive tools endpoints will be shared. When finished, click the OK button: In the run logs, user group names, for which you shared running instance, will appear near the label Share with : Copy the link near the label Endpoint , send it to users, for which group(s) you shared the instance: To share the SSH-access to a non-interactive run for the user group - use the similar steps. For a non-interactive run the \"share\" pop-up looks slightly different (\" Enable SSH connection \" checkboxes are missed): Work with a sharing running instance (for not owners/admins) A current user can be accessed to a service, without running own jobs, if that service was shared for a current user or his group(s). Note : for do that, user account shall be registered within CP users catalog and granted \"sharing\" permission for the instance. Way 1 Log in at the Cloud Pipeline. Open a new tab in a browser and input the link of the sharing running instance, that you received. The GUI of the Tool, of that running instance was shared, will be displayed. For example described above, it will be the RStudio GUI: Way 2 On the Home dashboard click button. In the opened popup enable the checkbox SERVICES and click the OK button: On the appeared SERVICES widget at the Home dashboard page accessible \"shared\" services will be displayed: Click it. The GUI of the Tool of the running shared instance will be displayed. Way 3. For the runs with the shared SSH-access If Enable SSH connection checkbox was set at the sharing configure form (for the interactive run) or the non-interactive run was shared with a current user or his group(s), the SSH-access to the running instance can be obtained. For that - hover over the service \"card\" in the SERVICES widget at the Home dashboard page and click the SSH hyperlink: A new page with the Terminal access to the shared instance will appear: Sharing runs with the anonymous users For certain use-cases, it is required to allow such type of access for any user, who has successfully passed the IdP authentication but is not registered in the Cloud Pipeline and also such users shall not be automatically registered at all and remain Anonymous . To enable such behavior, the following application properties have to be specified before the deployment: saml.user.auto.create=EXPLICIT_GROUP saml.user.allow.anonymous=true Once anonymous access is enabled system-wide each run that requires to be accessible by Anonymous has to be configured to share endpoints with the following user group - ROLE_ANONYMOUS_USER . It could be performed in the following way: Cloud Pipeline user launches the run whose interactive endpoint he wishes to share. The user opens the Run logs page of the launched run Then clicks the link near the Share with label: In the opened popup clicks the corresponding button to share with a group/role: In the appeared window, the user should select the ROLE_ANONYMOUS_USER role: Sharing with the Anonymous will be displayed at the Run logs page: The user should copy the Endpoint-link of the run and send it to the Anonymous user he wants to share. Anonymous user should open a new tab in a browser and just input the link of the sharing running instance, that he has received. If that user passes SAML authentication, he will get access to the endpoint. Attempts to open any other Platform pages will fail. Note : a user is treated as Anonymous if he is logging in and the SAML response is valid, but the user is not registered in the Platform and the auto-registration (of any kind) is not enabled.","title":"11.3. Sharing with other users"},{"location":"manual/11_Manage_Runs/11.3._Sharing_with_other_users_or_groups_of_users/#113-sharing-with-other-users-or-groups-of-users","text":"Overview Sharing a run with user(s) Sharing a run with users group(s) Work with a sharing running instance (for not owners/admins) Sharing runs with the anonymous users","title":"11.3. Sharing with other users or groups of users"},{"location":"manual/11_Manage_Runs/11.3._Sharing_with_other_users_or_groups_of_users/#overview","text":"For certain use cases it is beneficial to be able to share applications with other users/groups. Cloud platform allows ability when runs environments will be accessed for several users, not only for the user, who launched the run ( OWNER ). Sharing of a run - allows to share as the interactive tools endpoints (e.g. rstudio , jupyter , nomachine , etc.) and SSH sessions too.","title":"Overview"},{"location":"manual/11_Manage_Runs/11.3._Sharing_with_other_users_or_groups_of_users/#sharing-a-run-with-users","text":"In this example we will share a run with other user(s). Note : for do that, user account shall be registered within CP users catalog and granted READ EXECUTE for the pipeline/tool. User must be an OWNER of the running instance. Note : in the example we will share a run of the RStudio tool (for more information about launching Tools see 10.5. Launch a Tool ). Open run logs of the instance: Click the link near the label \" Share with \": In the opened pop-up window click button. In the appeared window enter user name, for whom you want to share running instance. Confirm selected user by clicking the OK button: If necessary, add more users. If you want to give a SSH-access to the instance - set the Enable SSH connection checkbox. If this checkbox is not set, only the interactive tools endpoints will be shared. When finished, click the Ok button: In the run logs, users names for whom you shared running instance will appear near the label Share with : Copy the link near the label Endpoint , send it to users, for whom you shared the instance: To share the SSH-access to a non-interactive run for the user - use the similar steps. For a non-interactive run the \"share\" pop-up looks slightly different (\" Enable SSH connection \" checkboxes are missed):","title":"Sharing a run with user(s)"},{"location":"manual/11_Manage_Runs/11.3._Sharing_with_other_users_or_groups_of_users/#sharing-a-run-with-users-groups","text":"In this example we will share a run with other users group(s). Note : for do that, user account shall be registered within CP users catalog and granted READ EXECUTE for the pipeline/tool. User must be an OWNER of the running instance. Note : in the example we will share a run of the RStudio tool (for more information about launching Tools see 10.5. Launch a Tool ). Open run logs of the instance: Click the link near the label Share with : In the opened pop-up window click button. In the appeared window enter user group's name, for which you want to share running instance. Confirm selected user group by click the OK button: If necessary, add more groups. If you want to give a SSH-access to the instance - set the Enable SSH connection checkbox. If this checkbox is not set, only the interactive tools endpoints will be shared. When finished, click the OK button: In the run logs, user group names, for which you shared running instance, will appear near the label Share with : Copy the link near the label Endpoint , send it to users, for which group(s) you shared the instance: To share the SSH-access to a non-interactive run for the user group - use the similar steps. For a non-interactive run the \"share\" pop-up looks slightly different (\" Enable SSH connection \" checkboxes are missed):","title":"Sharing a run with users group(s)"},{"location":"manual/11_Manage_Runs/11.3._Sharing_with_other_users_or_groups_of_users/#work-with-a-sharing-running-instance-for-not-ownersadmins","text":"A current user can be accessed to a service, without running own jobs, if that service was shared for a current user or his group(s). Note : for do that, user account shall be registered within CP users catalog and granted \"sharing\" permission for the instance.","title":"Work with a sharing running instance (for not owners/admins)"},{"location":"manual/11_Manage_Runs/11.3._Sharing_with_other_users_or_groups_of_users/#way-1","text":"Log in at the Cloud Pipeline. Open a new tab in a browser and input the link of the sharing running instance, that you received. The GUI of the Tool, of that running instance was shared, will be displayed. For example described above, it will be the RStudio GUI:","title":"Way 1"},{"location":"manual/11_Manage_Runs/11.3._Sharing_with_other_users_or_groups_of_users/#way-2","text":"On the Home dashboard click button. In the opened popup enable the checkbox SERVICES and click the OK button: On the appeared SERVICES widget at the Home dashboard page accessible \"shared\" services will be displayed: Click it. The GUI of the Tool of the running shared instance will be displayed.","title":"Way 2"},{"location":"manual/11_Manage_Runs/11.3._Sharing_with_other_users_or_groups_of_users/#way-3-for-the-runs-with-the-shared-ssh-access","text":"If Enable SSH connection checkbox was set at the sharing configure form (for the interactive run) or the non-interactive run was shared with a current user or his group(s), the SSH-access to the running instance can be obtained. For that - hover over the service \"card\" in the SERVICES widget at the Home dashboard page and click the SSH hyperlink: A new page with the Terminal access to the shared instance will appear:","title":"Way 3. For the runs with the shared SSH-access"},{"location":"manual/11_Manage_Runs/11.3._Sharing_with_other_users_or_groups_of_users/#sharing-runs-with-the-anonymous-users","text":"For certain use-cases, it is required to allow such type of access for any user, who has successfully passed the IdP authentication but is not registered in the Cloud Pipeline and also such users shall not be automatically registered at all and remain Anonymous . To enable such behavior, the following application properties have to be specified before the deployment: saml.user.auto.create=EXPLICIT_GROUP saml.user.allow.anonymous=true Once anonymous access is enabled system-wide each run that requires to be accessible by Anonymous has to be configured to share endpoints with the following user group - ROLE_ANONYMOUS_USER . It could be performed in the following way: Cloud Pipeline user launches the run whose interactive endpoint he wishes to share. The user opens the Run logs page of the launched run Then clicks the link near the Share with label: In the opened popup clicks the corresponding button to share with a group/role: In the appeared window, the user should select the ROLE_ANONYMOUS_USER role: Sharing with the Anonymous will be displayed at the Run logs page: The user should copy the Endpoint-link of the run and send it to the Anonymous user he wants to share. Anonymous user should open a new tab in a browser and just input the link of the sharing running instance, that he has received. If that user passes SAML authentication, he will get access to the endpoint. Attempts to open any other Platform pages will fail. Note : a user is treated as Anonymous if he is logging in and the SAML response is valid, but the user is not registered in the Platform and the auto-registration (of any kind) is not enabled.","title":"Sharing runs with the anonymous users"},{"location":"manual/11_Manage_Runs/11.4._Automatic_actions_after_notifications/","text":"Automatic actions with runs In this section, let's consider the configurable behavior of automatic actions/notifications for the launched runs that are being in \"idle\" or \"under pressure\" state for a long time. The following view of high-level metrics information for the Active Runs is implemented: - this auxiliary label is shown when node's CPU consumption is lower than a certain level, defined by the admin. This label should attract the users attention cause such run may produce extra costs. - this auxiliary label is shown when node's Memory/Disk consumption is higher than a certain level, defined by the admin. This label should attract the users attention cause such run may accidentally fail. These labels are displayed: at the Runs page at the Run logs page at the main dashboard (the ACTIVE RUNS panel) If a user clicks that label from the Runs or the Run logs page the Cluster node Monitor will be opened to view the current node consumption. Admins can configure the emergence of these labels and system actions for each run state (\"Idle\"/\"Pressure\") by the system-level parameters. \"Idle\" runs The system behavior for the \"idle\" runs is defined by the set of the following System-level parameters ( Preferences ): Preference name Description system.max.idle.timeout.minutes Specifies the duration in minutes after that the system should check node's activity. If after this duration node's CPU utilization will be below system.idle.cpu.threshold - email notification IDLE_RUN will be sent to the user and the run itself will be marked by the label system.idle.action.timeout.minutes Specifies the duration in minutes. If node's CPU utilization is below system.idle.cpu.threshold for this duration after the system.max.idle.timeout.minutes is over - an action, specified in system.idle.action will be performed system.idle.cpu.threshold Specifies percentage of the node's CPU utilization, below which an action shall be taken system.idle.action Sets which action to perform with the node, that has the CPU utilization below than system.idle.cpu.threshold : NOTIFY - only send email notification IDLE_RUN . This action will be repeated every system.idle.action.timeout.minutes if the node's CPU utilization will be still below than system.idle.cpu.threshold PAUSE - pause an instance if possible (only if the instance is On-Demand , Spot instances are skipped) and send single email notification IDLE_RUN_PAUSED PAUSE_OR_STOP - pause an instance if it is On-Demand or stop an instance if it is Spot and send the corresponding single email notification IDLE_RUN_PAUSED / IDLE_RUN_STOPPED STOP - Stop an instance, disregarding price-type, and send single email notification IDLE_RUN_STOPPED system.resource.monitoring.period Specifies period (in milliseconds) between the scannings of running instances to collect the monitoring metrics. After each such period, it's defined to display label for the specific instance or not Example of these settings: In general, the behavior will be the following: User launches a run After system.max.idle.timeout.minutes period, the system starts to check the node's activity. If the node's CPU utilization becomes below system.idle.cpu.threshold : email notification IDLE_RUN is being sent, the run itself is being marked by the label After system.idle.action.timeout.minutes , if the node's CPU utilization is still below system.idle.cpu.threshold : email notification IDLE_RUN is being sent (in case when system.idle.action is set as NOTIFY ) run is being paused/stopped and the corresponding email notification IDLE_RUN_PAUSED or IDLE_RUN_STOPPED is being sent (in case when system.idle.action is set as PAUSE / PAUSE_OR_STOP / STOP ) In case when system.idle.action is set as NOTIFY , email notifications IDLE_RUN continue to be sent every system.idle.action.timeout.minutes , if the node's CPU utilization remains below the system.idle.cpu.threshold The state of the label (to display or not) is checked every system.resource.monitoring.period The settings of the email notifications (message, the list of informed users, etc.) the admin can configure via the corresponding tab Email notifications of the system-level settings: Note : users can manually disable the automatic pausing of on-demand instances if they aren't used. For that the \" Auto pause \" checkbox at the Launch page shall be unchecked before the run: This action cancels only the auto pause, but the RUN_IDLE email notifications will be being sent (if the corresponding conditions will be met). \"Pressure\" runs The system behavior for the runs \"under pressure\" (high-consumed) is defined by the set of the following System-level parameters ( Preferences ): Preference name Description system.disk.consume.threshold Specifies the node's disk threshold (in %) above which the email notification HIGH_CONSUMED_RESOURCES will be sent and the corresponding run will be marked by the label system.memory.consume.threshold Specifies the node's memory threshold (in %) above which the email notification HIGH_CONSUMED_RESOURCES will be sent and the corresponding run will be marked by the label Example of these settings: So, when memory or disk consuming will be higher than a threshold value for a specified period of time (in average) - a notification will be sent (and resent after a delay, if the problem is still in place. The default repeat delay is 30 minutes, it could be configured before the stand deployment). Preferences of the notification could be configured at the HIGH_CONSUMED_RESOURCES section of the Email notifications of the system-level settings:","title":"11.4. Automatic labels and actions for the runs"},{"location":"manual/11_Manage_Runs/11.4._Automatic_actions_after_notifications/#automatic-actions-with-runs","text":"In this section, let's consider the configurable behavior of automatic actions/notifications for the launched runs that are being in \"idle\" or \"under pressure\" state for a long time. The following view of high-level metrics information for the Active Runs is implemented: - this auxiliary label is shown when node's CPU consumption is lower than a certain level, defined by the admin. This label should attract the users attention cause such run may produce extra costs. - this auxiliary label is shown when node's Memory/Disk consumption is higher than a certain level, defined by the admin. This label should attract the users attention cause such run may accidentally fail. These labels are displayed: at the Runs page at the Run logs page at the main dashboard (the ACTIVE RUNS panel) If a user clicks that label from the Runs or the Run logs page the Cluster node Monitor will be opened to view the current node consumption. Admins can configure the emergence of these labels and system actions for each run state (\"Idle\"/\"Pressure\") by the system-level parameters.","title":"Automatic actions with runs"},{"location":"manual/11_Manage_Runs/11.4._Automatic_actions_after_notifications/#idle-runs","text":"The system behavior for the \"idle\" runs is defined by the set of the following System-level parameters ( Preferences ): Preference name Description system.max.idle.timeout.minutes Specifies the duration in minutes after that the system should check node's activity. If after this duration node's CPU utilization will be below system.idle.cpu.threshold - email notification IDLE_RUN will be sent to the user and the run itself will be marked by the label system.idle.action.timeout.minutes Specifies the duration in minutes. If node's CPU utilization is below system.idle.cpu.threshold for this duration after the system.max.idle.timeout.minutes is over - an action, specified in system.idle.action will be performed system.idle.cpu.threshold Specifies percentage of the node's CPU utilization, below which an action shall be taken system.idle.action Sets which action to perform with the node, that has the CPU utilization below than system.idle.cpu.threshold : NOTIFY - only send email notification IDLE_RUN . This action will be repeated every system.idle.action.timeout.minutes if the node's CPU utilization will be still below than system.idle.cpu.threshold PAUSE - pause an instance if possible (only if the instance is On-Demand , Spot instances are skipped) and send single email notification IDLE_RUN_PAUSED PAUSE_OR_STOP - pause an instance if it is On-Demand or stop an instance if it is Spot and send the corresponding single email notification IDLE_RUN_PAUSED / IDLE_RUN_STOPPED STOP - Stop an instance, disregarding price-type, and send single email notification IDLE_RUN_STOPPED system.resource.monitoring.period Specifies period (in milliseconds) between the scannings of running instances to collect the monitoring metrics. After each such period, it's defined to display label for the specific instance or not Example of these settings: In general, the behavior will be the following: User launches a run After system.max.idle.timeout.minutes period, the system starts to check the node's activity. If the node's CPU utilization becomes below system.idle.cpu.threshold : email notification IDLE_RUN is being sent, the run itself is being marked by the label After system.idle.action.timeout.minutes , if the node's CPU utilization is still below system.idle.cpu.threshold : email notification IDLE_RUN is being sent (in case when system.idle.action is set as NOTIFY ) run is being paused/stopped and the corresponding email notification IDLE_RUN_PAUSED or IDLE_RUN_STOPPED is being sent (in case when system.idle.action is set as PAUSE / PAUSE_OR_STOP / STOP ) In case when system.idle.action is set as NOTIFY , email notifications IDLE_RUN continue to be sent every system.idle.action.timeout.minutes , if the node's CPU utilization remains below the system.idle.cpu.threshold The state of the label (to display or not) is checked every system.resource.monitoring.period The settings of the email notifications (message, the list of informed users, etc.) the admin can configure via the corresponding tab Email notifications of the system-level settings: Note : users can manually disable the automatic pausing of on-demand instances if they aren't used. For that the \" Auto pause \" checkbox at the Launch page shall be unchecked before the run: This action cancels only the auto pause, but the RUN_IDLE email notifications will be being sent (if the corresponding conditions will be met).","title":"\"Idle\" runs"},{"location":"manual/11_Manage_Runs/11.4._Automatic_actions_after_notifications/#pressure-runs","text":"The system behavior for the runs \"under pressure\" (high-consumed) is defined by the set of the following System-level parameters ( Preferences ): Preference name Description system.disk.consume.threshold Specifies the node's disk threshold (in %) above which the email notification HIGH_CONSUMED_RESOURCES will be sent and the corresponding run will be marked by the label system.memory.consume.threshold Specifies the node's memory threshold (in %) above which the email notification HIGH_CONSUMED_RESOURCES will be sent and the corresponding run will be marked by the label Example of these settings: So, when memory or disk consuming will be higher than a threshold value for a specified period of time (in average) - a notification will be sent (and resent after a delay, if the problem is still in place. The default repeat delay is 30 minutes, it could be configured before the stand deployment). Preferences of the notification could be configured at the HIGH_CONSUMED_RESOURCES section of the Email notifications of the system-level settings:","title":"\"Pressure\" runs"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/","text":"11. Manage Runs Overview ACTIVE RUNS Active run states Active run controls Active cluster runs Displaying additional node metrics COMPLETED RUNS Completed run states Completed run controls Completed cluster runs Run information page General information Instance Parameters Tasks Console output Controls Automatically rerun if a spot instance is terminated Overview \" Runs \" provides a list of active and completed pipeline runs. You can get parameters and logs of specific run and stop run here. \" Runs \" space has two tabs: Active runs view Completed runs view. Runs are organized in a table which is the same for both tabs: \"State\" icon - state of the run. Run - include: run name (upper row) - pipeline name and run id Cloud Region (bottom row) Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - corresponding text information also has a Provider name, e.g.: Parent run - parent run ID, if a run was launched by another run. Pipeline - include: pipeline name (upper row) - a name of a pipeline version name (bottom row) - a name of a pipeline version Docker image - a name of docker image. Started - time when a run was started. Completed - time when a run was finished. Elapsed - include: elapsed time (upper row) - a duration of a run estimated price (bottom row) - estimated price of run, which is calculated based on the run duration and selected instance type. This field is updated interactively (i.e. each 5 - 10 seconds). Owner - a user, which launched a run. Note : also you can view information about runs via CLI. See here . ACTIVE RUNS This tab displays a list of all pipelines that are currently running. Active run states - Queued state (\"sandglass\" icon) - a run is waiting in the queue for the available compute node. - Initializing state (\"rotating\" icon) - a run is being initialized, at this stage a new compute node will be created or an existing node will be reused. - Pulling state (\"download\" icon) - now pipeline Docker image is downloaded to the node. - Running state (stable \"play\" icon) - a pipeline is running. The node is appearing and pipeline input data is being downloaded to the node before the \" InitializeEnvironment \" service task appears. Pausing state (blinking \"pause\" icon) - a run is being paused. At this moment compute node will be stopped (but persisted) and the docker image state will be kept as well. - Paused state (stable \"pause\" icon) - a run is paused. At this moment compute node is already stopped but keeps it's state. Such run may be resumed. Resuming state (blinking \"play\" icon) - a paused run is being resumed. At this moment compute node is starting back from the stopped state. Also, help tooltips are provided when hovering a run state icon, e.g.: Tooltips contain a state name in bold (e.g. Queued ) and a short description of the state and info on the next stage. Active run controls Control Description PAUSE/RESUME Pauses/resumes a run. Available for On-demand non-cluster instances only. Learn more about feature here . TERMINATE Terminates compute node of a paused run without its resuming. Available for On-demand non-cluster instances only. Learn more about feature here . STOP This control stops a run execution. LOG To open a Run information page, press LOG button. Active cluster runs Cluster is a collection of instances which are connected so that they can be used together on a task. If launched run uses a cluster or an auto-scaled cluster (see sections here ), it has a certain designation: By default, only master-run is displaying at the table. To view nested runs (child-runs) click the Expand control in front of the muster-run ID: So, you can view an information about each child-run and its state, also you can stop specific nested run without stopping a parent run. You can open Run logs page (see below ) for any of the cluster runs by click it or LOG button next to run ID. Note : you can't pause cluster runs even with On-demand price type. Note : stopping a parent run will stop execution of all nested runs too. For runs with the auto-scaled cluster not all of the child-runs appear in the list immediately after parent run was launched, \"scale-up\" runs will appear only of necessity. Displaying additional node metrics According to the run states and system-level settings, additional metrics (labels) could be displayed for active runs: - Idle label - displays only for runs in Running state, for which node's CPU consumption level is below than a certain threshold for a certain period of time or longer. - Pressure label - displays only for runs in Running state, for which node's Memory/Disk consumption level is higher than a certain threshold. Values of these thresholds and time period are specified by admins via system-level settings (see here for more details and an example of using - here ). COMPLETED RUNS This tab displays a list of all pipelines runs that are already finished. Completed run states - Success state (\"OK\" icon) - successful pipeline execution. - Failed state (\"caution\" icon) - unsuccessful pipeline execution. - Stopped state (\"clock\" icon) - a pipeline manually stopped. Help tooltips are also provided when hovering a completed run state icon, e.g.: Completed run controls Control Description LINKS This control show input/output links of the pipeline RERUN This control allow rerunning of a completed run. The Launch a pipeline page will be open. LOG To open a Run information page, press LOG button. Completed cluster runs If completed run used a cluster or an auto-scaled cluster (see sections here ), it has a certain designation. Displaying of such runs on the COMPLETED RUNS tab is similar to the active cluster runs. You can view an information about each child-run and its state, also you can rerun specific nested run without a parent run. You can open Run logs page (see below ) for any of the cluster runs by click it or LOG button next to run ID: Run information page Click a row within a run list, \"Run information\" page will appear. It consists of several sections: General information This section displays general information about a run: Field Description State icon state of the run. Help tooltips are provided when hovering a run state icon, e.g.: Run ID unique ID of the run. Endpoint ( available only for tools runs ) endpoint hyperlink for the service launched in an interactive tool. For more details see 15. Interactive services . Share with ( available only for tools runs ) list of users/groups with whom an interactive tool application is shared. For more details see 11.3 Sharing with other users or groups of users . Owner a name of the user who started pipeline. Scheduled time when a pipeline was launched. Waiting for/Running for time a pipeline has been running. Started time when the node is initialized and a pipeline has started execution. Finished time when a pipeline finished execution. Estimated price price of a run according to a run duration and selected instance type. Nested runs the child-runs list in cases when a run has a number of children (e.g. a cluster run or any other case with the parent-id specified) Maintenance the list of rules that define automatical pausing/restarting schedule for on-demand non-clusters runs Nested runs Nested runs list is displaying only for master runs. It is the list with short informations about cluster child-runs: Each child-run record contains: State icons with help tooltips when hovering over them Pipeline name and version or docker image and version Run time duration Similar as a parent-run state, states for nested runs are automatically updated without page refreshing. To open any child-run log page - click its name in the list. Maintenance Here user can create/view/edit/remove schedule rules for the specific active launched run. That set rules allow to pause/resume the run automatically in the scheduled day and time. The Maintenance control is available only for active \"On-demand\" non-cluster runs. E.g.: To edit an existing schedule for the active launched run click the Configure button. The popup with created rules will appear: You can, for example, remove an existing rule and add new ones: Changes will be displayed at the Run logs page and will be applied to the active run: In general, this behavior is configured identically to the Maintenance control at the Launch page - for more details see 6.2. Launch a pipeline (item 5). Instance The \" Instance \" section lists calculation node and execution environment details that were assigned to the run when it was launched. Note : node IP is presented as a hyperlink. Clicking it will navigate to the node details, where technical information and resources utilization is available - for more details see here . Note : Docker image name link leads to a specific Tool's detail page (see an example ). Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - corresponding auxiliary Cloud Provider icon is additionally displayed, e.g.: Note : if specific run CPU/Memory/Disk consumption is lower or higher specified in the configurations values, the IDLE or PRESSURE labels will be displayed respectively: Parameters The parameters that were assigned to the run when it was launched are contained in this section. Note : parameters with types input/output/common/path are presented as hyperlinks, and will navigate to appropriate location in a Data Storage hierarchy. Note : if a user specifies system environment variables in parameter (e.g. RUN_ID ), GUI will substitute these variables with their values automatically in the \" Run information \" page. Tasks Here you can find a list of tasks of pipeline that are being executed or already finished. Clicking a task and its console output will be loaded in the right panel. Console output Here you can view console output from a whole pipeline or a selected task. It also shows a run failure cause if a run failed. Note : the Follow log control enables auto scrolling of the console output. It is useful for logs monitoring. Follow log is enabled by default, tick the box to turn it off. Also, during a pipeline run an extended node-level logging is maintained: kubelet logs (from all compute nodes) are written to the files Log files are streamed to the storage, identified by the storage.system.storage.name preference Users with the ROLE_ADMIN role can find the corresponding node logs (e.g. by the hostname or ip that are attached to the run information) in that storage by the path logs/nodes/{hostname} : Open the Run logs page of the run you want to see kubelet logs Select the InitializeNode task, find a node hostname in the console output: Copy the found hostname's value. Check the storage path specified at the storage.system.storage.name preference: Open in the Library that storage. Navigate in the opened storage to the path logs/nodes/ : Click the \"breadcrumbs\" control at the upper side of the page, enter / into the end of the path and after it paste the hostname value, copied at step 2: Press the Enter key. The folder with kubelet logs for the specified node will be opened: You can open it and see the list of logs files, divided by the messages type: You can view any of these files using Cloud Pipeline facilities or download them to your local machine: Controls Note : Completed and active runs have different controls. Example : controls of running Spot pipeline: Here's the list of all existing buttons Control Description BROWSE Allows to open the run instance filesystem in a Storage browser Web GUI - so the user can view, download, upload, delete files and directories for the current active run. COMMIT Allows modifying an existing tool that has been changed via ssh. See 10.4. Edit a Tool . EXPORT LOGS Allows to export logs. GRAPH VIEW For Luigi and WDL pipelines GRAPH VIEW is available along with a usual plain view of tasks. See 6.1.1. Building WDL pipeline with graphical PipelineBuilder . LAUNCH COMMAND Allows to generate the CLI pipe run command/API POST request for a job launch. PAUSE Allows to pause a run ( only for On-demand non-cluster runs ). RERUN Allows to rerun completed runs. RESUME Allows to resume a paused run ( only for On-demand non-cluster runs ). SHOW TIMINGS / HIDE TIMINGS Allows to show/hide duration of each task. SSH Allows to shh to the instance running \"sleep infinity\" mode. See 6.1. Create and configure pipeline . STOP Allows to stop a run. TERMINATE Allows to terminate compute node of a paused run without resuming ( only for On-demand non-cluster runs ). Automatically rerun if a spot instance is terminated In certain cases - Cloud Provider may terminate a node, that is used to run a job or an interactive tool. It may be in cases: Spot prices changed Cloud Provider experienced a hardware issue These cases aren't a Cloud Platform bug. In these cases: If a job fails due to server-related issue, special message is displayed, describing a reason for the hardware failure: If a batch job fails due to server-related issue and Cloud Provider reports one of the following instance status codes: Server.SpotInstanceShutdown - a spot instance was stopped due to price changes, Server.SpotInstanceTermination - a spot instance was terminated due to price changes, Server.InternalError - Cloud Provider hardware issue, batch job will be restarted from scratch automatically. Note : this behavior will occur, only if administrator applied and configured it (for more information see 12.10. Manage system-level settings ).","title":"11.0. Overview"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#11-manage-runs","text":"Overview ACTIVE RUNS Active run states Active run controls Active cluster runs Displaying additional node metrics COMPLETED RUNS Completed run states Completed run controls Completed cluster runs Run information page General information Instance Parameters Tasks Console output Controls Automatically rerun if a spot instance is terminated","title":"11. Manage Runs"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#overview","text":"\" Runs \" provides a list of active and completed pipeline runs. You can get parameters and logs of specific run and stop run here. \" Runs \" space has two tabs: Active runs view Completed runs view. Runs are organized in a table which is the same for both tabs: \"State\" icon - state of the run. Run - include: run name (upper row) - pipeline name and run id Cloud Region (bottom row) Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - corresponding text information also has a Provider name, e.g.: Parent run - parent run ID, if a run was launched by another run. Pipeline - include: pipeline name (upper row) - a name of a pipeline version name (bottom row) - a name of a pipeline version Docker image - a name of docker image. Started - time when a run was started. Completed - time when a run was finished. Elapsed - include: elapsed time (upper row) - a duration of a run estimated price (bottom row) - estimated price of run, which is calculated based on the run duration and selected instance type. This field is updated interactively (i.e. each 5 - 10 seconds). Owner - a user, which launched a run. Note : also you can view information about runs via CLI. See here .","title":"Overview"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#active-runs","text":"This tab displays a list of all pipelines that are currently running.","title":"ACTIVE RUNS"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#active-run-states","text":"- Queued state (\"sandglass\" icon) - a run is waiting in the queue for the available compute node. - Initializing state (\"rotating\" icon) - a run is being initialized, at this stage a new compute node will be created or an existing node will be reused. - Pulling state (\"download\" icon) - now pipeline Docker image is downloaded to the node. - Running state (stable \"play\" icon) - a pipeline is running. The node is appearing and pipeline input data is being downloaded to the node before the \" InitializeEnvironment \" service task appears. Pausing state (blinking \"pause\" icon) - a run is being paused. At this moment compute node will be stopped (but persisted) and the docker image state will be kept as well. - Paused state (stable \"pause\" icon) - a run is paused. At this moment compute node is already stopped but keeps it's state. Such run may be resumed. Resuming state (blinking \"play\" icon) - a paused run is being resumed. At this moment compute node is starting back from the stopped state. Also, help tooltips are provided when hovering a run state icon, e.g.: Tooltips contain a state name in bold (e.g. Queued ) and a short description of the state and info on the next stage.","title":"Active run states"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#active-run-controls","text":"Control Description PAUSE/RESUME Pauses/resumes a run. Available for On-demand non-cluster instances only. Learn more about feature here . TERMINATE Terminates compute node of a paused run without its resuming. Available for On-demand non-cluster instances only. Learn more about feature here . STOP This control stops a run execution. LOG To open a Run information page, press LOG button.","title":"Active run controls"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#active-cluster-runs","text":"Cluster is a collection of instances which are connected so that they can be used together on a task. If launched run uses a cluster or an auto-scaled cluster (see sections here ), it has a certain designation: By default, only master-run is displaying at the table. To view nested runs (child-runs) click the Expand control in front of the muster-run ID: So, you can view an information about each child-run and its state, also you can stop specific nested run without stopping a parent run. You can open Run logs page (see below ) for any of the cluster runs by click it or LOG button next to run ID. Note : you can't pause cluster runs even with On-demand price type. Note : stopping a parent run will stop execution of all nested runs too. For runs with the auto-scaled cluster not all of the child-runs appear in the list immediately after parent run was launched, \"scale-up\" runs will appear only of necessity.","title":"Active cluster runs"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#displaying-additional-node-metrics","text":"According to the run states and system-level settings, additional metrics (labels) could be displayed for active runs: - Idle label - displays only for runs in Running state, for which node's CPU consumption level is below than a certain threshold for a certain period of time or longer. - Pressure label - displays only for runs in Running state, for which node's Memory/Disk consumption level is higher than a certain threshold. Values of these thresholds and time period are specified by admins via system-level settings (see here for more details and an example of using - here ).","title":"Displaying additional node metrics"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#completed-runs","text":"This tab displays a list of all pipelines runs that are already finished.","title":"COMPLETED RUNS"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#completed-run-states","text":"- Success state (\"OK\" icon) - successful pipeline execution. - Failed state (\"caution\" icon) - unsuccessful pipeline execution. - Stopped state (\"clock\" icon) - a pipeline manually stopped. Help tooltips are also provided when hovering a completed run state icon, e.g.:","title":"Completed run states"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#completed-run-controls","text":"Control Description LINKS This control show input/output links of the pipeline RERUN This control allow rerunning of a completed run. The Launch a pipeline page will be open. LOG To open a Run information page, press LOG button.","title":"Completed run controls"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#completed-cluster-runs","text":"If completed run used a cluster or an auto-scaled cluster (see sections here ), it has a certain designation. Displaying of such runs on the COMPLETED RUNS tab is similar to the active cluster runs. You can view an information about each child-run and its state, also you can rerun specific nested run without a parent run. You can open Run logs page (see below ) for any of the cluster runs by click it or LOG button next to run ID:","title":"Completed cluster runs"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#run-information-page","text":"Click a row within a run list, \"Run information\" page will appear. It consists of several sections:","title":"Run information page"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#general-information","text":"This section displays general information about a run: Field Description State icon state of the run. Help tooltips are provided when hovering a run state icon, e.g.: Run ID unique ID of the run. Endpoint ( available only for tools runs ) endpoint hyperlink for the service launched in an interactive tool. For more details see 15. Interactive services . Share with ( available only for tools runs ) list of users/groups with whom an interactive tool application is shared. For more details see 11.3 Sharing with other users or groups of users . Owner a name of the user who started pipeline. Scheduled time when a pipeline was launched. Waiting for/Running for time a pipeline has been running. Started time when the node is initialized and a pipeline has started execution. Finished time when a pipeline finished execution. Estimated price price of a run according to a run duration and selected instance type. Nested runs the child-runs list in cases when a run has a number of children (e.g. a cluster run or any other case with the parent-id specified) Maintenance the list of rules that define automatical pausing/restarting schedule for on-demand non-clusters runs","title":"General information"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#nested-runs","text":"Nested runs list is displaying only for master runs. It is the list with short informations about cluster child-runs: Each child-run record contains: State icons with help tooltips when hovering over them Pipeline name and version or docker image and version Run time duration Similar as a parent-run state, states for nested runs are automatically updated without page refreshing. To open any child-run log page - click its name in the list.","title":"Nested runs"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#maintenance","text":"Here user can create/view/edit/remove schedule rules for the specific active launched run. That set rules allow to pause/resume the run automatically in the scheduled day and time. The Maintenance control is available only for active \"On-demand\" non-cluster runs. E.g.: To edit an existing schedule for the active launched run click the Configure button. The popup with created rules will appear: You can, for example, remove an existing rule and add new ones: Changes will be displayed at the Run logs page and will be applied to the active run: In general, this behavior is configured identically to the Maintenance control at the Launch page - for more details see 6.2. Launch a pipeline (item 5).","title":"Maintenance"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#instance","text":"The \" Instance \" section lists calculation node and execution environment details that were assigned to the run when it was launched. Note : node IP is presented as a hyperlink. Clicking it will navigate to the node details, where technical information and resources utilization is available - for more details see here . Note : Docker image name link leads to a specific Tool's detail page (see an example ). Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - corresponding auxiliary Cloud Provider icon is additionally displayed, e.g.: Note : if specific run CPU/Memory/Disk consumption is lower or higher specified in the configurations values, the IDLE or PRESSURE labels will be displayed respectively:","title":"Instance"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#parameters","text":"The parameters that were assigned to the run when it was launched are contained in this section. Note : parameters with types input/output/common/path are presented as hyperlinks, and will navigate to appropriate location in a Data Storage hierarchy. Note : if a user specifies system environment variables in parameter (e.g. RUN_ID ), GUI will substitute these variables with their values automatically in the \" Run information \" page.","title":"Parameters"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#tasks","text":"Here you can find a list of tasks of pipeline that are being executed or already finished. Clicking a task and its console output will be loaded in the right panel.","title":"Tasks"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#console-output","text":"Here you can view console output from a whole pipeline or a selected task. It also shows a run failure cause if a run failed. Note : the Follow log control enables auto scrolling of the console output. It is useful for logs monitoring. Follow log is enabled by default, tick the box to turn it off. Also, during a pipeline run an extended node-level logging is maintained: kubelet logs (from all compute nodes) are written to the files Log files are streamed to the storage, identified by the storage.system.storage.name preference Users with the ROLE_ADMIN role can find the corresponding node logs (e.g. by the hostname or ip that are attached to the run information) in that storage by the path logs/nodes/{hostname} : Open the Run logs page of the run you want to see kubelet logs Select the InitializeNode task, find a node hostname in the console output: Copy the found hostname's value. Check the storage path specified at the storage.system.storage.name preference: Open in the Library that storage. Navigate in the opened storage to the path logs/nodes/ : Click the \"breadcrumbs\" control at the upper side of the page, enter / into the end of the path and after it paste the hostname value, copied at step 2: Press the Enter key. The folder with kubelet logs for the specified node will be opened: You can open it and see the list of logs files, divided by the messages type: You can view any of these files using Cloud Pipeline facilities or download them to your local machine:","title":"Console output"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#controls","text":"Note : Completed and active runs have different controls. Example : controls of running Spot pipeline: Here's the list of all existing buttons Control Description BROWSE Allows to open the run instance filesystem in a Storage browser Web GUI - so the user can view, download, upload, delete files and directories for the current active run. COMMIT Allows modifying an existing tool that has been changed via ssh. See 10.4. Edit a Tool . EXPORT LOGS Allows to export logs. GRAPH VIEW For Luigi and WDL pipelines GRAPH VIEW is available along with a usual plain view of tasks. See 6.1.1. Building WDL pipeline with graphical PipelineBuilder . LAUNCH COMMAND Allows to generate the CLI pipe run command/API POST request for a job launch. PAUSE Allows to pause a run ( only for On-demand non-cluster runs ). RERUN Allows to rerun completed runs. RESUME Allows to resume a paused run ( only for On-demand non-cluster runs ). SHOW TIMINGS / HIDE TIMINGS Allows to show/hide duration of each task. SSH Allows to shh to the instance running \"sleep infinity\" mode. See 6.1. Create and configure pipeline . STOP Allows to stop a run. TERMINATE Allows to terminate compute node of a paused run without resuming ( only for On-demand non-cluster runs ).","title":"Controls"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#automatically-rerun-if-a-spot-instance-is-terminated","text":"In certain cases - Cloud Provider may terminate a node, that is used to run a job or an interactive tool. It may be in cases: Spot prices changed Cloud Provider experienced a hardware issue These cases aren't a Cloud Platform bug. In these cases: If a job fails due to server-related issue, special message is displayed, describing a reason for the hardware failure: If a batch job fails due to server-related issue and Cloud Provider reports one of the following instance status codes: Server.SpotInstanceShutdown - a spot instance was stopped due to price changes, Server.SpotInstanceTermination - a spot instance was terminated due to price changes, Server.InternalError - Cloud Provider hardware issue, batch job will be restarted from scratch automatically. Note : this behavior will occur, only if administrator applied and configured it (for more information see 12.10. Manage system-level settings ).","title":"Automatically rerun if a spot instance is terminated"},{"location":"manual/12_Manage_Settings/12.1._Add_a_new_system_event/","text":"12.1. Add a new system event An administrator can add System event notification only. Navigate to System events tab. Click +ADD . Enter a Title of notification. Enter a Body of the notification (optional). Markdown is supported for the body text. You can preview result at the Preview tab: Rank notification Severity (\" info \", \" warning \" or \" critical \"). Mark as blocking ( optional ). Blocking event emerges in the middle of the window and requires confirmation from the user to disappear. Mark as active ( optional ). Active notifications will be shown for all users of the Cloud Pipeline until admin sets them inactive. Click Create .","title":"12.1. Add a new system event"},{"location":"manual/12_Manage_Settings/12.1._Add_a_new_system_event/#121-add-a-new-system-event","text":"An administrator can add System event notification only. Navigate to System events tab. Click +ADD . Enter a Title of notification. Enter a Body of the notification (optional). Markdown is supported for the body text. You can preview result at the Preview tab: Rank notification Severity (\" info \", \" warning \" or \" critical \"). Mark as blocking ( optional ). Blocking event emerges in the middle of the window and requires confirmation from the user to disappear. Mark as active ( optional ). Active notifications will be shown for all users of the Cloud Pipeline until admin sets them inactive. Click Create .","title":"12.1. Add a new system event"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/","text":"12.10. Manage system-level settings User shall have ROLE_ADMIN to read and update system-level settings. Read system-level settings Base URLs Cluster Commit DTS Data sharing Data Storage Docker security FireCloud GCP Git Grid engine autoscaling Launch Miscellaneous Search System User Interface Make system-level settings visible to all users Update system-level settings Read system-level settings Hover to the Settings tab. Select the Preferences section. All system-level parameters are categorized into the several groups. Base URLs These settings define pipeline URLs: Setting name Description base.api.host REST API endpoint base.pipe.distributions.url URL that is used to download pipeline scripts base.dav.auth.url base.api.host.external REST API external endpoint Cluster Settings in this tab define different cluster options: Setting name Description cluster.keep.alive.minutes If node doesn't have a running pipeline on it for that amount of minutes, it will be shut down cluster.random.scheduling If this property is true, pipeline scheduler will rely on Kubernetes order of pods, otherwise pipelines will be ordered according to their parent (batch) ID cluster.instance.type Default instance type cluster.max.size Maximal number of nodes to be launched simultaneously cluster.instance.hdd_extra_multi Disk extra multiplier. Allows to add extra space, during a launch, to the disk size selected through the GUI. The size of that extra space is calculated as (Unarchived docker image size) * (Disk extra multiplier) cluster.min.size Minimal number of nodes to be launched at a time cluster.allowed.instance.types Allowed instance types. Can restrict available instance types for launching tools, pipelines, configurations cluster.enable.autoscaling Enables/disables Kubernetes autoscaler service instance.restart.state.reasons Instance status codes, upon receipt of which an instance tries automatically to restart cluster.networks.config Config that contains information to start new nodes in Cloud Provider cluster.batch.retry.count Count of automatically retries to relaunch a job, if one of the instance status codes from instance.restart.state.reasons returns, when a batch job fails instance.offer.update.rate How often instance cost is calculated (in milliseconds) cluster.autoscale.rate How often autoscaler checks what tasks are executed on each node (in milliseconds) cluster.nodeup.max.threads Maximal number of nodes that can be started simultaneously cluster.spot.bid.price The maximum price per hour that you are willing to pay for a Spot Instance. The default is the On-Demand price cloud.provider.default Sets a default CLoud Provider cluster.spot If this is true, spot instances will be launched by default cluster.docker.extra_multi Docker image extra multiplier. Allows to get an approximate size of the unarchived docker image. That size is calculated as (Archived docker image size) * (Docker image extra multiplier) cluster.kill.not.matching.nodes If this property is true, any free node that doesn't match configuration of a pending pod will be scaled down immediately, otherwise it will be left until it will be reused or expired. If most of the time we use nodes with the same configuration set this to true cluster.instance.hdd Default hard drive size for instance (in gigabytes) cluster.allowed.price.types Allowed price types. Can restrict available price types for launching tools, pipelines, configurations cluster.spot.alloc.strategy Parameter that sets the strategy of calculating the price limit for instance: on-demand - maximal instance price equals the price of the on-demand instance of the same type; manual - uses value from the cluster.spot.bid.price parameter cluster.allowed.instance.types.docker Allowed instance types for docker images (tools). Can restrict available instance types for launching tools. Has a higher priority for a tool than cluster.allowed.instance.types cluster.spot.max.attempts cluster.nodeup.retry.count Maximal number of tries to start the node cluster.high.non.batch.priority If this property is true, pipelines without parent (batch ID) will have the highest priority, otherwise - the lowest Commit This tab contains various commit settings: Setting name Description commit.pre.command.path Specifies a path to a script within a docker image, that will be executed in a currently running container, before docker commit occurs commit.username Git username commit.deploy.key Used to SSH for COMMIT. Key is stored in a DB commit.timeout Commit will fail if exceeded (in seconds) commit.post.command.path Specifies a path to a script within a docker image, that will be executed in a committed image, after docker commit occurs DTS These settings define DTS parameters: Setting name Description dts.launch.cmd dts.launch.script.url dts.dist.url Data Sharing These settings define data sharing parameters: Setting name Description data.sharing.base.api Specifies a format of the generating URL to the data storage with enabled sharing data.sharing.disclaimer Allows to set a warning text for the \"Share storage link\" pop-up Data Storage These settings define storage parameters: Setting name Description storage.fsbrowser.enabled Allows to enable FSBrowser storage.fsbrowser.wd Allows to set a work directory for FSBrowser (this directory will be opened by default and set as root ) storage.fsbrowser.transfer Allows to specify intermediate object storage for data transfer operations in FSBrowser (this is actual for \"heavy\" transfer operations to not reduce performance) storage.fsbrowser.port Allows to set a port for FSBrowser storage.fsbrowser.tmp A path to the directory where the temporary archive shall be created (when the folder is downloading). Archive is being removed when download is finished storage.fsbrowser.black.list List of directories/files which shall not be visible via FSBrowser storage.temp.credentials.duration Temporary credentials lifetime for Cloud Provider's operations with data storages (in seconds) storage.transfer.pipeline.id Pipeline ID that is used to automated data transfers from the external sites storage.system.storage.name Configures a system data storage for storing attachments from e.g. issues storage.mount.black.list List of directories where Data Storages couldn't be mounted storage.transfer.pipeline.version Pipeline version that is used to automated data transfers from the external sites storage.max.download.size Chunk size to download (bytes) storage.object.prefix A mandatory prefix for the new creating data storages storage.listing.time.limit Sets the timeout (in milliseconds) for the processing of the size getting for all input/common files before the pipeline launch. Default: 3000 milliseconds (3 sec). If computation of the files size doesn't end in this timeout, accumulated size will return as is Docker security This tab contains settings related to Docker security checks: Setting name Description security.tools.scan.all.registries If this is true, all registries will be scanned for Tools vulnerability security.tools.scan.clair.root.url Clair root URL security.tools.docker.comp.scan.root.url security.tools.jwt.token.expiration security.tools.scan.clair.connect.timeout Sets timeout for connection with Clair (in seconds) security.tools.policy.max.high.vulnerabilities Denies running a Tool if the number of high vulnerabilities exceeds the threshold. To disable the policy, set to -1 security.tools.grace.hours Allows users to run a new docker image (if it is not scanned yet) or an image with a lot of vulnerabilities during a specified period. During this period user will be able to run a tool, but an appropriate message will be displayed. Period lasts from date/time since the docker version became vulnerable or since the docker image's push time (if this version was not scanned yet) security.tools.scan.clair.read.timeout Sets timeout for Clair response (in seconds) security.tools.policy.deny.not.scanned Allow/deny execution of unscanned Tools security.tools.scan.enabled Enables/disables security scan security.tools.policy.max.medium.vulnerabilities Denies running a Tool if the number of medium vulnerabilities exceeds the threshold. To disable the policy, set to -1 security.tools.policy.max.critical.vulnerabilities Denies running a Tool if the number of critical vulnerabilities exceeds the threshold. To disable the policy, set to -1 security.tools.scan.schedule.cron Security scan schedule FireCloud These settings define FireCloud parameters: Setting name Description google.client.settings firecloud.base.url firecloud.billing.project google.client.id firecloud.launcher.cmd firecloud.instance.disk firecloud.launcher.tool google.redirect.url firecloud.api.scopes firecloud.instance.type firecloud.enable.user.auth GCP These settings define specific Google Cloud Platform parameters: Setting name Description gcp.sku.mapping gcp.regions.list Git These settings define Git parameters: Setting name Description ui.git.cli.configure.template Template for the message of the Git config that would be displayed for user in the \" Git CLI \" section of the System Settings git.user.name User name to access Git with pipelines git.external.url git.repository.hook.url git.token Token to access Git with pipelines git.repository.indexing.enabled Allows to enable the indexing of Git repository with pipelines git.user.id User id to access Git with pipelines git.host IP address where Git service is deployed Grid engine autoscaling See Appendix C. Working with autoscaled cluster runs for details. These settings define auto-scaled cluster parameters: Setting name Description ge.autoscaling.scale.down.timeout If jobs queue is empty or all jobs are running and there are some idle nodes longer than that timeout in seconds - auto-scaled cluster will start to drop idle auto-scaled nodes (\"scale-down\") ge.autoscaling.scale.up.timeout If some jobs are in waiting state longer than that timeout in seconds - auto-scaled cluster will start to attach new computation nodes to the cluster (\"scale-up\") ge.autoscaling.scale.up.polling.timeout Defines how many seconds GE autoscaler should wait for pod initialization and run initialization Launch Settings in this tab contains default Launch parameters: Setting name Description launch.jwt.token.expiration Lifetime of a pipeline token (in seconds) launch.max.scheduled.number Controls maximum number of scheduled at once runs launch.env.properties Sets of environment variables that will be passed to each running Tool launch.docker.image Default Docker image launch.cmd.template Default cmd template launch.container.cpu.resource launch.run.visibility Allow to view foreign runs based on pipeline permissions (value INHERIT ) or restrict visibility of all non-owner runs (value OWNER ) launch.dind.enable Enables Docker in Docker functionality launch.dind.container.vars Allows to specify the variables, which will be passed to the DIND container (if they are set for the host environment) launch.dind.mounts List of mounts that shall be added to k8s pod for Docker in Docker launch.task.status.update.rate Sets task status update rate, on which application will query kubernetes cluster for running task status, ms. Pod Monitor launch.pods.release.rate launch.system.parameters System parameters, that are used when launching pipelines Miscellaneous Setting name Description misc.max.tool.icon.size.kb Sets maximum size (in Kb) of the uploaded icon for the tool system.events.confirmation.metadata.key Sets the KEY for the user's attribute displaying information about \"blocking\" notifications confirmation Search Settings in this tab contains Elasticsearch parameters: Setting name Description search.elastic.scheme search.elastic.allowed.users.field search.elastic.denied.users.field search.elastic.denied.groups.field search.elastic.type.field search.elastic.host search.elastic.index.type.prefix search.elastic.port search.elastic.search.fields search.elastic.index.common.prefix search.elastic.allowed.groups.field System The settings in this tab contain parameters and actions that are performed depending on the system monitoring metrics: Setting name Description system.max.idle.timeout.minutes Specifies a duration in minutes. If CPU utilization is below system.idle.cpu.threshold for this duration - notification will be sent to the user and the corresponding run will be marked by the \"IDLE\" label system.idle.action.timeout.minutes Specifies a duration in minutes. If CPU utilization is below system.idle.cpu.threshold for this duration - an action, specified in system.idle.action will be performed system.resource.monitoring.period Specifies period (in seconds) between the users' instances scanning to collect the monitoring metrics system.monitoring.time.range Specifies time delay (in sec) after which the notification \"HIGH_CONSUMED_RESOURCES\" would be sent again, if the problem is still in place system.disk.consume.threshold Specifies disk threshold (in %) above which the notification \"HIGH_CONSUMED_RESOURCES\" would be sent and the corresponding run will be marked by the \"PRESSURE\" label system.idle.cpu.threshold Specifies percentage of the CPU utilization, below which action shall be taken system.resource.monitoring.stats.retention.period Specifies the time period (in days) during which resources utilization data is stored system.memory.consume.threshold Specifies memory threshold (in %) above which the notification \"HIGH_CONSUMED_RESOURCES\" would be sent and the corresponding run will be marked by the \"PRESSURE\" label system.idle.action Sets which action to perform on the instance, that showed low CPU utilization (that is below system.idle.cpu.threshold ): NOTIFY - only send notification PAUSE - pause an instance if possible (e.g. instance is On-Demand, Spot instances are skipped) PAUSE_OR_STOP - pause an instance if it is On-Demand, stop an instance if it is Spot STOP - Stop an instance, disregarding price-type system.external.services.endpoints User Interface Here different user interface settings can be found: Setting name Description ui.pipeline.deployment.name UI deployment name ui.pipe.cli.install.template CLI install templates for different operating systems ui.pipe.drive.mapping ui.project.indicator These attributes define a Project folder ui.pipe.cli.configure.template CLI configure templates for different operating systems ui.support.template Markdown-formatted text that will be displayed in tooltip of the \"support\" info in the main menu. If nothing is specified (empty), support icon in the main menu will not be displayed ui.controls.settings JSON file that contains control settings Make system-level settings visible to all users Hover to the Settings tab. Select the Preferences section. Choose one of the tabs with system level settings (e.g. Grid engine autoscaling ). Press the \" Eye \" button near any setting. Now it will be visible to all users by using the API. Note : press \" Eye \" button again to hide it from all users. Update system-level settings Choose any system-level setting and change its value (e.g. change cluster.keep.alive.minutes value from 10 to 15). Press the Save button. Note : before saving you can press the Revert button to return setting's value to the previous state.","title":"12.10. Manage system-level settings"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#1210-manage-system-level-settings","text":"User shall have ROLE_ADMIN to read and update system-level settings. Read system-level settings Base URLs Cluster Commit DTS Data sharing Data Storage Docker security FireCloud GCP Git Grid engine autoscaling Launch Miscellaneous Search System User Interface Make system-level settings visible to all users Update system-level settings","title":"12.10. Manage system-level settings"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#read-system-level-settings","text":"Hover to the Settings tab. Select the Preferences section. All system-level parameters are categorized into the several groups.","title":"Read system-level settings"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#base-urls","text":"These settings define pipeline URLs: Setting name Description base.api.host REST API endpoint base.pipe.distributions.url URL that is used to download pipeline scripts base.dav.auth.url base.api.host.external REST API external endpoint","title":"Base URLs"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#cluster","text":"Settings in this tab define different cluster options: Setting name Description cluster.keep.alive.minutes If node doesn't have a running pipeline on it for that amount of minutes, it will be shut down cluster.random.scheduling If this property is true, pipeline scheduler will rely on Kubernetes order of pods, otherwise pipelines will be ordered according to their parent (batch) ID cluster.instance.type Default instance type cluster.max.size Maximal number of nodes to be launched simultaneously cluster.instance.hdd_extra_multi Disk extra multiplier. Allows to add extra space, during a launch, to the disk size selected through the GUI. The size of that extra space is calculated as (Unarchived docker image size) * (Disk extra multiplier) cluster.min.size Minimal number of nodes to be launched at a time cluster.allowed.instance.types Allowed instance types. Can restrict available instance types for launching tools, pipelines, configurations cluster.enable.autoscaling Enables/disables Kubernetes autoscaler service instance.restart.state.reasons Instance status codes, upon receipt of which an instance tries automatically to restart cluster.networks.config Config that contains information to start new nodes in Cloud Provider cluster.batch.retry.count Count of automatically retries to relaunch a job, if one of the instance status codes from instance.restart.state.reasons returns, when a batch job fails instance.offer.update.rate How often instance cost is calculated (in milliseconds) cluster.autoscale.rate How often autoscaler checks what tasks are executed on each node (in milliseconds) cluster.nodeup.max.threads Maximal number of nodes that can be started simultaneously cluster.spot.bid.price The maximum price per hour that you are willing to pay for a Spot Instance. The default is the On-Demand price cloud.provider.default Sets a default CLoud Provider cluster.spot If this is true, spot instances will be launched by default cluster.docker.extra_multi Docker image extra multiplier. Allows to get an approximate size of the unarchived docker image. That size is calculated as (Archived docker image size) * (Docker image extra multiplier) cluster.kill.not.matching.nodes If this property is true, any free node that doesn't match configuration of a pending pod will be scaled down immediately, otherwise it will be left until it will be reused or expired. If most of the time we use nodes with the same configuration set this to true cluster.instance.hdd Default hard drive size for instance (in gigabytes) cluster.allowed.price.types Allowed price types. Can restrict available price types for launching tools, pipelines, configurations cluster.spot.alloc.strategy Parameter that sets the strategy of calculating the price limit for instance: on-demand - maximal instance price equals the price of the on-demand instance of the same type; manual - uses value from the cluster.spot.bid.price parameter cluster.allowed.instance.types.docker Allowed instance types for docker images (tools). Can restrict available instance types for launching tools. Has a higher priority for a tool than cluster.allowed.instance.types cluster.spot.max.attempts cluster.nodeup.retry.count Maximal number of tries to start the node cluster.high.non.batch.priority If this property is true, pipelines without parent (batch ID) will have the highest priority, otherwise - the lowest","title":"Cluster"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#commit","text":"This tab contains various commit settings: Setting name Description commit.pre.command.path Specifies a path to a script within a docker image, that will be executed in a currently running container, before docker commit occurs commit.username Git username commit.deploy.key Used to SSH for COMMIT. Key is stored in a DB commit.timeout Commit will fail if exceeded (in seconds) commit.post.command.path Specifies a path to a script within a docker image, that will be executed in a committed image, after docker commit occurs","title":"Commit"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#dts","text":"These settings define DTS parameters: Setting name Description dts.launch.cmd dts.launch.script.url dts.dist.url","title":"DTS"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#data-sharing","text":"These settings define data sharing parameters: Setting name Description data.sharing.base.api Specifies a format of the generating URL to the data storage with enabled sharing data.sharing.disclaimer Allows to set a warning text for the \"Share storage link\" pop-up","title":"Data Sharing"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#data-storage","text":"These settings define storage parameters: Setting name Description storage.fsbrowser.enabled Allows to enable FSBrowser storage.fsbrowser.wd Allows to set a work directory for FSBrowser (this directory will be opened by default and set as root ) storage.fsbrowser.transfer Allows to specify intermediate object storage for data transfer operations in FSBrowser (this is actual for \"heavy\" transfer operations to not reduce performance) storage.fsbrowser.port Allows to set a port for FSBrowser storage.fsbrowser.tmp A path to the directory where the temporary archive shall be created (when the folder is downloading). Archive is being removed when download is finished storage.fsbrowser.black.list List of directories/files which shall not be visible via FSBrowser storage.temp.credentials.duration Temporary credentials lifetime for Cloud Provider's operations with data storages (in seconds) storage.transfer.pipeline.id Pipeline ID that is used to automated data transfers from the external sites storage.system.storage.name Configures a system data storage for storing attachments from e.g. issues storage.mount.black.list List of directories where Data Storages couldn't be mounted storage.transfer.pipeline.version Pipeline version that is used to automated data transfers from the external sites storage.max.download.size Chunk size to download (bytes) storage.object.prefix A mandatory prefix for the new creating data storages storage.listing.time.limit Sets the timeout (in milliseconds) for the processing of the size getting for all input/common files before the pipeline launch. Default: 3000 milliseconds (3 sec). If computation of the files size doesn't end in this timeout, accumulated size will return as is","title":"Data Storage"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#docker-security","text":"This tab contains settings related to Docker security checks: Setting name Description security.tools.scan.all.registries If this is true, all registries will be scanned for Tools vulnerability security.tools.scan.clair.root.url Clair root URL security.tools.docker.comp.scan.root.url security.tools.jwt.token.expiration security.tools.scan.clair.connect.timeout Sets timeout for connection with Clair (in seconds) security.tools.policy.max.high.vulnerabilities Denies running a Tool if the number of high vulnerabilities exceeds the threshold. To disable the policy, set to -1 security.tools.grace.hours Allows users to run a new docker image (if it is not scanned yet) or an image with a lot of vulnerabilities during a specified period. During this period user will be able to run a tool, but an appropriate message will be displayed. Period lasts from date/time since the docker version became vulnerable or since the docker image's push time (if this version was not scanned yet) security.tools.scan.clair.read.timeout Sets timeout for Clair response (in seconds) security.tools.policy.deny.not.scanned Allow/deny execution of unscanned Tools security.tools.scan.enabled Enables/disables security scan security.tools.policy.max.medium.vulnerabilities Denies running a Tool if the number of medium vulnerabilities exceeds the threshold. To disable the policy, set to -1 security.tools.policy.max.critical.vulnerabilities Denies running a Tool if the number of critical vulnerabilities exceeds the threshold. To disable the policy, set to -1 security.tools.scan.schedule.cron Security scan schedule","title":"Docker security"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#firecloud","text":"These settings define FireCloud parameters: Setting name Description google.client.settings firecloud.base.url firecloud.billing.project google.client.id firecloud.launcher.cmd firecloud.instance.disk firecloud.launcher.tool google.redirect.url firecloud.api.scopes firecloud.instance.type firecloud.enable.user.auth","title":"FireCloud"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#gcp","text":"These settings define specific Google Cloud Platform parameters: Setting name Description gcp.sku.mapping gcp.regions.list","title":"GCP"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#git","text":"These settings define Git parameters: Setting name Description ui.git.cli.configure.template Template for the message of the Git config that would be displayed for user in the \" Git CLI \" section of the System Settings git.user.name User name to access Git with pipelines git.external.url git.repository.hook.url git.token Token to access Git with pipelines git.repository.indexing.enabled Allows to enable the indexing of Git repository with pipelines git.user.id User id to access Git with pipelines git.host IP address where Git service is deployed","title":"Git"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#grid-engine-autoscaling","text":"See Appendix C. Working with autoscaled cluster runs for details. These settings define auto-scaled cluster parameters: Setting name Description ge.autoscaling.scale.down.timeout If jobs queue is empty or all jobs are running and there are some idle nodes longer than that timeout in seconds - auto-scaled cluster will start to drop idle auto-scaled nodes (\"scale-down\") ge.autoscaling.scale.up.timeout If some jobs are in waiting state longer than that timeout in seconds - auto-scaled cluster will start to attach new computation nodes to the cluster (\"scale-up\") ge.autoscaling.scale.up.polling.timeout Defines how many seconds GE autoscaler should wait for pod initialization and run initialization","title":"Grid engine autoscaling"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#launch","text":"Settings in this tab contains default Launch parameters: Setting name Description launch.jwt.token.expiration Lifetime of a pipeline token (in seconds) launch.max.scheduled.number Controls maximum number of scheduled at once runs launch.env.properties Sets of environment variables that will be passed to each running Tool launch.docker.image Default Docker image launch.cmd.template Default cmd template launch.container.cpu.resource launch.run.visibility Allow to view foreign runs based on pipeline permissions (value INHERIT ) or restrict visibility of all non-owner runs (value OWNER ) launch.dind.enable Enables Docker in Docker functionality launch.dind.container.vars Allows to specify the variables, which will be passed to the DIND container (if they are set for the host environment) launch.dind.mounts List of mounts that shall be added to k8s pod for Docker in Docker launch.task.status.update.rate Sets task status update rate, on which application will query kubernetes cluster for running task status, ms. Pod Monitor launch.pods.release.rate launch.system.parameters System parameters, that are used when launching pipelines","title":"Launch"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#miscellaneous","text":"Setting name Description misc.max.tool.icon.size.kb Sets maximum size (in Kb) of the uploaded icon for the tool system.events.confirmation.metadata.key Sets the KEY for the user's attribute displaying information about \"blocking\" notifications confirmation","title":"Miscellaneous"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#search","text":"Settings in this tab contains Elasticsearch parameters: Setting name Description search.elastic.scheme search.elastic.allowed.users.field search.elastic.denied.users.field search.elastic.denied.groups.field search.elastic.type.field search.elastic.host search.elastic.index.type.prefix search.elastic.port search.elastic.search.fields search.elastic.index.common.prefix search.elastic.allowed.groups.field","title":"Search"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#system","text":"The settings in this tab contain parameters and actions that are performed depending on the system monitoring metrics: Setting name Description system.max.idle.timeout.minutes Specifies a duration in minutes. If CPU utilization is below system.idle.cpu.threshold for this duration - notification will be sent to the user and the corresponding run will be marked by the \"IDLE\" label system.idle.action.timeout.minutes Specifies a duration in minutes. If CPU utilization is below system.idle.cpu.threshold for this duration - an action, specified in system.idle.action will be performed system.resource.monitoring.period Specifies period (in seconds) between the users' instances scanning to collect the monitoring metrics system.monitoring.time.range Specifies time delay (in sec) after which the notification \"HIGH_CONSUMED_RESOURCES\" would be sent again, if the problem is still in place system.disk.consume.threshold Specifies disk threshold (in %) above which the notification \"HIGH_CONSUMED_RESOURCES\" would be sent and the corresponding run will be marked by the \"PRESSURE\" label system.idle.cpu.threshold Specifies percentage of the CPU utilization, below which action shall be taken system.resource.monitoring.stats.retention.period Specifies the time period (in days) during which resources utilization data is stored system.memory.consume.threshold Specifies memory threshold (in %) above which the notification \"HIGH_CONSUMED_RESOURCES\" would be sent and the corresponding run will be marked by the \"PRESSURE\" label system.idle.action Sets which action to perform on the instance, that showed low CPU utilization (that is below system.idle.cpu.threshold ): NOTIFY - only send notification PAUSE - pause an instance if possible (e.g. instance is On-Demand, Spot instances are skipped) PAUSE_OR_STOP - pause an instance if it is On-Demand, stop an instance if it is Spot STOP - Stop an instance, disregarding price-type system.external.services.endpoints","title":"System"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#user-interface","text":"Here different user interface settings can be found: Setting name Description ui.pipeline.deployment.name UI deployment name ui.pipe.cli.install.template CLI install templates for different operating systems ui.pipe.drive.mapping ui.project.indicator These attributes define a Project folder ui.pipe.cli.configure.template CLI configure templates for different operating systems ui.support.template Markdown-formatted text that will be displayed in tooltip of the \"support\" info in the main menu. If nothing is specified (empty), support icon in the main menu will not be displayed ui.controls.settings JSON file that contains control settings","title":"User Interface"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#make-system-level-settings-visible-to-all-users","text":"Hover to the Settings tab. Select the Preferences section. Choose one of the tabs with system level settings (e.g. Grid engine autoscaling ). Press the \" Eye \" button near any setting. Now it will be visible to all users by using the API. Note : press \" Eye \" button again to hide it from all users.","title":"Make system-level settings visible to all users"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#update-system-level-settings","text":"Choose any system-level setting and change its value (e.g. change cluster.keep.alive.minutes value from 10 to 15). Press the Save button. Note : before saving you can press the Revert button to return setting's value to the previous state.","title":"Update system-level settings"},{"location":"manual/12_Manage_Settings/12.11._Advanced_features/","text":"12.11. Advanced features via System Preferences User shall have ROLE_ADMIN to configure system-level settings. Setup swap files for the Cloud VMs In certain cases jobs may fail with unexpected errors if the compute node runs Out of memory . Admin users can configure a default swap file to the compute node being created. This allow to avoid runs failures due to memory limits. To configure the size and the location of the swap : Open the Settings pop-up Click the Preference tab Select the Cluster section Click in the field under the cluster.networks.config label Insert the similar json block into a region/cloud specific configuration: Where: swap_ratio - defines a swap file size. It is equal the node RAM multiplied by that ratio. If ratio is 0, a swap file will not be created (default value: 0) swap_location - defines a location of the swap file. If that option is not set - default location will be used (default: AWS will use SSD/gp2 EBS, Azure will use Temporary Storage ) Click the Save button Click the OK button Now, while launch any pipeline, you can see specified swap settings in the run logs: To check that settings were applied, open SSH session and input the swapon command:","title":"12.11. Advanced features"},{"location":"manual/12_Manage_Settings/12.11._Advanced_features/#1211-advanced-features-via-system-preferences","text":"User shall have ROLE_ADMIN to configure system-level settings.","title":"12.11. Advanced features via System Preferences"},{"location":"manual/12_Manage_Settings/12.11._Advanced_features/#setup-swap-files-for-the-cloud-vms","text":"In certain cases jobs may fail with unexpected errors if the compute node runs Out of memory . Admin users can configure a default swap file to the compute node being created. This allow to avoid runs failures due to memory limits. To configure the size and the location of the swap : Open the Settings pop-up Click the Preference tab Select the Cluster section Click in the field under the cluster.networks.config label Insert the similar json block into a region/cloud specific configuration: Where: swap_ratio - defines a swap file size. It is equal the node RAM multiplied by that ratio. If ratio is 0, a swap file will not be created (default value: 0) swap_location - defines a location of the swap file. If that option is not set - default location will be used (default: AWS will use SSD/gp2 EBS, Azure will use Temporary Storage ) Click the Save button Click the OK button Now, while launch any pipeline, you can see specified swap settings in the run logs: To check that settings were applied, open SSH session and input the swapon command:","title":"Setup swap files for the Cloud VMs"},{"location":"manual/12_Manage_Settings/12.12._System_logs/","text":"12.12. System logs User shall have ROLE_ADMIN to view system security logs. Filters Date filter Service filter User filter Message filter Advanced filters Hostname filter Type filter Show service account events The System Logs tab contains the full list of security trail events. Each record in the logs list contains: Field Description Date The date and time of the log event Log status The status of the log message ( INFO , ERROR , etc.) Log message Description of the log event User User name who performed the event Service Service name that registered the event ( api-srv , edge ) Type Log message type (currently, only security type is available) Filters By default, in the list all logs are displayed from new to old. For the more convenient search of the desired logs, there are filters over the logs list: You may combine them in any order for your needs. Date filter To restrict the list of logs for a specific date/time interval - use the From and To controls. For example, to view logs for all events that were today after 21:30 : Click the From control: Click the Select time button Select the desired time (the left column for hours, middle - for minutes, right - for secs): Click the Ok button The logs list will be filtered: Service filter To restrict the list of logs for a specific service - use the Service control. You may select the desired service from the dropdown list, e.g.: The logs list will be filtered automatically: Multi-select is supported. User filter To restrict the list of logs for a specific user(s) - use the User control. You may select the desired user from the dropdown list. Multi-select is supported. Message filter To find the log by its event message (or its part) - use the Message field. Just click this field, specify the desired text and press the Enter key, e.g.: Advanced filters To open advanced filters click the Show advanced button. Additional filters will appear: Hostname filter To restrict the list of logs for a certain service host(s) - use the Hostname control. You may select the desired host from the dropdown list. Multi-select is supported. Type filter To restrict the list of logs for a certain log message type(s) - use the Type control. You may select the desired type from the dropdown list. Note : currently, only security type is available. Show service account events The Include Service Account Events checkbox allows to show/hide log message from the service account (main admin user). Since from this account many messages are received, much more than from other users, by default these messages are hidden.","title":"12.12. System logs"},{"location":"manual/12_Manage_Settings/12.12._System_logs/#1212-system-logs","text":"User shall have ROLE_ADMIN to view system security logs. Filters Date filter Service filter User filter Message filter Advanced filters Hostname filter Type filter Show service account events The System Logs tab contains the full list of security trail events. Each record in the logs list contains: Field Description Date The date and time of the log event Log status The status of the log message ( INFO , ERROR , etc.) Log message Description of the log event User User name who performed the event Service Service name that registered the event ( api-srv , edge ) Type Log message type (currently, only security type is available)","title":"12.12. System logs"},{"location":"manual/12_Manage_Settings/12.12._System_logs/#filters","text":"By default, in the list all logs are displayed from new to old. For the more convenient search of the desired logs, there are filters over the logs list: You may combine them in any order for your needs.","title":"Filters"},{"location":"manual/12_Manage_Settings/12.12._System_logs/#date-filter","text":"To restrict the list of logs for a specific date/time interval - use the From and To controls. For example, to view logs for all events that were today after 21:30 : Click the From control: Click the Select time button Select the desired time (the left column for hours, middle - for minutes, right - for secs): Click the Ok button The logs list will be filtered:","title":"Date filter"},{"location":"manual/12_Manage_Settings/12.12._System_logs/#service-filter","text":"To restrict the list of logs for a specific service - use the Service control. You may select the desired service from the dropdown list, e.g.: The logs list will be filtered automatically: Multi-select is supported.","title":"Service filter"},{"location":"manual/12_Manage_Settings/12.12._System_logs/#user-filter","text":"To restrict the list of logs for a specific user(s) - use the User control. You may select the desired user from the dropdown list. Multi-select is supported.","title":"User filter"},{"location":"manual/12_Manage_Settings/12.12._System_logs/#message-filter","text":"To find the log by its event message (or its part) - use the Message field. Just click this field, specify the desired text and press the Enter key, e.g.:","title":"Message filter"},{"location":"manual/12_Manage_Settings/12.12._System_logs/#advanced-filters","text":"To open advanced filters click the Show advanced button. Additional filters will appear:","title":"Advanced filters"},{"location":"manual/12_Manage_Settings/12.12._System_logs/#hostname-filter","text":"To restrict the list of logs for a certain service host(s) - use the Hostname control. You may select the desired host from the dropdown list. Multi-select is supported.","title":"Hostname filter"},{"location":"manual/12_Manage_Settings/12.12._System_logs/#type-filter","text":"To restrict the list of logs for a certain log message type(s) - use the Type control. You may select the desired type from the dropdown list. Note : currently, only security type is available.","title":"Type filter"},{"location":"manual/12_Manage_Settings/12.12._System_logs/#show-service-account-events","text":"The Include Service Account Events checkbox allows to show/hide log message from the service account (main admin user). Since from this account many messages are received, much more than from other users, by default these messages are hidden.","title":"Show service account events"},{"location":"manual/12_Manage_Settings/12.2._Edit_a_system_event/","text":"12.2. Edit a system event An administrator can edit System events notifications only. Navigate to System events tab . Click the Edit button. Change any field: Title of the notification. Body of the notification. Notification Severity (\" info \", \" warning \" or \" critical \"). Blocking box. Blocking event emerges in the middle of the window and requires confirmation from the user to disappear. Active box. Active notifications will be shown for all users of the Cloud Pipeline until admin sets them inactive. Click Save . To delete a system event: Click the Delete button. Confirm the deletion.","title":"12.2. Edit a system event"},{"location":"manual/12_Manage_Settings/12.2._Edit_a_system_event/#122-edit-a-system-event","text":"An administrator can edit System events notifications only. Navigate to System events tab . Click the Edit button. Change any field: Title of the notification. Body of the notification. Notification Severity (\" info \", \" warning \" or \" critical \"). Blocking box. Blocking event emerges in the middle of the window and requires confirmation from the user to disappear. Active box. Active notifications will be shown for all users of the Cloud Pipeline until admin sets them inactive. Click Save . To delete a system event: Click the Delete button. Confirm the deletion.","title":"12.2. Edit a system event"},{"location":"manual/12_Manage_Settings/12.3._Create_a_new_user/","text":"12.3. Create a new user User shall have ROLE_ADMIN to create a new user. Navigate to User management tab. Click + Create user control. The Create user form will be opened. This form contains the following sections: Name - a new user's name. Default data storage - drop-down list suggested a default data storage to the created user. Assign group or role - drop-down list suggested the existing roles and groups assign. View of roles and groups that are assigned to a new user. Note : the groups and roles, marked as default, will be shown. Enter a name for the new user. Note : there is no restriction to username format, but it is highly recommended to name a user according to your SSO scheme. Select a default data storage if it is necessary. Select desired groups and roles to assign the new user. Click the Create button and the new user will be displayed in the Users tab table.","title":"12.3. Create a new user"},{"location":"manual/12_Manage_Settings/12.3._Create_a_new_user/#123-create-a-new-user","text":"User shall have ROLE_ADMIN to create a new user. Navigate to User management tab. Click + Create user control. The Create user form will be opened. This form contains the following sections: Name - a new user's name. Default data storage - drop-down list suggested a default data storage to the created user. Assign group or role - drop-down list suggested the existing roles and groups assign. View of roles and groups that are assigned to a new user. Note : the groups and roles, marked as default, will be shown. Enter a name for the new user. Note : there is no restriction to username format, but it is highly recommended to name a user according to your SSO scheme. Select a default data storage if it is necessary. Select desired groups and roles to assign the new user. Click the Create button and the new user will be displayed in the Users tab table.","title":"12.3. Create a new user"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/","text":"12.4. Edit/delete a user Edit a user Default data storage Groups (roles) management Attributes Launch options Possibility to revert changes Block/unblock a user Delete a user User shall have ROLE_ADMIN to edit/delete users. Edit a user For edit a user: Open Users tab on User management section of system-level settings. Find a user. Click Edit button in the row opposite the user name: Pop-up window will be shown: On this form there are several blocks of the settings for a user. Default data storage Here you can select default data storage for a user: Groups (roles) management In this block you can set groups and roles for the selected user: For more information about changing a set of the roles/groups for the specific user see 12.8. Change a set of roles/groups for a user . Attributes In this block you can set metadata tags (attributes) for a user. These tags represent key/value pairs, same as pipeline/folder tags. For more information see 17. CP objects tagging by additional attributes . \"Blocking\" notifications track One of the special attribute that is set automatically - information about the notifications confirmation: Via that attribute you can view, which \"blocking\" notifications were confirmed by the user (about system notifications see here ). This attribute is shown only for users that confirmed at least one \"blocking\" notification. By default, this attribute has the following pair: KEY - confirmed_notifications (that name could be changed via the system-level preference system.events.confirmation.metadata.key ) VALUE - link that shows summary count of confirmed notifications for the user To open the detailed table with confirmed notifications for the user: Click the VALUE link: Here you can view detailed information about confirmed notifications - their titles, messages and datetime of the confirmation: Also you can open \"raw\" JSON view of the detailed table, if necessary. For that, click the EDIT button under the detailed table: Here you can edit the contents. Click the SAVE button to save changes: Launch options In this block you can specify some restrictions for a user on allowed instance types and price types. Here you can specify: Field Description Example Allowed instance types mask This mask restrict for a specific user allowed instance types for launching tools, pipelines and configurations If you want user will be able to launch runs with only \"m5...\" instances types, mask would be m5* : In that case, before launching tool, pipeline or configuration, dropdown list of available node types will be look like this: Allowed tool instance types mask This mask restrict for a specific user allowed instance types only for tools - launching from tools menu or main dashboard. This mask has higher priority for launching tool than Allowed instance types mask . It's meaning that in case when both masks are set - for the launching tool will be applied Allowed tool instance types mask . If you want user will be able to launch tools with only some of \"large m5...\" instances types, mask would be m5*.large* : In that case, before launching tool, dropdown list of available node types will be look like this: Allowed price types In this field you may restrict, what price types will be allowed for a user. If you want user will be able to launch only \"On-demand\" runs, select it in the dropdown list: In that case, before launching tool, dropdown list of price types will be look like this: Jobs visibility In this field you may restrict the visibility of running jobs on the Active Runs page for non-owner users. If you want user will be able to view all pipeline runs (for that pipelines on which user has corresponding permissions), select \"Inherit\" in this dropdown list: If you want user can view only runs he launched, select \"Only owner\": To apply set restrictions for a user click Setting restrictions on allowed instance types/price types is a convenient way to minimize a number of invalid configurations runs. Such restrictions could be set not only for a user, but on another levels too. In CP platform next hierarchy is set for applying of inputted allowed instance types (sorted by priority): User level (specified for a user on \"User management\" tab) (see above ) User group level (specified for a group (role) on \"User management\" tab. If a user is a member of several groups - list of allowed instances will be summarized across all the groups) (see 12.6. Edit a group/role ) Tool level (specified for a tool on \"Instance management\" panel) (see 10.5. Launch a Tool ) (global) cluster.allowed.instance.types.docker (specified on \"Cluster\" tab in \"Preferences\" section of system-level settings) (see 12.10. Manage system-level settings ) (global) cluster.allowed.instance.types (specified on \"Cluster\" tab in \"Preferences\" section of system-level settings) (see 12.10. Manage system-level settings ) After specifying allowed instance types, all GUI forms that allow to select the list of instance types (configurations/launch forms) - will display only valid instance type, according to hierarchy above. For price type specifying - if it is set for the user/group/tool - GUI will allow to select only that price type. In CP platform next hierarchy is set for applying of jobs visibility (sorted by priority): User level - highest priority (specified for a user) (see above ) Group level (specified for a group/role) (see 12.6. Edit a group/role ) (global) launch.run.visibility (specified as global defaults via system-level settings) (see 12.10. Manage system-level settings ) Possibility to revert changes In certain cases, there could be convenient to undo all changes in a user profile when modifying it - without closing the form. The admin has such ability: open the User management tab select the desired user to modify, click the Edit button to open the popup with the user's settings edit some settings if needed to revert done changes - click the REVERT button at the bottom of the form ( Note : it's possible only before saving!): all done unsaved changes are reverted. The REVERT button becomes disabled: Note : in such way all unsaved changes of user settings could be reverted - Default data storage , Roles Groups list, Attributes and Launch options . Block/unblock a user To block user: Open Users tab on User management section of system-level settings. Find a user. Click Edit button in the row opposite the user name. In the opened pop-up window click the BLOCK button in the left bottom corner. Confirm the blocking: To unblock user: Open Users tab on User management section of system-level settings. Find a user. Click Edit button in the row opposite the user name. In the opened pop-up window click the UNBLOCK button in the left bottom corner. Confirm the unblocking: Delete a user To delete a user: Open Users tab on User management section of system-level settings. Find a user. Click Edit button in the row opposite the user name. In the opened pop-up window click Delete button in the left bottom corner. Confirm the deletion:","title":"12.4. Edit/delete a user"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#124-editdelete-a-user","text":"Edit a user Default data storage Groups (roles) management Attributes Launch options Possibility to revert changes Block/unblock a user Delete a user User shall have ROLE_ADMIN to edit/delete users.","title":"12.4. Edit/delete a user"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#edit-a-user","text":"For edit a user: Open Users tab on User management section of system-level settings. Find a user. Click Edit button in the row opposite the user name: Pop-up window will be shown: On this form there are several blocks of the settings for a user.","title":"Edit a user"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#default-data-storage","text":"Here you can select default data storage for a user:","title":"Default data storage"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#groups-roles-management","text":"In this block you can set groups and roles for the selected user: For more information about changing a set of the roles/groups for the specific user see 12.8. Change a set of roles/groups for a user .","title":"Groups (roles) management"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#attributes","text":"In this block you can set metadata tags (attributes) for a user. These tags represent key/value pairs, same as pipeline/folder tags. For more information see 17. CP objects tagging by additional attributes .","title":"Attributes"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#blocking-notifications-track","text":"One of the special attribute that is set automatically - information about the notifications confirmation: Via that attribute you can view, which \"blocking\" notifications were confirmed by the user (about system notifications see here ). This attribute is shown only for users that confirmed at least one \"blocking\" notification. By default, this attribute has the following pair: KEY - confirmed_notifications (that name could be changed via the system-level preference system.events.confirmation.metadata.key ) VALUE - link that shows summary count of confirmed notifications for the user To open the detailed table with confirmed notifications for the user: Click the VALUE link: Here you can view detailed information about confirmed notifications - their titles, messages and datetime of the confirmation: Also you can open \"raw\" JSON view of the detailed table, if necessary. For that, click the EDIT button under the detailed table: Here you can edit the contents. Click the SAVE button to save changes:","title":"\"Blocking\" notifications track"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#launch-options","text":"In this block you can specify some restrictions for a user on allowed instance types and price types. Here you can specify: Field Description Example Allowed instance types mask This mask restrict for a specific user allowed instance types for launching tools, pipelines and configurations If you want user will be able to launch runs with only \"m5...\" instances types, mask would be m5* : In that case, before launching tool, pipeline or configuration, dropdown list of available node types will be look like this: Allowed tool instance types mask This mask restrict for a specific user allowed instance types only for tools - launching from tools menu or main dashboard. This mask has higher priority for launching tool than Allowed instance types mask . It's meaning that in case when both masks are set - for the launching tool will be applied Allowed tool instance types mask . If you want user will be able to launch tools with only some of \"large m5...\" instances types, mask would be m5*.large* : In that case, before launching tool, dropdown list of available node types will be look like this: Allowed price types In this field you may restrict, what price types will be allowed for a user. If you want user will be able to launch only \"On-demand\" runs, select it in the dropdown list: In that case, before launching tool, dropdown list of price types will be look like this: Jobs visibility In this field you may restrict the visibility of running jobs on the Active Runs page for non-owner users. If you want user will be able to view all pipeline runs (for that pipelines on which user has corresponding permissions), select \"Inherit\" in this dropdown list: If you want user can view only runs he launched, select \"Only owner\": To apply set restrictions for a user click Setting restrictions on allowed instance types/price types is a convenient way to minimize a number of invalid configurations runs. Such restrictions could be set not only for a user, but on another levels too. In CP platform next hierarchy is set for applying of inputted allowed instance types (sorted by priority): User level (specified for a user on \"User management\" tab) (see above ) User group level (specified for a group (role) on \"User management\" tab. If a user is a member of several groups - list of allowed instances will be summarized across all the groups) (see 12.6. Edit a group/role ) Tool level (specified for a tool on \"Instance management\" panel) (see 10.5. Launch a Tool ) (global) cluster.allowed.instance.types.docker (specified on \"Cluster\" tab in \"Preferences\" section of system-level settings) (see 12.10. Manage system-level settings ) (global) cluster.allowed.instance.types (specified on \"Cluster\" tab in \"Preferences\" section of system-level settings) (see 12.10. Manage system-level settings ) After specifying allowed instance types, all GUI forms that allow to select the list of instance types (configurations/launch forms) - will display only valid instance type, according to hierarchy above. For price type specifying - if it is set for the user/group/tool - GUI will allow to select only that price type. In CP platform next hierarchy is set for applying of jobs visibility (sorted by priority): User level - highest priority (specified for a user) (see above ) Group level (specified for a group/role) (see 12.6. Edit a group/role ) (global) launch.run.visibility (specified as global defaults via system-level settings) (see 12.10. Manage system-level settings )","title":"Launch options"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#possibility-to-revert-changes","text":"In certain cases, there could be convenient to undo all changes in a user profile when modifying it - without closing the form. The admin has such ability: open the User management tab select the desired user to modify, click the Edit button to open the popup with the user's settings edit some settings if needed to revert done changes - click the REVERT button at the bottom of the form ( Note : it's possible only before saving!): all done unsaved changes are reverted. The REVERT button becomes disabled: Note : in such way all unsaved changes of user settings could be reverted - Default data storage , Roles Groups list, Attributes and Launch options .","title":"Possibility to revert changes"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#blockunblock-a-user","text":"To block user: Open Users tab on User management section of system-level settings. Find a user. Click Edit button in the row opposite the user name. In the opened pop-up window click the BLOCK button in the left bottom corner. Confirm the blocking: To unblock user: Open Users tab on User management section of system-level settings. Find a user. Click Edit button in the row opposite the user name. In the opened pop-up window click the UNBLOCK button in the left bottom corner. Confirm the unblocking:","title":"Block/unblock a user"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#delete-a-user","text":"To delete a user: Open Users tab on User management section of system-level settings. Find a user. Click Edit button in the row opposite the user name. In the opened pop-up window click Delete button in the left bottom corner. Confirm the deletion:","title":"Delete a user"},{"location":"manual/12_Manage_Settings/12.5._Create_a_group/","text":"12.5. Create a group User shall have ROLE_ADMIN to create a new group. Navigate to User management tab. Click Groups tab. Click + Create group button. Enter a name for the new group (e.g. NEW_GROUP ). If you want to grant the group and its permissions to all new users mark the group as Default . Select a default data storage for the created group if it is necessary. Click the Create button.","title":"12.5. Create a group"},{"location":"manual/12_Manage_Settings/12.5._Create_a_group/#125-create-a-group","text":"User shall have ROLE_ADMIN to create a new group. Navigate to User management tab. Click Groups tab. Click + Create group button. Enter a name for the new group (e.g. NEW_GROUP ). If you want to grant the group and its permissions to all new users mark the group as Default . Select a default data storage for the created group if it is necessary. Click the Create button.","title":"12.5. Create a group"},{"location":"manual/12_Manage_Settings/12.6._Edit_a_group_role/","text":"12.6. Edit a group (role) Edit a group (role) Default data storage User management Attributes Launch options Possibility to revert changes Block/unblock a group User shall have ROLE_ADMIN to edit groups/roles. Edit a group (role) For edit a group/role: Open Groups/Roles tab on User management section of the system-level settings. Find a group (role). Click Edit button in the row opposite the user name: Pop-up window will be shown: On this form there are several blocks of the settings for a group/role. Default data storage Here you can select default data storage for a group/role: User management In this block you can change a member list of the selected group/role: For more information see 12.8. Change a set of roles/groups for a user . Attributes In this block you can set metadata tags (attributes) for a group. These tags represent key/value pairs, same as pipeline/folder tags. For more information see 17. CP objects tagging by additional attributes . Launch options In this block you can specify some restrictions for a group of users/role on allowed instance types and price types. Here you can specify: Field Description Example Allowed instance types mask This mask restrict for a specific group/role allowed instance types for launching tools, pipelines and configurations If you want members of a certain group/role will be able to launch runs with only \"m5...\" instances types, mask would be m5* : In that case, before launching tool, pipeline or configuration, dropdown list of available node types will be look like this: Allowed tool instance types mask This mask restrict for a specific group/role allowed instance types only for tools - launching from tools menu or main dashboard. This mask has higher priority for launching tool than Allowed instance types mask . It's meaning that in case when both masks are set - for the launching tool will be applied Allowed tool instance types mask . If you want members of a certain group/role will be able to launch tools with only some of \"large\" \"m5...\" instances types, mask would be m5*.large* : In that case, before launching tool, dropdown list of available node types will be look like this: Allowed price types In this field you may restrict, what price types will be allowed for a group/role. If you want members of a certain group/role will be able to launch \"On-demand\" runs only, select it in the dropdown list: In that case, before launching tool, dropdown list of price types will be look like this: Jobs visibility In this field you may restrict the visibility of running jobs on the Active Runs page for non-owner users. If you want users from that group (role) will be able to view all pipeline runs (for that pipelines on which users have corresponding permissions), select \"Inherit\" in this dropdown list: If you want users from that group (role) can view only runs they launched, select \"Only owner\": To apply set restrictions for a group/role click button. Setting restrictions on allowed instance types/price types is a convenient way to minimize a number of invalid configurations runs. Such restrictions could be set not only for a group/role, but on another levels too. In CP platform next hierarchy is set for applying of inputted allowed instance types (sorted by priority): User level (specified for a user on \"User management\" tab) (see 12.4. Edit/delete a user ) User group level (specified for a group (role) on \"User management\" tab. If a user is a member of several groups - list of allowed instances will be summarized across all the groups) (see above ) Tool level (specified for a tool on \"Instance management\" panel) (see 10.5. Launch a Tool ) (global) cluster.allowed.instance.types.docker (specified on \"Cluster\" tab in \"Preferences\" section of system-level settings) (see 12.10. Manage system-level settings ) (global) cluster.allowed.instance.types (specified on \"Cluster\" tab in \"Preferences\" section of system-level settings) (see 12.10. Manage system-level settings ) After specifying allowed instance types, all GUI forms that allow to select the list of instance types (configurations/launch forms) - will display only valid instance type, according to hierarchy above. For price type specifying - if it is set for the user/group/tool - GUI will allow to select only that price type. In CP platform next hierarchy is set for applying of jobs visibility (sorted by priority): User level - highest priority (specified for a user) (see 12.4. Edit/delete a user ) Group level (specified for a group/role) (see above ) (global) launch.run.visibility (specified as global defaults via system-level settings) (see 12.10. Manage system-level settings ) Possibility to revert changes In certain cases, there could be convenient to undo all changes in a group/role profile when modifying it - without closing the form. The admin has such ability: open the User management tab and then the Groups / Roles tab select the desired group to modify, click the Edit button to open the popup with the group's settings edit some settings if needed to revert done changes - click the REVERT button at the bottom of the form ( Note : it's possible only before saving!): all done unsaved changes are reverted. The REVERT button becomes disabled: Note : in such way all unsaved changes of user settings could be reverted - Default data storage , Users list, Attributes and Launch options . Block/unblock a group To block a group: Open the Groups tab on the User management section of the system-level settings. Click the Edit button next to the group's name. Note : system groups are created by the SSO authentication system automatically and can not be found here. Pop-up window will be displayed: Click the BLOCK button Confirm the blocking: To unblock a group: Open the Groups tab on the User management section of the system-level settings. Click the Edit button next to the group's name. Pop-up window will be shown: Click the UNBLOCK button Confirm the unblocking:","title":"12.6. Edit a group/role"},{"location":"manual/12_Manage_Settings/12.6._Edit_a_group_role/#126-edit-a-group-role","text":"Edit a group (role) Default data storage User management Attributes Launch options Possibility to revert changes Block/unblock a group User shall have ROLE_ADMIN to edit groups/roles.","title":"12.6. Edit a group (role)"},{"location":"manual/12_Manage_Settings/12.6._Edit_a_group_role/#edit-a-group-role","text":"For edit a group/role: Open Groups/Roles tab on User management section of the system-level settings. Find a group (role). Click Edit button in the row opposite the user name: Pop-up window will be shown: On this form there are several blocks of the settings for a group/role.","title":"Edit a group (role)"},{"location":"manual/12_Manage_Settings/12.6._Edit_a_group_role/#default-data-storage","text":"Here you can select default data storage for a group/role:","title":"Default data storage"},{"location":"manual/12_Manage_Settings/12.6._Edit_a_group_role/#user-management","text":"In this block you can change a member list of the selected group/role: For more information see 12.8. Change a set of roles/groups for a user .","title":"User management"},{"location":"manual/12_Manage_Settings/12.6._Edit_a_group_role/#attributes","text":"In this block you can set metadata tags (attributes) for a group. These tags represent key/value pairs, same as pipeline/folder tags. For more information see 17. CP objects tagging by additional attributes .","title":"Attributes"},{"location":"manual/12_Manage_Settings/12.6._Edit_a_group_role/#launch-options","text":"In this block you can specify some restrictions for a group of users/role on allowed instance types and price types. Here you can specify: Field Description Example Allowed instance types mask This mask restrict for a specific group/role allowed instance types for launching tools, pipelines and configurations If you want members of a certain group/role will be able to launch runs with only \"m5...\" instances types, mask would be m5* : In that case, before launching tool, pipeline or configuration, dropdown list of available node types will be look like this: Allowed tool instance types mask This mask restrict for a specific group/role allowed instance types only for tools - launching from tools menu or main dashboard. This mask has higher priority for launching tool than Allowed instance types mask . It's meaning that in case when both masks are set - for the launching tool will be applied Allowed tool instance types mask . If you want members of a certain group/role will be able to launch tools with only some of \"large\" \"m5...\" instances types, mask would be m5*.large* : In that case, before launching tool, dropdown list of available node types will be look like this: Allowed price types In this field you may restrict, what price types will be allowed for a group/role. If you want members of a certain group/role will be able to launch \"On-demand\" runs only, select it in the dropdown list: In that case, before launching tool, dropdown list of price types will be look like this: Jobs visibility In this field you may restrict the visibility of running jobs on the Active Runs page for non-owner users. If you want users from that group (role) will be able to view all pipeline runs (for that pipelines on which users have corresponding permissions), select \"Inherit\" in this dropdown list: If you want users from that group (role) can view only runs they launched, select \"Only owner\": To apply set restrictions for a group/role click button. Setting restrictions on allowed instance types/price types is a convenient way to minimize a number of invalid configurations runs. Such restrictions could be set not only for a group/role, but on another levels too. In CP platform next hierarchy is set for applying of inputted allowed instance types (sorted by priority): User level (specified for a user on \"User management\" tab) (see 12.4. Edit/delete a user ) User group level (specified for a group (role) on \"User management\" tab. If a user is a member of several groups - list of allowed instances will be summarized across all the groups) (see above ) Tool level (specified for a tool on \"Instance management\" panel) (see 10.5. Launch a Tool ) (global) cluster.allowed.instance.types.docker (specified on \"Cluster\" tab in \"Preferences\" section of system-level settings) (see 12.10. Manage system-level settings ) (global) cluster.allowed.instance.types (specified on \"Cluster\" tab in \"Preferences\" section of system-level settings) (see 12.10. Manage system-level settings ) After specifying allowed instance types, all GUI forms that allow to select the list of instance types (configurations/launch forms) - will display only valid instance type, according to hierarchy above. For price type specifying - if it is set for the user/group/tool - GUI will allow to select only that price type. In CP platform next hierarchy is set for applying of jobs visibility (sorted by priority): User level - highest priority (specified for a user) (see 12.4. Edit/delete a user ) Group level (specified for a group/role) (see above ) (global) launch.run.visibility (specified as global defaults via system-level settings) (see 12.10. Manage system-level settings )","title":"Launch options"},{"location":"manual/12_Manage_Settings/12.6._Edit_a_group_role/#possibility-to-revert-changes","text":"In certain cases, there could be convenient to undo all changes in a group/role profile when modifying it - without closing the form. The admin has such ability: open the User management tab and then the Groups / Roles tab select the desired group to modify, click the Edit button to open the popup with the group's settings edit some settings if needed to revert done changes - click the REVERT button at the bottom of the form ( Note : it's possible only before saving!): all done unsaved changes are reverted. The REVERT button becomes disabled: Note : in such way all unsaved changes of user settings could be reverted - Default data storage , Users list, Attributes and Launch options .","title":"Possibility to revert changes"},{"location":"manual/12_Manage_Settings/12.6._Edit_a_group_role/#blockunblock-a-group","text":"To block a group: Open the Groups tab on the User management section of the system-level settings. Click the Edit button next to the group's name. Note : system groups are created by the SSO authentication system automatically and can not be found here. Pop-up window will be displayed: Click the BLOCK button Confirm the blocking: To unblock a group: Open the Groups tab on the User management section of the system-level settings. Click the Edit button next to the group's name. Pop-up window will be shown: Click the UNBLOCK button Confirm the unblocking:","title":"Block/unblock a group"},{"location":"manual/12_Manage_Settings/12.7._Delete_a_group/","text":"12.7. Delete a group User shall have ROLE_ADMIN to delete a group. Navigate to User management tab. Move to Groups tab. Click Delete button next to the group's name. Note : system groups are created by the SSO authentication system automatically and can not be found/deleted here. Confirm the deletion:","title":"12.7. Delete a group"},{"location":"manual/12_Manage_Settings/12.7._Delete_a_group/#127-delete-a-group","text":"User shall have ROLE_ADMIN to delete a group. Navigate to User management tab. Move to Groups tab. Click Delete button next to the group's name. Note : system groups are created by the SSO authentication system automatically and can not be found/deleted here. Confirm the deletion:","title":"12.7. Delete a group"},{"location":"manual/12_Manage_Settings/12.8._Change_a_set_of_roles_groups_for_a_user/","text":"12.8. Change a set of roles/groups for a user User shall have ROLE_ADMIN to change groups(roles) for a user. There are two ways to set a role or group to a user: Change a set of roles and groups to a selected user from the Users tab . Change a member list for a selected role or group . Note : the scenarios below shows a process using Roles as an example. Setting groups for a user happens in the same manner. Change a set of roles and groups to a selected user Navigate to User management tab. Make sure that you are in the Users tab area. Find user on the list (you can use Search field - see the picture below, 1 ). Click the Edit button (see the picture below, 2 ). The editing form is open. To assign a role or group to the user, click on \"Add role or group\" field and select the desired item from the drop-down list. When the desired item is selected, the + Add control will be enabled. To delete roles or groups, use the Delete button. Click OK and all changes will be saved and displayed in the Users tab table. Change a member list for a selected role or group Navigate to the User management tab. Move to the Roles tab. Click the Edit button next to Role's name. You'll see a list of users assigned to the role. Look for the desired user via Search field. When the user is selected, the +Add user control will be enabled. Click the +Add user control to add a new user to the role member list. To delete a user from the role member list, click the Delete button next to a user name. Click OK and all changes will be saved and displayed in the Users tab table.","title":"12.8. Change a set of roles/groups for a user"},{"location":"manual/12_Manage_Settings/12.8._Change_a_set_of_roles_groups_for_a_user/#128-change-a-set-of-rolesgroups-for-a-user","text":"User shall have ROLE_ADMIN to change groups(roles) for a user. There are two ways to set a role or group to a user: Change a set of roles and groups to a selected user from the Users tab . Change a member list for a selected role or group . Note : the scenarios below shows a process using Roles as an example. Setting groups for a user happens in the same manner.","title":"12.8. Change a set of roles/groups for a user"},{"location":"manual/12_Manage_Settings/12.8._Change_a_set_of_roles_groups_for_a_user/#change-a-set-of-roles-and-groups-to-a-selected-user","text":"Navigate to User management tab. Make sure that you are in the Users tab area. Find user on the list (you can use Search field - see the picture below, 1 ). Click the Edit button (see the picture below, 2 ). The editing form is open. To assign a role or group to the user, click on \"Add role or group\" field and select the desired item from the drop-down list. When the desired item is selected, the + Add control will be enabled. To delete roles or groups, use the Delete button. Click OK and all changes will be saved and displayed in the Users tab table.","title":"Change a set of roles and groups to a selected user"},{"location":"manual/12_Manage_Settings/12.8._Change_a_set_of_roles_groups_for_a_user/#change-a-member-list-for-a-selected-role-or-group","text":"Navigate to the User management tab. Move to the Roles tab. Click the Edit button next to Role's name. You'll see a list of users assigned to the role. Look for the desired user via Search field. When the user is selected, the +Add user control will be enabled. Click the +Add user control to add a new user to the role member list. To delete a user from the role member list, click the Delete button next to a user name. Click OK and all changes will be saved and displayed in the Users tab table.","title":"Change a member list for a selected role or group"},{"location":"manual/12_Manage_Settings/12.9._Change_email_notification/","text":"12.9. Change email notification User shall have ROLE_ADMIN to manage Email notifications. Navigate to the Settings tab. Select the Email notifications section. Choose any of the email notification types (e.g. LONG_INIT ) on the left: Remove Keep admins informed and Keep owners informed options. This email notification type will no longer inform admins and owners. Add a new user to the Informed users . While typing system will suggest you users. When you selected all users, click outside this field. Change the Threshold parameter to e.g. 1400. Press the Save button to save all changes to the LONG_INIT email notification template.","title":"12.9. Change email notification"},{"location":"manual/12_Manage_Settings/12.9._Change_email_notification/#129-change-email-notification","text":"User shall have ROLE_ADMIN to manage Email notifications. Navigate to the Settings tab. Select the Email notifications section. Choose any of the email notification types (e.g. LONG_INIT ) on the left: Remove Keep admins informed and Keep owners informed options. This email notification type will no longer inform admins and owners. Add a new user to the Informed users . While typing system will suggest you users. When you selected all users, click outside this field. Change the Threshold parameter to e.g. 1400. Press the Save button to save all changes to the LONG_INIT email notification template.","title":"12.9. Change email notification"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/","text":"12. Manage Settings System Settings consist of CLI , System events , User management , Email notifications , Preferences , Cloud Regions and System logs tabs. To open System Settings click the gear icon on the main menu in the left side of the Cloud Pipeline application: CLI tab Pipe CLI Git CLI System events User management Users Groups Roles Email notifications Preferences Cloud Regions System logs CLI tab \" CLI \" tab generates two types of CLI installation and configuration commands to set CLI for the Cloud Pipeline - Pipe CLI and Git CLI . You can select each of them by click the corresponding option in the \" CLI \" tab menu. Pipe CLI Control Description Operation system Choose an operation system from drop-down list and the instruction how to install the Cloud Pipeline CLI will appear in the window below. Generate access key Generates access token to be used by CLI. Valid till A date access key expires. For more details see 14.1. Install and setup CLI . Git CLI Here you can see instructions how to configure your Git client to work with the Cloud Pipeline. System events This tab is visible only for administrator. System events tab represents system events notifications. Here you can create, edit, delete system events notifications. System events notifications are organized into a table. It represents the body of the notification , its severity status (\" info \", \" warning \" or \" critical \") and date of creation , activity status . Note : Variants of activity status: Blocking event emerges in the middle of the window and requires confirmation from the user to disappear. Active notifications will be shown for all users of the Cloud Pipeline until admin sets them inactive. Administrator can edit and delete notifications via corresponding buttons. System events controls Controls are at the top right of the table. Control Description Expand/Collapse This button ( 1 ) shows/hides the body of the event. Refresh To refresh a list of notifications press this control ( 2 ). + ADD This control ( 3 ) allows to create new notification. Edit The control ( 4 ) opens the edit form of the event. Delete To delete an event click the control ( 5 ). User management This tab is visible only for administrator. The User management tab helps to manage user groups and system roles. To grant or refuse permissions to a specific group of users (e.g. project team members), you can just create a user group and grant or refuse permissions to the specific set of objects to the whole group. System roles is one of the principal tool for managing security access to the objects. Even if you have WRITE permission for a folder object, you might be not able to create a pipeline there, if you don't have the ROLE_PIPELINE_MANAGER role. Note : About permissions, you can read more here . User management consists of the 3 following tabs: Users , Groups , Roles . Users This table view displays a list of users and their additional information: Name - an authenticated domain account (SAML/OAuth/OpenID), e.g. e-mail. Groups - a set of groups assigned to a user. It could be whether CP's user's groups and groups, given to each user automatically by SSO authentication system . Note : automatically created groups based on SSO authentication system are light-grey colored. Roles - a set of system roles assigned to a user. Users tab controls Control Description Search field To search particular user from a list of users, start to enter the user's name (see the picture below, 1 ). + Create user This control (see the picture below, 2 ) opens a \"Create user\" form, which can be used to create a new user. Export users Allows export all users with selected set of attributes (see the picture below, 3 ). Edit Allows changing a list of roles or groups assigned to a user (see the picture below, 4 ). Export users The Export users button allows administrator to export user list in .csv format. There are 2 export options: Default configuration In this case, the file with all users and full list of their properties ( ID , username , attributes (first and last name, email), registration date , first login date , list of groups , list of roles , blocked / unblocked state, default data storage ) will be downloaded to the local workstation. To export full user list with default configuration: click the Export users button in the right upper corner of the Users tab in the User management dashboard or hover the v button next to the Export users button and click the Default configuration item in the appeared drop-down menu Custom configuration Custom configuration allows admins to select which user properties should be downloaded: hover the v button next to the Export users button and click the Custom configuration item in the appeared drop-down menu the modal window with the list of available user properties to export will appear, e.g.: User can select any set of attributes by marking/unmarking corresponding checkboxes. At least 1 checkbox should be marked to export user list. Note : enabled Header checkbox adds the headers row into the exporting file to download the result file with full user list and custom set of their properties click the Download CSV button. Groups The \"Groups\" tab shows a set of user groups created in CP. Here you can grant or refuse users in a group membership. Note that this tab displays groups created in CP only, not given by SSO authentication system . Groups tab controls Control Description Search field To search particular group from a list of groups, start to enter the group name (see the picture above, 1 ). + Create group Create a new group (see the picture above, 2 ). Edit This control (see the picture above, 3 ) allows changing a list of users owning this group. Delete Delete a group (see the picture above, 4 ). Roles The \" Roles \" tab shows a set of predefined system roles that couldn't be extended or reduced. Here you can grant or refuse users in a role. There is a list of CP system roles: Role Description ROLE_ADMIN The user gets Read/Write/Execute/Owner permissions to all objects in the system. Note : The owner of the object can manage its Access Control List. OWNER property is assigned to a user has created an object by default. ROLE_USER basic user. ROLE_ANONYMOUS_USER specific role for the ability of sharing interactive runs endpoints to the anonymous users (users, who have successfully passed the IdP authentication but is not registered in the Cloud Pipeline). ROLE_PIPELINE_MANAGER allows to create/delete Pipelines (given to each user by default). ROLE_FOLDER_MANAGER allows to create/delete Folders (given to each user by default). ROLE_CONFIGURATION_MANAGER allows to create/delete Cluster Configurations (given to each user by default). ROLE_STORAGE_MANAGER allows to create/delete Data Storages . ROLE_TOOL_GROUP_MANAGER allows to create/delete Tool groups . ROLE_ENTITIES_MANAGER allows to create/delete Entities . Set of user's roles combined with permission settings defines allowed actions for the user and therefore the layout of GUI buttons. A user sees GUI options in appliance with his rights. Note : roles 3-8 are checked if a user has WRITE permission for the parent object. Roles tab controls Control Description Search field To search particular group from a list of roles, start to enter the role name (see the picture above, 1 ). Edit Allows changing a list of users assigned the role (see the picture above, 2 ). Email notifications This tab is visible only for administrator. The email notifications helps to keep track of what's happening in the Cloud Pipeline. On the left you can see a list of the email notification templates. Email notifications tab controls Control Descriptions Enabled checkbox If set, email distribution of the selected type will be enabled. Keep admins informed checkbox If set, all emails with such type will be sent to all users with ROLE_ADMIN role. Keep owners informed checkbox If set, all emails with such type will be sent to the OWNERS of the corresponding Cloud Pipeline objects. Informed users text field Select users that will get such email types. Threshold text field Amount of seconds that is required for the process to generate email. Resend delay text field Amount of seconds that is required for the process to generate a repeat email notification on that subject. Subject text field Email notification subject. Body text field Body of the email notification. Revert button Return an email settings to the previous unsaved state. Save button Saves current email notification settings. Also you can switch from the Edit to the Preview mode to see how the Subject and the Body of the email notification will actually look: Note : this is the current list of notification templates. It might be extended in the future. Notification type Description HIGH_CONSUMED_RESOURCES tells that memory or disk consuming is higher than a threshold value for a specific period of time IDLE_RUN tells that the job is idle for a long time IDLE_RUN_PAUSED tells that the job was paused because it was idle for a long time IDLE_RUN_STOPPED tells that the job was stopped because it was idle for a long time LONG_INIT tells that the job is initializing for a long time LONG_RUNNING tells that the job is running for a long time NEW_ISSUE notifies about new issue NEW_ISSUE_COMMENT tells that an issue was commented PIPELINE_RUN_STATUS email about current pipeline status Preferences This tab is visible only for administrator. The Preferences tab contains different global settings for the Cloud Pipeline. These settings determine default behavior of the Cloud Pipeline. On the left you can see a set of sections. Each section contains a list of global settings. See more information here . Cloud Regions This tab is visible only for administrator. The Cloud Regions tab contains different settings for the specific Cloud Regions. You could manage regions, add or remove them from the Cloud Pipeline. System logs This tab is visible only for administrator. The System logs tab contains the following audit trail events: users' authentication attempts users' profiles modifications platform objects' permissions management access to interactive applications from pipeline runs other platform functionality features For more details see here .","title":"12.0. Overview"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#12-manage-settings","text":"System Settings consist of CLI , System events , User management , Email notifications , Preferences , Cloud Regions and System logs tabs. To open System Settings click the gear icon on the main menu in the left side of the Cloud Pipeline application: CLI tab Pipe CLI Git CLI System events User management Users Groups Roles Email notifications Preferences Cloud Regions System logs","title":"12. Manage Settings"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#cli-tab","text":"\" CLI \" tab generates two types of CLI installation and configuration commands to set CLI for the Cloud Pipeline - Pipe CLI and Git CLI . You can select each of them by click the corresponding option in the \" CLI \" tab menu.","title":"CLI tab"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#pipe-cli","text":"Control Description Operation system Choose an operation system from drop-down list and the instruction how to install the Cloud Pipeline CLI will appear in the window below. Generate access key Generates access token to be used by CLI. Valid till A date access key expires. For more details see 14.1. Install and setup CLI .","title":"Pipe CLI"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#git-cli","text":"Here you can see instructions how to configure your Git client to work with the Cloud Pipeline.","title":"Git CLI"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#system-events","text":"This tab is visible only for administrator. System events tab represents system events notifications. Here you can create, edit, delete system events notifications. System events notifications are organized into a table. It represents the body of the notification , its severity status (\" info \", \" warning \" or \" critical \") and date of creation , activity status . Note : Variants of activity status: Blocking event emerges in the middle of the window and requires confirmation from the user to disappear. Active notifications will be shown for all users of the Cloud Pipeline until admin sets them inactive. Administrator can edit and delete notifications via corresponding buttons.","title":"System events"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#system-events-controls","text":"Controls are at the top right of the table. Control Description Expand/Collapse This button ( 1 ) shows/hides the body of the event. Refresh To refresh a list of notifications press this control ( 2 ). + ADD This control ( 3 ) allows to create new notification. Edit The control ( 4 ) opens the edit form of the event. Delete To delete an event click the control ( 5 ).","title":"System events controls"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#user-management","text":"This tab is visible only for administrator. The User management tab helps to manage user groups and system roles. To grant or refuse permissions to a specific group of users (e.g. project team members), you can just create a user group and grant or refuse permissions to the specific set of objects to the whole group. System roles is one of the principal tool for managing security access to the objects. Even if you have WRITE permission for a folder object, you might be not able to create a pipeline there, if you don't have the ROLE_PIPELINE_MANAGER role. Note : About permissions, you can read more here . User management consists of the 3 following tabs: Users , Groups , Roles .","title":"User management"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#users","text":"This table view displays a list of users and their additional information: Name - an authenticated domain account (SAML/OAuth/OpenID), e.g. e-mail. Groups - a set of groups assigned to a user. It could be whether CP's user's groups and groups, given to each user automatically by SSO authentication system . Note : automatically created groups based on SSO authentication system are light-grey colored. Roles - a set of system roles assigned to a user.","title":"Users"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#users-tab-controls","text":"Control Description Search field To search particular user from a list of users, start to enter the user's name (see the picture below, 1 ). + Create user This control (see the picture below, 2 ) opens a \"Create user\" form, which can be used to create a new user. Export users Allows export all users with selected set of attributes (see the picture below, 3 ). Edit Allows changing a list of roles or groups assigned to a user (see the picture below, 4 ).","title":"Users tab controls"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#export-users","text":"The Export users button allows administrator to export user list in .csv format. There are 2 export options: Default configuration In this case, the file with all users and full list of their properties ( ID , username , attributes (first and last name, email), registration date , first login date , list of groups , list of roles , blocked / unblocked state, default data storage ) will be downloaded to the local workstation. To export full user list with default configuration: click the Export users button in the right upper corner of the Users tab in the User management dashboard or hover the v button next to the Export users button and click the Default configuration item in the appeared drop-down menu Custom configuration Custom configuration allows admins to select which user properties should be downloaded: hover the v button next to the Export users button and click the Custom configuration item in the appeared drop-down menu the modal window with the list of available user properties to export will appear, e.g.: User can select any set of attributes by marking/unmarking corresponding checkboxes. At least 1 checkbox should be marked to export user list. Note : enabled Header checkbox adds the headers row into the exporting file to download the result file with full user list and custom set of their properties click the Download CSV button.","title":"Export users"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#groups","text":"The \"Groups\" tab shows a set of user groups created in CP. Here you can grant or refuse users in a group membership. Note that this tab displays groups created in CP only, not given by SSO authentication system .","title":"Groups"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#groups-tab-controls","text":"Control Description Search field To search particular group from a list of groups, start to enter the group name (see the picture above, 1 ). + Create group Create a new group (see the picture above, 2 ). Edit This control (see the picture above, 3 ) allows changing a list of users owning this group. Delete Delete a group (see the picture above, 4 ).","title":"Groups tab controls"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#roles","text":"The \" Roles \" tab shows a set of predefined system roles that couldn't be extended or reduced. Here you can grant or refuse users in a role. There is a list of CP system roles: Role Description ROLE_ADMIN The user gets Read/Write/Execute/Owner permissions to all objects in the system. Note : The owner of the object can manage its Access Control List. OWNER property is assigned to a user has created an object by default. ROLE_USER basic user. ROLE_ANONYMOUS_USER specific role for the ability of sharing interactive runs endpoints to the anonymous users (users, who have successfully passed the IdP authentication but is not registered in the Cloud Pipeline). ROLE_PIPELINE_MANAGER allows to create/delete Pipelines (given to each user by default). ROLE_FOLDER_MANAGER allows to create/delete Folders (given to each user by default). ROLE_CONFIGURATION_MANAGER allows to create/delete Cluster Configurations (given to each user by default). ROLE_STORAGE_MANAGER allows to create/delete Data Storages . ROLE_TOOL_GROUP_MANAGER allows to create/delete Tool groups . ROLE_ENTITIES_MANAGER allows to create/delete Entities . Set of user's roles combined with permission settings defines allowed actions for the user and therefore the layout of GUI buttons. A user sees GUI options in appliance with his rights. Note : roles 3-8 are checked if a user has WRITE permission for the parent object.","title":"Roles"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#roles-tab-controls","text":"Control Description Search field To search particular group from a list of roles, start to enter the role name (see the picture above, 1 ). Edit Allows changing a list of users assigned the role (see the picture above, 2 ).","title":"Roles tab controls"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#email-notifications","text":"This tab is visible only for administrator. The email notifications helps to keep track of what's happening in the Cloud Pipeline. On the left you can see a list of the email notification templates.","title":"Email notifications"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#email-notifications-tab-controls","text":"Control Descriptions Enabled checkbox If set, email distribution of the selected type will be enabled. Keep admins informed checkbox If set, all emails with such type will be sent to all users with ROLE_ADMIN role. Keep owners informed checkbox If set, all emails with such type will be sent to the OWNERS of the corresponding Cloud Pipeline objects. Informed users text field Select users that will get such email types. Threshold text field Amount of seconds that is required for the process to generate email. Resend delay text field Amount of seconds that is required for the process to generate a repeat email notification on that subject. Subject text field Email notification subject. Body text field Body of the email notification. Revert button Return an email settings to the previous unsaved state. Save button Saves current email notification settings. Also you can switch from the Edit to the Preview mode to see how the Subject and the Body of the email notification will actually look: Note : this is the current list of notification templates. It might be extended in the future. Notification type Description HIGH_CONSUMED_RESOURCES tells that memory or disk consuming is higher than a threshold value for a specific period of time IDLE_RUN tells that the job is idle for a long time IDLE_RUN_PAUSED tells that the job was paused because it was idle for a long time IDLE_RUN_STOPPED tells that the job was stopped because it was idle for a long time LONG_INIT tells that the job is initializing for a long time LONG_RUNNING tells that the job is running for a long time NEW_ISSUE notifies about new issue NEW_ISSUE_COMMENT tells that an issue was commented PIPELINE_RUN_STATUS email about current pipeline status","title":"Email notifications tab controls"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#preferences","text":"This tab is visible only for administrator. The Preferences tab contains different global settings for the Cloud Pipeline. These settings determine default behavior of the Cloud Pipeline. On the left you can see a set of sections. Each section contains a list of global settings. See more information here .","title":"Preferences"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#cloud-regions","text":"This tab is visible only for administrator. The Cloud Regions tab contains different settings for the specific Cloud Regions. You could manage regions, add or remove them from the Cloud Pipeline.","title":"Cloud Regions"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#system-logs","text":"This tab is visible only for administrator. The System logs tab contains the following audit trail events: users' authentication attempts users' profiles modifications platform objects' permissions management access to interactive applications from pipeline runs other platform functionality features For more details see here .","title":"System logs"},{"location":"manual/13_Permissions/13._Permissions/","text":"13. Permissions Overview Owner property How to change an owner Admin role Permission settings Overview Security Policies Scheme is organized by 2 principal tools: groups (user groups and system roles) and Access Control List defined for each CP's object. Note : About groups and system roles you can read more here . Object's Access Control List specifies who can work with the object and what he can do with it. It is defined as a pair of attributes: a User or User Group ID Permissions The permission settings are divided into the following options which can be combined for the object: Read Write Execute Below is a mapping of the objects' possible actions to permissions which demonstrates what actions will be allowed or denied to a user or user group. Note : according to Security Policies Scheme WRITE permission is not enough to add/delete any Cloud Pipeline object. A specific *_MANAGER role is required also (about roles see here ). Object User Action Permission Folder View folder Read List folder contents Create object (e.g folder, pipeline, etc.) Write Delete folder Rename folder Change parent Upload metadata Pipeline *Permissions for a pipeline version are inherited from the pipeline View pipeline Read List pipeline attributes Delete a pipeline Write Edit pipeline attributes Change parent Run a pipeline Execute DataStorage *Permissions for files in data storage are inherited from the data storage View datastorage Read List datastorage contents Delete a datastorage Write Edit datastorage attributes/contents Change parent Pipeline run View runs Inherited from a run pipeline View run logs Launch a pipeline Stop a run Rerun Tool run View runs Admin and Owner only View run logs Launch a pipeline Stop a run Rerun Cluster node View a cluster Inherited from a currently assigned run View node details Terminate a node Docker Registry View registry Read Add registry Write Delete registry Edit registry attributes Run a child tool Execute Tool group View tool group Read Create tool group Write Edit tool group Delete tool group Run a child tool Execute Tool View enabled tool Read View disabled tools list Edit tool attributes Write Run tool without a pipeline Execute Instance management Admin and Owner only Run configuration View run configuration Read Delete a run configuration Write Edit run configuration attributes Change parent Run a run configuration Execute Note : also you can manage permissions on the Cloud Pipeline objects via CLI. See 14.7. View and manage Permissions via CLI . Owner property Each object has an additional \"Owner\" property. The owner of the object can manage its Access Control List. Owner property is assigned to a user that created an object. How to change an owner The Owner of an object can be changed easily for: Folders; Pipelines and pipelines versions; Data storages; Run configurations; Docker registries, Tool Groups and Tools. Note : you shall have Owner or Admin role. To change an owner of an object: Select an object. Click \"Gear\" icon in the top-right corner of the screen. Navigate to Permissions tab. Note : To edit permissions: for a Folder - click the \"Gear\" icon \u2192 Edit folder for a Docker registry - click the \"Gear\" icon \u2192 Registry \u2192 Edit . for a Tool group - click the \"Gear\" icon \u2192 Group \u2192 Edit for a Tool - click the \"Gear\" icon \u2192 Permissions . Click owner's name. Now you can edit it: Start to enter a desired username and system will suggest you the existing users. Click the desired username. Click \"Apply\" control and the changes will be saved. Also you can change an owner of an object via pipe CLI - see here . Admin role Admin property can be given by assigning ROLE_ADMIN to a user (about roles see here ). The user gets Read/Write/Execute/Owner permissions to all objects in the system. Initially, a user with ROLE_ADMIN shall be an authenticated domain account (SAML/OAuth/OpenID) defined during a system deployment (in some properties file or database). Permission settings The permissions could be granted to a user in one of the following ways: the system has a \"default\" system role or user group. This type of system roles or groups assigned by default once a user is created; assigned user groups or system role where every member has the same permissions for specific objects; granted permissions for specific user. The priority of permissions granted for specific object explicitly is higher than the group or role permissions, e.g. if a basic user is included into the group that doesn't have an access to some folder but he has permissions explicitly defined for himself that allow him to work with that folder, he will have an access to it. To assign object's permissions to a user or user group you shall move to the object's page and click the \"Gear\" icon in the top-right corner of the screen and select \"Permissions\" tab. Note : To edit permissions: for a Folder - click the \"Gear\" icon \u2192 Edit folder for a Docker registry - click the \"Gear\" icon \u2192 Registry \u2192 Edit . for a Tool group - click the \"Gear\" icon \u2192 Group \u2192 Edit for a Tool - click the \"Gear\" icon \u2192 Permissions . You can explicitly define permissions for the object for a particular user or group of users (users within the same Group) in the \"Permission\" form by clicking on its name in the \"Groups and users\" list. Note : if you couldn't find the desired user or user group, you can add it via \"Add a user\" and \"Add a user group\" controls (see the picture below, 1 ). The additional section for a particular user or user group suggests you tick the desired grants. Here you can allow or deny specific permission options (see the picture above, 2 ). Note : if you don't tick any possible variant, it will be inherited from the parent object (e.g. a pipeline in a folder inherits permissions from it) (see the picture above, 3 ). Example 1: according to the picture above, a user will get WRITE and EXECUTE permissions for an object, and the READ permission will be inherited from the parent object. Example 2: on the picture below we see Permission form of a run configuration. We grant a user READ and EXECUTE permissions, but deny WRITE permission. So the user is able to see the run configuration and run it, but he can not edit its parameters:","title":"13. Permissions"},{"location":"manual/13_Permissions/13._Permissions/#13-permissions","text":"Overview Owner property How to change an owner Admin role Permission settings","title":"13. Permissions"},{"location":"manual/13_Permissions/13._Permissions/#overview","text":"Security Policies Scheme is organized by 2 principal tools: groups (user groups and system roles) and Access Control List defined for each CP's object. Note : About groups and system roles you can read more here . Object's Access Control List specifies who can work with the object and what he can do with it. It is defined as a pair of attributes: a User or User Group ID Permissions The permission settings are divided into the following options which can be combined for the object: Read Write Execute Below is a mapping of the objects' possible actions to permissions which demonstrates what actions will be allowed or denied to a user or user group. Note : according to Security Policies Scheme WRITE permission is not enough to add/delete any Cloud Pipeline object. A specific *_MANAGER role is required also (about roles see here ). Object User Action Permission Folder View folder Read List folder contents Create object (e.g folder, pipeline, etc.) Write Delete folder Rename folder Change parent Upload metadata Pipeline *Permissions for a pipeline version are inherited from the pipeline View pipeline Read List pipeline attributes Delete a pipeline Write Edit pipeline attributes Change parent Run a pipeline Execute DataStorage *Permissions for files in data storage are inherited from the data storage View datastorage Read List datastorage contents Delete a datastorage Write Edit datastorage attributes/contents Change parent Pipeline run View runs Inherited from a run pipeline View run logs Launch a pipeline Stop a run Rerun Tool run View runs Admin and Owner only View run logs Launch a pipeline Stop a run Rerun Cluster node View a cluster Inherited from a currently assigned run View node details Terminate a node Docker Registry View registry Read Add registry Write Delete registry Edit registry attributes Run a child tool Execute Tool group View tool group Read Create tool group Write Edit tool group Delete tool group Run a child tool Execute Tool View enabled tool Read View disabled tools list Edit tool attributes Write Run tool without a pipeline Execute Instance management Admin and Owner only Run configuration View run configuration Read Delete a run configuration Write Edit run configuration attributes Change parent Run a run configuration Execute Note : also you can manage permissions on the Cloud Pipeline objects via CLI. See 14.7. View and manage Permissions via CLI .","title":"Overview"},{"location":"manual/13_Permissions/13._Permissions/#owner-property","text":"Each object has an additional \"Owner\" property. The owner of the object can manage its Access Control List. Owner property is assigned to a user that created an object.","title":"Owner property"},{"location":"manual/13_Permissions/13._Permissions/#how-to-change-an-owner","text":"The Owner of an object can be changed easily for: Folders; Pipelines and pipelines versions; Data storages; Run configurations; Docker registries, Tool Groups and Tools. Note : you shall have Owner or Admin role. To change an owner of an object: Select an object. Click \"Gear\" icon in the top-right corner of the screen. Navigate to Permissions tab. Note : To edit permissions: for a Folder - click the \"Gear\" icon \u2192 Edit folder for a Docker registry - click the \"Gear\" icon \u2192 Registry \u2192 Edit . for a Tool group - click the \"Gear\" icon \u2192 Group \u2192 Edit for a Tool - click the \"Gear\" icon \u2192 Permissions . Click owner's name. Now you can edit it: Start to enter a desired username and system will suggest you the existing users. Click the desired username. Click \"Apply\" control and the changes will be saved. Also you can change an owner of an object via pipe CLI - see here .","title":"How to change an owner"},{"location":"manual/13_Permissions/13._Permissions/#admin-role","text":"Admin property can be given by assigning ROLE_ADMIN to a user (about roles see here ). The user gets Read/Write/Execute/Owner permissions to all objects in the system. Initially, a user with ROLE_ADMIN shall be an authenticated domain account (SAML/OAuth/OpenID) defined during a system deployment (in some properties file or database).","title":"Admin role"},{"location":"manual/13_Permissions/13._Permissions/#permission-settings","text":"The permissions could be granted to a user in one of the following ways: the system has a \"default\" system role or user group. This type of system roles or groups assigned by default once a user is created; assigned user groups or system role where every member has the same permissions for specific objects; granted permissions for specific user. The priority of permissions granted for specific object explicitly is higher than the group or role permissions, e.g. if a basic user is included into the group that doesn't have an access to some folder but he has permissions explicitly defined for himself that allow him to work with that folder, he will have an access to it. To assign object's permissions to a user or user group you shall move to the object's page and click the \"Gear\" icon in the top-right corner of the screen and select \"Permissions\" tab. Note : To edit permissions: for a Folder - click the \"Gear\" icon \u2192 Edit folder for a Docker registry - click the \"Gear\" icon \u2192 Registry \u2192 Edit . for a Tool group - click the \"Gear\" icon \u2192 Group \u2192 Edit for a Tool - click the \"Gear\" icon \u2192 Permissions . You can explicitly define permissions for the object for a particular user or group of users (users within the same Group) in the \"Permission\" form by clicking on its name in the \"Groups and users\" list. Note : if you couldn't find the desired user or user group, you can add it via \"Add a user\" and \"Add a user group\" controls (see the picture below, 1 ). The additional section for a particular user or user group suggests you tick the desired grants. Here you can allow or deny specific permission options (see the picture above, 2 ). Note : if you don't tick any possible variant, it will be inherited from the parent object (e.g. a pipeline in a folder inherits permissions from it) (see the picture above, 3 ). Example 1: according to the picture above, a user will get WRITE and EXECUTE permissions for an object, and the READ permission will be inherited from the parent object. Example 2: on the picture below we see Permission form of a run configuration. We grant a user READ and EXECUTE permissions, but deny WRITE permission. So the user is able to see the run configuration and run it, but he can not edit its parameters:","title":"Permission settings"},{"location":"manual/14_CLI/14.1._Install_and_setup_CLI/","text":"14.1. Install and setup CLI How to install and setup pipe CLI configure command options pipe configuration for using NTLM Authentication Proxy Allow to run pipe commands on behalf of the other user Using pipe token command Using '--user' option Update the CLI How to install and setup pipe CLI Go to Settings \u2192 CLI tab. Select Pipe CLI item at the left panel. Select your Operation System from the list. Follow the installation instructions for your OS (e.g. Linux). Commands below shall be executed in the Terminal. When installation is finished, type pipe in the Terminal to test pipe installation. This command shall produce short description of pipe CLI and pipe CLI commands (it is the same as the pipe --help command execution). Then type the pipe --version command - this command will output only the Cloud Pipeline CLI version: Return to the Web GUI and press the Generate access key button. Copy CLI configure command Paste copied command into the Terminal and run it to configure: Now Cloud Pipeline CLI is ready to use. To check it, run the command pipe --version again: This time the command will output the Cloud Pipeline CLI version, API version, short info about received access token (the user-owner name, usage dates) Note : when pipe CLI is being configured JWT token is given for one month, if user didn't select another expiration date. The warning about the expiration date of the provided token is printed, if it is less than 7 days left: after pipe configure command executing: when any other command is running, e.g.: Note : If any exceptions occur during installation, follow the instructions in the Terminal. Notice that Python 2 / Python 3 has to be installed to run CLI. Python can be downloaded here https://www.python.org/downloads/ . Note : pip package manager is required for CLI installation if you selected Operation System \u2192 Other on step 2. Modern Python versions come bundled with pip . On top of that, with this type of installation you'll also need internet connection to install dependencies. configure command options Options Description Required options -a / --auth-token Token for API authentication -s / --api URL of a Pipeline API endpoint -tz / --timezone [local|utc] Sets presentation timezone. Default: local Non-required options -p / --proxy URL of a proxy for all calls -nt / --proxy-ntlm Enables NTLM proxy support -nu / --proxy-ntlm-user Sets username for NTLM proxy authorization -np / --proxy-ntlm-pass Sets password for NTLM proxy authorization -nd / --proxy-ntlm-domain Sets domain for NTLM proxy authorization Note : there is not necessary to set all options while input that command. If some options are not set directly - user shall be prompted for them in an interactive manner, if they will not be set in an interactive manner, default values will be used (where it is possible). pipe configuration for using NTLM Authentication Proxy CLI pipe can be configured for using NTLM Authentication Proxy, when running in Linux. For that, use the ntlm options described above while execute pipe configure command. If pipe configure command is executing with specified --proxy-ntlm option, pipe will try to get the proxy value from the --proxy option or the environment variables ( --proxy option has a higher priority). Example: pipe configure --proxy-ntlm --proxy http://myproxy:3128 Allow to run pipe commands on behalf of the other user Note : the functionality is able only for users with the ROLE_ADMIN role. It could be convenient/useful for administrators to perform some operations on behalf of the other user (e.g. check permissions/act as a service account/etc.). For that in pipe CLI, there are two abilities: separate pipe token command common option -u ( --user ) for all pipe commands Using pipe token command This command prints the JWT token for a specified user. JWT token could be used manually as authentication API token with the pipe configure command (as described above ) - to configure pipe CLI on behalf of the desired user. The format of the command: pipe token USER_ID [OPTIONS] USER_ID is the name of the user account. Options Description Non-required options -d / --duration The number of days the token will be valid. If it's not set - the default value will be used, same as in the GUI For example, to get a JWT token for the user USER3 for 5 days: Using '--user' option To run separate pipe commands on behalf of the other user - the common option, that was added to all pipe commands, can be used: --user|-u USER_ID (where USER_ID is the name of the user account). Note : the option isn't available for the commands configure , --version , --help . If this option is specified - operation (command execution) will be performed using the corresponding user account. Some examples: view active runs on behalf of the user without ROLE_ADMIN role: list storage content on behalf of the user without ROLE_ADMIN role that hasn't read permission on that storage: list storage content and the attempt to upload the file on behalf of the user without ROLE_ADMIN role that has read permission and hasn't write permission on that storage: Update the CLI The command to update the Cloud Pipeline CLI version: pipe update [PATH] PATH - defines the API URL path to download Cloud Pipeline CLI source (optional argument). This command compare the CLI and API versions. If the CLI version is less than the API one, it will update the CLI - the latest Cloud Pipeline CLI version will be installed. Otherwise no actions will be performed. Example:","title":"14.1. Install and setup"},{"location":"manual/14_CLI/14.1._Install_and_setup_CLI/#141-install-and-setup-cli","text":"How to install and setup pipe CLI configure command options pipe configuration for using NTLM Authentication Proxy Allow to run pipe commands on behalf of the other user Using pipe token command Using '--user' option Update the CLI","title":"14.1. Install and setup CLI"},{"location":"manual/14_CLI/14.1._Install_and_setup_CLI/#how-to-install-and-setup-pipe-cli","text":"Go to Settings \u2192 CLI tab. Select Pipe CLI item at the left panel. Select your Operation System from the list. Follow the installation instructions for your OS (e.g. Linux). Commands below shall be executed in the Terminal. When installation is finished, type pipe in the Terminal to test pipe installation. This command shall produce short description of pipe CLI and pipe CLI commands (it is the same as the pipe --help command execution). Then type the pipe --version command - this command will output only the Cloud Pipeline CLI version: Return to the Web GUI and press the Generate access key button. Copy CLI configure command Paste copied command into the Terminal and run it to configure: Now Cloud Pipeline CLI is ready to use. To check it, run the command pipe --version again: This time the command will output the Cloud Pipeline CLI version, API version, short info about received access token (the user-owner name, usage dates) Note : when pipe CLI is being configured JWT token is given for one month, if user didn't select another expiration date. The warning about the expiration date of the provided token is printed, if it is less than 7 days left: after pipe configure command executing: when any other command is running, e.g.: Note : If any exceptions occur during installation, follow the instructions in the Terminal. Notice that Python 2 / Python 3 has to be installed to run CLI. Python can be downloaded here https://www.python.org/downloads/ . Note : pip package manager is required for CLI installation if you selected Operation System \u2192 Other on step 2. Modern Python versions come bundled with pip . On top of that, with this type of installation you'll also need internet connection to install dependencies.","title":"How to install and setup pipe CLI"},{"location":"manual/14_CLI/14.1._Install_and_setup_CLI/#configure-command-options","text":"Options Description Required options -a / --auth-token Token for API authentication -s / --api URL of a Pipeline API endpoint -tz / --timezone [local|utc] Sets presentation timezone. Default: local Non-required options -p / --proxy URL of a proxy for all calls -nt / --proxy-ntlm Enables NTLM proxy support -nu / --proxy-ntlm-user Sets username for NTLM proxy authorization -np / --proxy-ntlm-pass Sets password for NTLM proxy authorization -nd / --proxy-ntlm-domain Sets domain for NTLM proxy authorization Note : there is not necessary to set all options while input that command. If some options are not set directly - user shall be prompted for them in an interactive manner, if they will not be set in an interactive manner, default values will be used (where it is possible).","title":"configure command options"},{"location":"manual/14_CLI/14.1._Install_and_setup_CLI/#pipe-configuration-for-using-ntlm-authentication-proxy","text":"CLI pipe can be configured for using NTLM Authentication Proxy, when running in Linux. For that, use the ntlm options described above while execute pipe configure command. If pipe configure command is executing with specified --proxy-ntlm option, pipe will try to get the proxy value from the --proxy option or the environment variables ( --proxy option has a higher priority). Example: pipe configure --proxy-ntlm --proxy http://myproxy:3128","title":"pipe configuration for using NTLM Authentication Proxy"},{"location":"manual/14_CLI/14.1._Install_and_setup_CLI/#allow-to-run-pipe-commands-on-behalf-of-the-other-user","text":"Note : the functionality is able only for users with the ROLE_ADMIN role. It could be convenient/useful for administrators to perform some operations on behalf of the other user (e.g. check permissions/act as a service account/etc.). For that in pipe CLI, there are two abilities: separate pipe token command common option -u ( --user ) for all pipe commands","title":"Allow to run pipe commands on behalf of the other user"},{"location":"manual/14_CLI/14.1._Install_and_setup_CLI/#using-pipe-token-command","text":"This command prints the JWT token for a specified user. JWT token could be used manually as authentication API token with the pipe configure command (as described above ) - to configure pipe CLI on behalf of the desired user. The format of the command: pipe token USER_ID [OPTIONS] USER_ID is the name of the user account. Options Description Non-required options -d / --duration The number of days the token will be valid. If it's not set - the default value will be used, same as in the GUI For example, to get a JWT token for the user USER3 for 5 days:","title":"Using pipe token command"},{"location":"manual/14_CLI/14.1._Install_and_setup_CLI/#using-user-option","text":"To run separate pipe commands on behalf of the other user - the common option, that was added to all pipe commands, can be used: --user|-u USER_ID (where USER_ID is the name of the user account). Note : the option isn't available for the commands configure , --version , --help . If this option is specified - operation (command execution) will be performed using the corresponding user account. Some examples: view active runs on behalf of the user without ROLE_ADMIN role: list storage content on behalf of the user without ROLE_ADMIN role that hasn't read permission on that storage: list storage content and the attempt to upload the file on behalf of the user without ROLE_ADMIN role that has read permission and hasn't write permission on that storage:","title":"Using '--user' option"},{"location":"manual/14_CLI/14.1._Install_and_setup_CLI/#update-the-cli","text":"The command to update the Cloud Pipeline CLI version: pipe update [PATH] PATH - defines the API URL path to download Cloud Pipeline CLI source (optional argument). This command compare the CLI and API versions. If the CLI version is less than the API one, it will update the CLI - the latest Cloud Pipeline CLI version will be installed. Otherwise no actions will be performed. Example:","title":"Update the CLI"},{"location":"manual/14_CLI/14.2._View_and_manage_Attributes_via_CLI/","text":"14.2. View and manage Attributes via CLI View attributes Manage attributes Add and Edit attributes Delete attributes Cloud Pipeline CLI has to be installed. See 14.1. Install and setup CLI . View attributes To view attributes of the object you need READ permission for the object. See 13. Permissions . Command to list all tags for a specific object: pipe tag get Object class Object id/name Two parameters shall be specified: Object class - defines a name of the object class. Possible values: data_storage , docker_registry , folder , metadata_entity , pipeline , tool , tool_group , configuration . Object id or name - defines an ID or name of an object of the specified object class. Note : full path to the object has to be specified. Paths to Docker registry and Tool objects should include registry IP address. The example below lists attributes of the data storage with ID 3976 : pipe tag get data_storage 3976 To list attributes of the tool group library : Manage attributes A user has to be an administrator ( ROLE_ADMIN ) or an owner ( OWNER ) of the object to edit attributes. See 13. Permissions . A user can add new attributes, edit or delete existing attributes via CLI. Add and Edit attributes To add new and edit existing attributes the following command is used: pipe tag set Object class Object id/name List of KEY=VALUE Three parameters shall be specified: Object class - defines a name of the object class. Possible values: data_storage , docker_registry , folder , metadata_entity , pipeline , tool , tool_group , configuration . Object id/name - defines an ID or name of an object of the specified object class. Note : full path to the object has to be specified. Paths to Docker registry and Tool objects should include registry IP address. List of KEY=VALUE - list of tags to set. Can be specified as a single KEY=VALUE pair or a list of them. Note : if a specific tag key already exists for an object, it will be overwritten . The example below sets attributes ds_key2 = testvalue2_update and ds_key3 = testvalue3 for the data storage with ID 3976 : pipe tag set data_storage 3976 ds_key2=testvalue2_update ds_key3=testvalue3 Delete attributes To delete attributes the following command is used: pipe tag delete Object class Object id/name List of KEYs Three parameters shall be specified: Object class - defines a name of the object class. Possible values: data_storage , docker_registry , folder , metadata_entity , pipeline , tool , tool_group , configuration . Object id/name - defines an ID or name of an object of the specified object class. Note : full path to the object has to be specified. Paths to Docker registry and Tool objects should include registry IP address. List of KEYs - list of attribute keys to delete. The example below deletes attributes ds_key1 , ds_key3 from the the data storage with ID 3976 : pipe tag delete data_storage 3976 ds_key1 ds_key3","title":"14.2. View and manage Attributes"},{"location":"manual/14_CLI/14.2._View_and_manage_Attributes_via_CLI/#142-view-and-manage-attributes-via-cli","text":"View attributes Manage attributes Add and Edit attributes Delete attributes Cloud Pipeline CLI has to be installed. See 14.1. Install and setup CLI .","title":"14.2. View and manage Attributes via CLI"},{"location":"manual/14_CLI/14.2._View_and_manage_Attributes_via_CLI/#view-attributes","text":"To view attributes of the object you need READ permission for the object. See 13. Permissions . Command to list all tags for a specific object: pipe tag get Object class Object id/name Two parameters shall be specified: Object class - defines a name of the object class. Possible values: data_storage , docker_registry , folder , metadata_entity , pipeline , tool , tool_group , configuration . Object id or name - defines an ID or name of an object of the specified object class. Note : full path to the object has to be specified. Paths to Docker registry and Tool objects should include registry IP address. The example below lists attributes of the data storage with ID 3976 : pipe tag get data_storage 3976 To list attributes of the tool group library :","title":"View attributes"},{"location":"manual/14_CLI/14.2._View_and_manage_Attributes_via_CLI/#manage-attributes","text":"A user has to be an administrator ( ROLE_ADMIN ) or an owner ( OWNER ) of the object to edit attributes. See 13. Permissions . A user can add new attributes, edit or delete existing attributes via CLI.","title":"Manage attributes"},{"location":"manual/14_CLI/14.2._View_and_manage_Attributes_via_CLI/#add-and-edit-attributes","text":"To add new and edit existing attributes the following command is used: pipe tag set Object class Object id/name List of KEY=VALUE Three parameters shall be specified: Object class - defines a name of the object class. Possible values: data_storage , docker_registry , folder , metadata_entity , pipeline , tool , tool_group , configuration . Object id/name - defines an ID or name of an object of the specified object class. Note : full path to the object has to be specified. Paths to Docker registry and Tool objects should include registry IP address. List of KEY=VALUE - list of tags to set. Can be specified as a single KEY=VALUE pair or a list of them. Note : if a specific tag key already exists for an object, it will be overwritten . The example below sets attributes ds_key2 = testvalue2_update and ds_key3 = testvalue3 for the data storage with ID 3976 : pipe tag set data_storage 3976 ds_key2=testvalue2_update ds_key3=testvalue3","title":"Add and Edit attributes"},{"location":"manual/14_CLI/14.2._View_and_manage_Attributes_via_CLI/#delete-attributes","text":"To delete attributes the following command is used: pipe tag delete Object class Object id/name List of KEYs Three parameters shall be specified: Object class - defines a name of the object class. Possible values: data_storage , docker_registry , folder , metadata_entity , pipeline , tool , tool_group , configuration . Object id/name - defines an ID or name of an object of the specified object class. Note : full path to the object has to be specified. Paths to Docker registry and Tool objects should include registry IP address. List of KEYs - list of attribute keys to delete. The example below deletes attributes ds_key1 , ds_key3 from the the data storage with ID 3976 : pipe tag delete data_storage 3976 ds_key1 ds_key3","title":"Delete attributes"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/","text":"14.3. Manage Storage via CLI Create a datastorage List storages/storage content Show storage usage Edit a datastorage Change backup duration, STS/LTS duration, versioning Change a parent folder for a datastorage Delete a datastorage Create a folder in a storage Upload and download data Control File versions Show files versions Restore files Delete an object from a datastorage Manage datastorage objects attributes Get object attributes Set object attributes Delete object attributes Mounting of storages Mount a storage Unmount a storage Cloud Pipeline CLI has to be installed. See 14.1. Install and setup CLI . To perform different operations with object storages the command set pipe storage is used. Create a datastorage The command to create an object storage: pipe storage create [OPTIONS] Options Description Required options -n / --name Alias of the new object storage -p / --path Datastorage path Non-required options -d / --description Description of the object storage -sts / --short_term_storage Number of days for storing data in the short term storage. Note : This option is available not for all Cloud Providers -lts / --long_term_storage Number of days for storing data in the long term storage. Note : This option is available not for all Cloud Providers -v / --versioning Enable versioning for this object storage. Note : This option is available not for all Cloud Providers -b / --backup_duration Number of days for storing backups of the storage. Note : This option is available not for all Cloud Providers -t / --type Type of the Cloud for the object storage. Depends on the Cloud Provider. Possible values - S3 (for AWS), AZ (for MS Azure), GS (for GCP) and NFS (for FS mounts) -f / --parent_folder Name/ID of the folder which will contain the object storage. Default value - library root folder -c / --on_cloud To create datastorage on the Cloud. This flag shall be specified, only if a new datastorage is being created. If user want to add an existing storage this flag should not be specified -r / --region_id Cloud region ID where storage shall be created Note : there is not necessary to set all options while input that command. If some options are not set - user shall be prompted for them in an interactive manner, if they will not be set in an interactive manner, default values will be used. In the example below the object storage output_results for the new datastorage output_results_storage will be created in the folder with ID 297 with the following options: STS duration - 20 days, LTS duration - 45 days, backup duration - 30 days. pipe storage create -n output-results -p output-results-storage -c -sts 20 -lts 45 -b 30 -f 297 -c flag was specify to create a storage in the Cloud. As you can see - Description , Type of the Cloud and Cloud Region ID fields were left empty. So, values for these options will be set as default. List storages/storage content The command to view the full storage list of the current platform deployment: pipe storage ls [OPTIONS] Options Description Non-required options -l / --show_details Show details Perform the following command to view full storage list and to check that the object storage from the example above was created: pipe storage ls -l The command to view the content of the datastorage: pipe storage ls [OPTIONS] Path Path - defines a full path to the datastorage. Path value must begin from the Cloud prefix ( S3 (for AWS), AZ (for MS Azure), GS (for GCP) or common CP (instead of described ones, regardless of Provider)). Options Description Non-required options -l / --show_details Show details -v / --show_versions Show object versions. Only for storages with enabled versioning -r / --recursive Recursive listing -p / --page Maximum number of records to show -a / --all Show all results at once ignoring page settings Example of the detailed datastorage content view: pipe storage ls --show_details s3://objstor Or you can use the common CP prefix instead of S3 : pipe storage ls --show_details cp://objstor Result will be the same: Show storage usage To obtain a \"disk usage\" information on the supported data storage or its inner folder(s) use the command: pipe storage du [OPTIONS] [STORAGE] The command prints for the data storages/path: summary number of files in the storage/path summary size of files in the storage/path Options Description Non-required options -p / --relative-path The relative path inside the storage -f / --format The size unit format (default: Mb ). Possible values: K , Kb , KB - for kilobytes; M , Mb , MB - for megabytes; G , Gb , GB - for gigabytes -d / --depth The maximum depth level of the nesting folders. Note : this option isn't supported for the FS storages yet STORAGE - defines the datastorage name/path. Without specifying any options and storage this command prints the full list of the available storages (both types - object and FS) with the \"usage\" information for each of them: pipe storage du With specifying the storage name this command prints the \"usage\" information only by that storage, e.g.: pipe storage du objectdatastorage With -p ( --relative-path ) option the command prints the \"usage\" information for the specified path in the required storage, e.g.: pipe storage du objectdatastorage -p innerdir1 With -d ( --depth ) option the command prints the \"usage\" information in the required storage (and path) for the specified folders nesting depth, e.g.: pipe storage du objectdatastorage -p innerdir2 -d 1 Specified command will print the \"usage\" information for all folders with the nesting depth no more 1 relative to the path objectdatastorage/innerdir2/ : With -f ( --format ) option user can change the unit for the size value, e.g. to print the storage usage in gigabytes: pipe storage du objectdatastorage -f GB Edit a datastorage Change backup duration, STS/LTS duration, versioning The Command to change backup duration, select STS/LTS duration or enable versioning: pipe storage policy [OPTIONS] Options Description Required options -n / --name Alias/path of the storage to update the policy. Specified without Cloud prefix Non-required options -sts / --short_term_storage Number of days for storing data in the short term storage. Note : This option is available not for all Cloud Providers -lts / --long_term_storage Number of days for storing data in the long term storage. Note : This option is available not for all Cloud Providers -v / --versioning Enable versioning for this object storage. Note : This option is available not for all Cloud Providers -b / --backup_duration Number of days for storing backups of the storage. Note : This option is available not for all Cloud Providers In the example below backup duration is set to 25 days, STS and LTS durations are set to 50 days and 100 days respectively for the datastorage objstor . Also, we enable versioning for that datastorage: pipe storage policy -n objstor -b 25 -sts 50 -lts 100 -v Note : there is not necessary to set all options while input that command. If some options are not set - user shall be prompted for them in an interactive manner, if they will not be set in an interactive manner, default values will be used. You can check via the GUI that parameters were changed: Change a parent folder for a datastorage The command to move a datastorage to a new parent folder: pipe storage mvtodir Storage Directory Directory - name of the folder to which the object storage will be moved. Storage - alias/path of the storage to move. Specified without Cloud prefix. In the example below we will move the storage objstor to the folder \" InnerFolder \": pipe storage mvtodir objstor InnerFolder Delete a datastorage The command to delete an object storage: pipe storage delete [OPTIONS] Options Description Required options -n / --name Alias/path of the storage to delete. Specified without Cloud prefix Non-required options -c / --on_cloud To delete datastorage from a Cloud. If this option isn't set, datastorage will just become unregistered -y / --yes Do not ask confirmation In the example below we will delete the output-results-storage without asking confirmation: pipe storage delete -n output-results-storage -y As the command above was performed withoud --on_cloud option, output-results-storage wasn't deleted from a Cloud actually and it might be added again to the Cloud Pipeline platform via described storage create command: Create a folder in a storage The command to create a folder in a storage: pipe storage mkdir List of FOLDERs List of FOLDERs - defines a list of the folders paths. Each path in the list shall be a full path to a new folder in the specific datastorage. Path value must begin from the Cloud prefix ( S3 (for AWS), AZ (for MS Azure), GS (for GCP) or common CP (instead of described ones, regardless of Provider)). In the example below we will create folders \" new-folder1 \", \" new-folder2 \" in the storage output-results-storage and then will check that they're exist via described storage ls command: pipe storage mkdir cp://output-results-storage/new-folder1 cp://output-results-storage/new-folder2 Upload and download data There are two commands to upload/download data: pipe storage cp [OPTIONS] Source Destination pipe storage mv [OPTIONS] Source Destination By cp command you can copy files from one datastorage to another one or between local filesystem and a datastorage (in both directions). By mv command you can move files from one datastorage to another one or between local filesystem and a datastorage (in both directions). Source - defines a path (in the local filesystem or datastorage) to the object to be copied/moved. Destination - defines a path (in the local filesystem or datastorage) to the object where source object will be copied/moved. Options Description Non-required options -r / --recursive Recursive source scan. This option is not needed when you copy/move a single file -f / --force Rewrite files in destination -e / --exclude Exclude all files matching this pattern from processing -i / --include Include only files matching this pattern into processing -q / --quiet Quiet mode -s / --skip-existing Skip files existing in destination, if they have size matching source -t / --tags Set object attributes during processing. attributes can be specified as single KEY=VALUE pair or a list of them. If this option specified all existent attributes will be overwritten -l / --file-list Path to the file with file paths that should be copied/moved. This file should be tab delimited and consist of two columns: relative path to file and size -sl / --symlinks [follow|filter|skip] Describe symlinks processing strategy for local sources. Possible values: follow - follow symlinks (default); skip - do not follow symlinks; filter - follow symlinks but check for cyclic links In the example below we will upload files from the local filesystem to the storage objstor into the folder \" upload \": pipe storage cp ~/data cp://objstor/upload --recursive Application will start uploading files and print progress. You can view it: After uploading is complete, check that they're uploaded - via described storage ls command: In the example below we will upload the file \" 2.fastq \" from the local filesystem in the quiet mode, rewrite it to the storage objstor into the folder \" upload \" and set two attributes on it: storage cp -f -q ~/data/2.fastq cp://objstor/upload/ -t testkey1=testvalue1 -t testkey2=testvalue2 After that, we will check attributes of the uploaded file - via storage get-object-tags command: Note : Files uploaded via CLI will have the following attributes and values automatically set: CP_OWNER . The value of the attribute will be set as a user ID. CP_SOURCE . The value of the attribute will be set as a local path used to upload. The example demonstrates automatic file tagging after data uploading: The example below demonstrates how to download files from the datastorage folder to the local filesystem. Also we will not download files which names starts from 1. : pipe storage cp -r -e 1.* cp://objstor/upload/ ~/data/input/ Control File versions Note : This feature is available not for all Cloud Providers. Currently, it is supported by AWS and GCP . Show files versions To view file versions for the storage with enabled versioning use the storage ls command with the both specified -l and -v options, e.g.: pipe storage ls -l -v cp://versioning-storage As you can see - \" file1 \" and \" file3 \" each has 1 version. \" file2 \" has 3 versions. \" file4 \" has 2 verions. To view versions of a specific file specify its full path, e.g.: pipe storage ls -l -v cp://versioning-storage/file4 Restore files The command to restore a previous version of a file: pipe storage restore [OPTIONS] Path Path - defines a full path to the file/directory in a datastorage. Options Description Non-required options -v / --version To restore a specified version -r / --recursive To restore the whole directory hierarchy. Note : this feature is yet supported for the AWS Cloud Provider only -i / --include Include only files matching this pattern into processing -e / --exclude Exclude all files matching this pattern from processing By this command you can restore file version in a datastorage. If version is not specified via -v option it will try to restore the latest non-deleted version. Otherwise a specified version will be restored. If the Path is a directory, the command gets the top-level deleted files from the Path directory and restore them to the latest version. The example below shows how to set one of the previous versions for the \" file2 \" as the latest: pipe storage restore -v Version cp://versioning-storage/file2 Note : When a specified version of the \" file2 \" is restored, a copy of that version is created to become the latest version of the file. You can restore a deleted file without specifying a version. It works only for files with a Delete marker as the latest version (\" file4 \" in the example below). In such case the command will be, e.g.: pipe storage restore cp://versioning-storage/file4 Note : Before we restored the file \" file4 \" its latest version was a Delete marker . After restoration this marker disappeared. The example below shows how to restore the latest version for only \"*.txt\" files recursively in the deleted directory (the directory that was marked with a Delete marker previously), e.g.: pipe storage restore --recursive --include *.txt s3://objstor/examples/ After restoring, check that the \"*.txt\" file in the subfolder also restored: Delete an object from a datastorage The command to delete an object from a storage: pipe storage rm [OPTIONS] Path Path - defines a full path to the object in a datastorage. Options Description Non-required options -y / --yes Do not ask confirmation -v / --version Delete a specified version of an object -d / --hard-delete Completely delete an object from a storage -r / --recursive Recursive deletion (required for deleting folders) -e / --exclude Exclude all files matching this pattern from processing -i / --include Include only files matching this pattern into processing The example below demonstrates how to delete a file from the storage objstor : pipe storage rm cp://objstor/simplefile If this command is performed without options over the object from the storage with enabled versioning, that object will not be removed completely, it will remain in a datastorage and get a Delete marker . Such objects can be restored via storage restore command. In the example below we set a Delete marker to the file \" file1 \": Note : the latest version of the file \" file1 \" is marked with Delete marker now. To completely delete an object from a datastorage use -d option. In the example below we will completely delete the file \" file2 \" from the storage with enabled versioning without asking confirmation: pipe storage rm -y -d cp://versioning-storage/file2 To delete a specific version of an object use -v option. In the example below we will delete one of the versions of the file \" file4 \" from the storage with enabled versioning without asking confirmation: pipe storage rm -y -v Version cp://versioning-storage/file4 In the example below we will completely delete files from the folder \" initial-data \" - for that we will use --recursive option. But we will delete only files whose names contain symbol 3 - for that we will use --include option: pipe storage rm --yes --hard-delete --include *3* --recursive cp://versioning-storage/initial-data/ Manage datastorage objects attributes This section is about attribute management of the inner datastorages files via CLI. To manage attributes of other Cloud Pipeline objects - see 14.2. View and manage Attributes via CLI . Get object attributes The command to list attributes of a specific file in a datastorage: pipe storage get-object-tags [OPTIONS] Path Path - defines a full path to a file in a datastorage. Path value must begin from the Cloud prefix ( S3 (for AWS), AZ (for MS Azure), GS (for GCP) or common CP (instead of described ones, regardless of Provider)). Options Description Non-required options -v / --version To get attributes for a specified file version. If option is not set, but datastorage versioning is enabled - processing will be performed for the latest file version In the example below we will get attributes for the file \" file1 \" from the datastorage objstor : pipe storage get-object-tags cp://objstor/file1 In the example below we will get attributes for the previous version of the file \" file1 \" from the datastorage objstor : pipe storage get-object-tags -v Version cp://objstor/file1 Set object attributes The command to set attributes for a specific file in a datastorage: pipe storage set-object-tags [OPTIONS] Path List of KEY=VALUE Path - defines a full path to a file in a datastorage. Path value must begin from the Cloud prefix ( S3 (for AWS), AZ (for MS Azure), GS (for GCP) or common CP (instead of described ones, regardless of Provider)). List of KEY=VALUE - list of attributes to set. Can be specified as a single KEY=VALUE pair or a list of them. Options Description Non-required options -v / --version To set attributes for a specified file version. If option is not set, but datastorage versioning is enabled - processing will be performed for the latest file version Note : if a specific attribute key already exists for a file, it will be overwritten . In the example below we will set attributes for the file \" file2 \" from the datastorage objstor and then check that attributes are set by the storage get-object-tags command: pipe storage set-object-tags cp://objstor/file2 example_key1=example_value1 example_key2=example_value2 Delete object attributes The command to delete attributes for a specific file in a datastorage: pipe storage delete-object-tags [OPTIONS] Path List of KEYs Path - defines a full path to a file in a datastorage. Path value must begin from the Cloud prefix ( S3 (for AWS), AZ (for MS Azure), GS (for GCP) or common CP (instead of described ones, regardless of Provider)). List of KEYs - list of attribute keys to delete. Options Description Non-required options -v / --version To delete attributes for a specified file version. If option is not set, but datastorage versioning is enabled - processing will be performed for the latest file version In the example below we will delete attribute tagkey1 for the previous version of the file \" file1 \" from the datastorage objstor : pipe storage delete-object-tags -v Version cp://objstor/file1 tagkey1 Mounting of storages pipe cli supports mounting data storages (both - File Storages and Object Storages) to Linux and Mac workstations (requires FUSE installed). Note : This feature is available not for all Cloud Providers. Currently, it is supported only by AWS For the mounted storages, regular listing/read/write commands are supported (e.g. cp , mv , ls , mkdir , rm , fallocate , truncate , dd , etc. - according to the corresponding OS), users can manage files/folders as with any general hard drive. Mount a storage The command to mount a data storage into the mountpoint: pipe storage mount [OPTIONS] Mountpoint Mountpoint - defines a full path to a directory on the workstation where the data storage shall be mounted. Options Description Required options -f / --file File System mode. In this mode, all available File Storages will be mounted into the mountpoint. This option is mutually exclusive with -b option -b / --bucket Object Storage mode. In this mode, the specified Object Storage will be mounted into the mountpoint. This option is mutually exclusive with -f option Non-required options -m / --mode Allows to set a mask that defines access permissions (to the owner / group / others ). This mask is set in the numerical view (three-digit octal number) -o / --options Allows to specify any mount options supported by underlying FUSE implementation -l / --log-file If set, standard/error output of mount operations will be written into the specified file -q / --quiet Quiet mode -t / --threads Enable multithreading - allows several processes simultaneously interact with the mount point In the example below we will mount the Object Storage named objstor into the directory \" mountdir \" and specify the file \" mount.log \" as log-file for mount operations: pipe storage mount -l ~/mount.log -b objstor ~/mountdir In the example below we will mount the Object Storage named objectdatastorage into the directory \" mountdir \" and set full access permissons (read, write, execute) to the owner, read and execute permissions to the group and no permissions to others: pipe storage mount -b objectdatastorage -m 750 ~/mountdir Note : if -m option isn't specified, during mounting the default permissions mask will be set - 700 (full access to the owner and no permissions to the group and others) Unmount a storage The command to unmount a mountpoint: pipe storage umount [OPTIONS] Mountpoint Mountpoint - defines a full path to a directory on the workstation where the data storage was mounted. Options Description Non-required options -q / --quiet Quiet mode In the example below we will unmount a storage from the directory \" mountdir \": pipe storage umount ~/mountdir","title":"14.3. Manage Data Storage"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#143-manage-storage-via-cli","text":"Create a datastorage List storages/storage content Show storage usage Edit a datastorage Change backup duration, STS/LTS duration, versioning Change a parent folder for a datastorage Delete a datastorage Create a folder in a storage Upload and download data Control File versions Show files versions Restore files Delete an object from a datastorage Manage datastorage objects attributes Get object attributes Set object attributes Delete object attributes Mounting of storages Mount a storage Unmount a storage Cloud Pipeline CLI has to be installed. See 14.1. Install and setup CLI . To perform different operations with object storages the command set pipe storage is used.","title":"14.3. Manage Storage via CLI"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#create-a-datastorage","text":"The command to create an object storage: pipe storage create [OPTIONS] Options Description Required options -n / --name Alias of the new object storage -p / --path Datastorage path Non-required options -d / --description Description of the object storage -sts / --short_term_storage Number of days for storing data in the short term storage. Note : This option is available not for all Cloud Providers -lts / --long_term_storage Number of days for storing data in the long term storage. Note : This option is available not for all Cloud Providers -v / --versioning Enable versioning for this object storage. Note : This option is available not for all Cloud Providers -b / --backup_duration Number of days for storing backups of the storage. Note : This option is available not for all Cloud Providers -t / --type Type of the Cloud for the object storage. Depends on the Cloud Provider. Possible values - S3 (for AWS), AZ (for MS Azure), GS (for GCP) and NFS (for FS mounts) -f / --parent_folder Name/ID of the folder which will contain the object storage. Default value - library root folder -c / --on_cloud To create datastorage on the Cloud. This flag shall be specified, only if a new datastorage is being created. If user want to add an existing storage this flag should not be specified -r / --region_id Cloud region ID where storage shall be created Note : there is not necessary to set all options while input that command. If some options are not set - user shall be prompted for them in an interactive manner, if they will not be set in an interactive manner, default values will be used. In the example below the object storage output_results for the new datastorage output_results_storage will be created in the folder with ID 297 with the following options: STS duration - 20 days, LTS duration - 45 days, backup duration - 30 days. pipe storage create -n output-results -p output-results-storage -c -sts 20 -lts 45 -b 30 -f 297 -c flag was specify to create a storage in the Cloud. As you can see - Description , Type of the Cloud and Cloud Region ID fields were left empty. So, values for these options will be set as default.","title":"Create a datastorage"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#list-storagesstorage-content","text":"The command to view the full storage list of the current platform deployment: pipe storage ls [OPTIONS] Options Description Non-required options -l / --show_details Show details Perform the following command to view full storage list and to check that the object storage from the example above was created: pipe storage ls -l The command to view the content of the datastorage: pipe storage ls [OPTIONS] Path Path - defines a full path to the datastorage. Path value must begin from the Cloud prefix ( S3 (for AWS), AZ (for MS Azure), GS (for GCP) or common CP (instead of described ones, regardless of Provider)). Options Description Non-required options -l / --show_details Show details -v / --show_versions Show object versions. Only for storages with enabled versioning -r / --recursive Recursive listing -p / --page Maximum number of records to show -a / --all Show all results at once ignoring page settings Example of the detailed datastorage content view: pipe storage ls --show_details s3://objstor Or you can use the common CP prefix instead of S3 : pipe storage ls --show_details cp://objstor Result will be the same:","title":"List storages/storage content"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#show-storage-usage","text":"To obtain a \"disk usage\" information on the supported data storage or its inner folder(s) use the command: pipe storage du [OPTIONS] [STORAGE] The command prints for the data storages/path: summary number of files in the storage/path summary size of files in the storage/path Options Description Non-required options -p / --relative-path The relative path inside the storage -f / --format The size unit format (default: Mb ). Possible values: K , Kb , KB - for kilobytes; M , Mb , MB - for megabytes; G , Gb , GB - for gigabytes -d / --depth The maximum depth level of the nesting folders. Note : this option isn't supported for the FS storages yet STORAGE - defines the datastorage name/path. Without specifying any options and storage this command prints the full list of the available storages (both types - object and FS) with the \"usage\" information for each of them: pipe storage du With specifying the storage name this command prints the \"usage\" information only by that storage, e.g.: pipe storage du objectdatastorage With -p ( --relative-path ) option the command prints the \"usage\" information for the specified path in the required storage, e.g.: pipe storage du objectdatastorage -p innerdir1 With -d ( --depth ) option the command prints the \"usage\" information in the required storage (and path) for the specified folders nesting depth, e.g.: pipe storage du objectdatastorage -p innerdir2 -d 1 Specified command will print the \"usage\" information for all folders with the nesting depth no more 1 relative to the path objectdatastorage/innerdir2/ : With -f ( --format ) option user can change the unit for the size value, e.g. to print the storage usage in gigabytes: pipe storage du objectdatastorage -f GB","title":"Show storage usage"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#edit-a-datastorage","text":"","title":"Edit a datastorage"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#change-backup-duration-stslts-duration-versioning","text":"The Command to change backup duration, select STS/LTS duration or enable versioning: pipe storage policy [OPTIONS] Options Description Required options -n / --name Alias/path of the storage to update the policy. Specified without Cloud prefix Non-required options -sts / --short_term_storage Number of days for storing data in the short term storage. Note : This option is available not for all Cloud Providers -lts / --long_term_storage Number of days for storing data in the long term storage. Note : This option is available not for all Cloud Providers -v / --versioning Enable versioning for this object storage. Note : This option is available not for all Cloud Providers -b / --backup_duration Number of days for storing backups of the storage. Note : This option is available not for all Cloud Providers In the example below backup duration is set to 25 days, STS and LTS durations are set to 50 days and 100 days respectively for the datastorage objstor . Also, we enable versioning for that datastorage: pipe storage policy -n objstor -b 25 -sts 50 -lts 100 -v Note : there is not necessary to set all options while input that command. If some options are not set - user shall be prompted for them in an interactive manner, if they will not be set in an interactive manner, default values will be used. You can check via the GUI that parameters were changed:","title":"Change backup duration, STS/LTS duration, versioning"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#change-a-parent-folder-for-a-datastorage","text":"The command to move a datastorage to a new parent folder: pipe storage mvtodir Storage Directory Directory - name of the folder to which the object storage will be moved. Storage - alias/path of the storage to move. Specified without Cloud prefix. In the example below we will move the storage objstor to the folder \" InnerFolder \": pipe storage mvtodir objstor InnerFolder","title":"Change a parent folder for a datastorage"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#delete-a-datastorage","text":"The command to delete an object storage: pipe storage delete [OPTIONS] Options Description Required options -n / --name Alias/path of the storage to delete. Specified without Cloud prefix Non-required options -c / --on_cloud To delete datastorage from a Cloud. If this option isn't set, datastorage will just become unregistered -y / --yes Do not ask confirmation In the example below we will delete the output-results-storage without asking confirmation: pipe storage delete -n output-results-storage -y As the command above was performed withoud --on_cloud option, output-results-storage wasn't deleted from a Cloud actually and it might be added again to the Cloud Pipeline platform via described storage create command:","title":"Delete a datastorage"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#create-a-folder-in-a-storage","text":"The command to create a folder in a storage: pipe storage mkdir List of FOLDERs List of FOLDERs - defines a list of the folders paths. Each path in the list shall be a full path to a new folder in the specific datastorage. Path value must begin from the Cloud prefix ( S3 (for AWS), AZ (for MS Azure), GS (for GCP) or common CP (instead of described ones, regardless of Provider)). In the example below we will create folders \" new-folder1 \", \" new-folder2 \" in the storage output-results-storage and then will check that they're exist via described storage ls command: pipe storage mkdir cp://output-results-storage/new-folder1 cp://output-results-storage/new-folder2","title":"Create a folder in a\u00a0storage"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#upload-and-download-data","text":"There are two commands to upload/download data: pipe storage cp [OPTIONS] Source Destination pipe storage mv [OPTIONS] Source Destination By cp command you can copy files from one datastorage to another one or between local filesystem and a datastorage (in both directions). By mv command you can move files from one datastorage to another one or between local filesystem and a datastorage (in both directions). Source - defines a path (in the local filesystem or datastorage) to the object to be copied/moved. Destination - defines a path (in the local filesystem or datastorage) to the object where source object will be copied/moved. Options Description Non-required options -r / --recursive Recursive source scan. This option is not needed when you copy/move a single file -f / --force Rewrite files in destination -e / --exclude Exclude all files matching this pattern from processing -i / --include Include only files matching this pattern into processing -q / --quiet Quiet mode -s / --skip-existing Skip files existing in destination, if they have size matching source -t / --tags Set object attributes during processing. attributes can be specified as single KEY=VALUE pair or a list of them. If this option specified all existent attributes will be overwritten -l / --file-list Path to the file with file paths that should be copied/moved. This file should be tab delimited and consist of two columns: relative path to file and size -sl / --symlinks [follow|filter|skip] Describe symlinks processing strategy for local sources. Possible values: follow - follow symlinks (default); skip - do not follow symlinks; filter - follow symlinks but check for cyclic links In the example below we will upload files from the local filesystem to the storage objstor into the folder \" upload \": pipe storage cp ~/data cp://objstor/upload --recursive Application will start uploading files and print progress. You can view it: After uploading is complete, check that they're uploaded - via described storage ls command: In the example below we will upload the file \" 2.fastq \" from the local filesystem in the quiet mode, rewrite it to the storage objstor into the folder \" upload \" and set two attributes on it: storage cp -f -q ~/data/2.fastq cp://objstor/upload/ -t testkey1=testvalue1 -t testkey2=testvalue2 After that, we will check attributes of the uploaded file - via storage get-object-tags command: Note : Files uploaded via CLI will have the following attributes and values automatically set: CP_OWNER . The value of the attribute will be set as a user ID. CP_SOURCE . The value of the attribute will be set as a local path used to upload. The example demonstrates automatic file tagging after data uploading: The example below demonstrates how to download files from the datastorage folder to the local filesystem. Also we will not download files which names starts from 1. : pipe storage cp -r -e 1.* cp://objstor/upload/ ~/data/input/","title":"Upload and download data"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#control-file-versions","text":"Note : This feature is available not for all Cloud Providers. Currently, it is supported by AWS and GCP .","title":"Control File versions"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#show-files-versions","text":"To view file versions for the storage with enabled versioning use the storage ls command with the both specified -l and -v options, e.g.: pipe storage ls -l -v cp://versioning-storage As you can see - \" file1 \" and \" file3 \" each has 1 version. \" file2 \" has 3 versions. \" file4 \" has 2 verions. To view versions of a specific file specify its full path, e.g.: pipe storage ls -l -v cp://versioning-storage/file4","title":"Show files versions"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#restore-files","text":"The command to restore a previous version of a file: pipe storage restore [OPTIONS] Path Path - defines a full path to the file/directory in a datastorage. Options Description Non-required options -v / --version To restore a specified version -r / --recursive To restore the whole directory hierarchy. Note : this feature is yet supported for the AWS Cloud Provider only -i / --include Include only files matching this pattern into processing -e / --exclude Exclude all files matching this pattern from processing By this command you can restore file version in a datastorage. If version is not specified via -v option it will try to restore the latest non-deleted version. Otherwise a specified version will be restored. If the Path is a directory, the command gets the top-level deleted files from the Path directory and restore them to the latest version. The example below shows how to set one of the previous versions for the \" file2 \" as the latest: pipe storage restore -v Version cp://versioning-storage/file2 Note : When a specified version of the \" file2 \" is restored, a copy of that version is created to become the latest version of the file. You can restore a deleted file without specifying a version. It works only for files with a Delete marker as the latest version (\" file4 \" in the example below). In such case the command will be, e.g.: pipe storage restore cp://versioning-storage/file4 Note : Before we restored the file \" file4 \" its latest version was a Delete marker . After restoration this marker disappeared. The example below shows how to restore the latest version for only \"*.txt\" files recursively in the deleted directory (the directory that was marked with a Delete marker previously), e.g.: pipe storage restore --recursive --include *.txt s3://objstor/examples/ After restoring, check that the \"*.txt\" file in the subfolder also restored:","title":"Restore files"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#delete-an-object-from-a-datastorage","text":"The command to delete an object from a storage: pipe storage rm [OPTIONS] Path Path - defines a full path to the object in a datastorage. Options Description Non-required options -y / --yes Do not ask confirmation -v / --version Delete a specified version of an object -d / --hard-delete Completely delete an object from a storage -r / --recursive Recursive deletion (required for deleting folders) -e / --exclude Exclude all files matching this pattern from processing -i / --include Include only files matching this pattern into processing The example below demonstrates how to delete a file from the storage objstor : pipe storage rm cp://objstor/simplefile If this command is performed without options over the object from the storage with enabled versioning, that object will not be removed completely, it will remain in a datastorage and get a Delete marker . Such objects can be restored via storage restore command. In the example below we set a Delete marker to the file \" file1 \": Note : the latest version of the file \" file1 \" is marked with Delete marker now. To completely delete an object from a datastorage use -d option. In the example below we will completely delete the file \" file2 \" from the storage with enabled versioning without asking confirmation: pipe storage rm -y -d cp://versioning-storage/file2 To delete a specific version of an object use -v option. In the example below we will delete one of the versions of the file \" file4 \" from the storage with enabled versioning without asking confirmation: pipe storage rm -y -v Version cp://versioning-storage/file4 In the example below we will completely delete files from the folder \" initial-data \" - for that we will use --recursive option. But we will delete only files whose names contain symbol 3 - for that we will use --include option: pipe storage rm --yes --hard-delete --include *3* --recursive cp://versioning-storage/initial-data/","title":"Delete an object from a datastorage"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#manage-datastorage-objects-attributes","text":"This section is about attribute management of the inner datastorages files via CLI. To manage attributes of other Cloud Pipeline objects - see 14.2. View and manage Attributes via CLI .","title":"Manage datastorage objects attributes"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#get-object-attributes","text":"The command to list attributes of a specific file in a datastorage: pipe storage get-object-tags [OPTIONS] Path Path - defines a full path to a file in a datastorage. Path value must begin from the Cloud prefix ( S3 (for AWS), AZ (for MS Azure), GS (for GCP) or common CP (instead of described ones, regardless of Provider)). Options Description Non-required options -v / --version To get attributes for a specified file version. If option is not set, but datastorage versioning is enabled - processing will be performed for the latest file version In the example below we will get attributes for the file \" file1 \" from the datastorage objstor : pipe storage get-object-tags cp://objstor/file1 In the example below we will get attributes for the previous version of the file \" file1 \" from the datastorage objstor : pipe storage get-object-tags -v Version cp://objstor/file1","title":"Get object attributes"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#set-object-attributes","text":"The command to set attributes for a specific file in a datastorage: pipe storage set-object-tags [OPTIONS] Path List of KEY=VALUE Path - defines a full path to a file in a datastorage. Path value must begin from the Cloud prefix ( S3 (for AWS), AZ (for MS Azure), GS (for GCP) or common CP (instead of described ones, regardless of Provider)). List of KEY=VALUE - list of attributes to set. Can be specified as a single KEY=VALUE pair or a list of them. Options Description Non-required options -v / --version To set attributes for a specified file version. If option is not set, but datastorage versioning is enabled - processing will be performed for the latest file version Note : if a specific attribute key already exists for a file, it will be overwritten . In the example below we will set attributes for the file \" file2 \" from the datastorage objstor and then check that attributes are set by the storage get-object-tags command: pipe storage set-object-tags cp://objstor/file2 example_key1=example_value1 example_key2=example_value2","title":"Set object attributes"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#delete-object-attributes","text":"The command to delete attributes for a specific file in a datastorage: pipe storage delete-object-tags [OPTIONS] Path List of KEYs Path - defines a full path to a file in a datastorage. Path value must begin from the Cloud prefix ( S3 (for AWS), AZ (for MS Azure), GS (for GCP) or common CP (instead of described ones, regardless of Provider)). List of KEYs - list of attribute keys to delete. Options Description Non-required options -v / --version To delete attributes for a specified file version. If option is not set, but datastorage versioning is enabled - processing will be performed for the latest file version In the example below we will delete attribute tagkey1 for the previous version of the file \" file1 \" from the datastorage objstor : pipe storage delete-object-tags -v Version cp://objstor/file1 tagkey1","title":"Delete object attributes"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#mounting-of-storages","text":"pipe cli supports mounting data storages (both - File Storages and Object Storages) to Linux and Mac workstations (requires FUSE installed). Note : This feature is available not for all Cloud Providers. Currently, it is supported only by AWS For the mounted storages, regular listing/read/write commands are supported (e.g. cp , mv , ls , mkdir , rm , fallocate , truncate , dd , etc. - according to the corresponding OS), users can manage files/folders as with any general hard drive.","title":"Mounting of storages"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#mount-a-storage","text":"The command to mount a data storage into the mountpoint: pipe storage mount [OPTIONS] Mountpoint Mountpoint - defines a full path to a directory on the workstation where the data storage shall be mounted. Options Description Required options -f / --file File System mode. In this mode, all available File Storages will be mounted into the mountpoint. This option is mutually exclusive with -b option -b / --bucket Object Storage mode. In this mode, the specified Object Storage will be mounted into the mountpoint. This option is mutually exclusive with -f option Non-required options -m / --mode Allows to set a mask that defines access permissions (to the owner / group / others ). This mask is set in the numerical view (three-digit octal number) -o / --options Allows to specify any mount options supported by underlying FUSE implementation -l / --log-file If set, standard/error output of mount operations will be written into the specified file -q / --quiet Quiet mode -t / --threads Enable multithreading - allows several processes simultaneously interact with the mount point In the example below we will mount the Object Storage named objstor into the directory \" mountdir \" and specify the file \" mount.log \" as log-file for mount operations: pipe storage mount -l ~/mount.log -b objstor ~/mountdir In the example below we will mount the Object Storage named objectdatastorage into the directory \" mountdir \" and set full access permissons (read, write, execute) to the owner, read and execute permissions to the group and no permissions to others: pipe storage mount -b objectdatastorage -m 750 ~/mountdir Note : if -m option isn't specified, during mounting the default permissions mask will be set - 700 (full access to the owner and no permissions to the group and others)","title":"Mount a storage"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#unmount-a-storage","text":"The command to unmount a mountpoint: pipe storage umount [OPTIONS] Mountpoint Mountpoint - defines a full path to a directory on the workstation where the data storage was mounted. Options Description Non-required options -q / --quiet Quiet mode In the example below we will unmount a storage from the directory \" mountdir \": pipe storage umount ~/mountdir","title":"Unmount a storage"},{"location":"manual/14_CLI/14.4._View_pipeline_definitions_via_CLI/","text":"14.4. View pipeline definitions via CLI The command to view pipeline definitions: pipe view-pipes [OPTIONS] [PIPELINE] PIPELINE - pipeline name or ID. Options Description Non-required options -v / --versions List versions of a pipeline -p / --parameters List parameters of a pipeline -s / --storage-rules List storage rules of a pipeline -r / --permissions List user permissions of a pipeline Without any arguments that command will output the list of all pipelines, e.g.: With specifying pipeline name/ID that command will output the definition of a specific pipeline. E.g., to view info about the pipeline with ID 3212 : pipe view-pipes 3212 To view pipeline versions, parameter list and storage rules - use -v , -p and -s options accordingly: Note : you can view pipeline parameter list by another command - run -n Pipeline name/ID -p . See more details here . To view permissions on a specific pipeline - use the -r option: Note : you can view pipeline permissions by another command - view-acl -t pipeline Pipeline name/ID . See more details here .","title":"14.4. View pipeline definitions"},{"location":"manual/14_CLI/14.4._View_pipeline_definitions_via_CLI/#144-view-pipeline-definitions-via-cli","text":"The command to view pipeline definitions: pipe view-pipes [OPTIONS] [PIPELINE] PIPELINE - pipeline name or ID. Options Description Non-required options -v / --versions List versions of a pipeline -p / --parameters List parameters of a pipeline -s / --storage-rules List storage rules of a pipeline -r / --permissions List user permissions of a pipeline Without any arguments that command will output the list of all pipelines, e.g.: With specifying pipeline name/ID that command will output the definition of a specific pipeline. E.g., to view info about the pipeline with ID 3212 : pipe view-pipes 3212 To view pipeline versions, parameter list and storage rules - use -v , -p and -s options accordingly: Note : you can view pipeline parameter list by another command - run -n Pipeline name/ID -p . See more details here . To view permissions on a specific pipeline - use the -r option: Note : you can view pipeline permissions by another command - view-acl -t pipeline Pipeline name/ID . See more details here .","title":"14.4. View pipeline definitions via CLI"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/","text":"14.5. Manage pipeline executions via CLI View pipeline runs Schedule a pipeline execution Change execution environment Change advanced options Setting parameters for a launch Launch a cluster Launch a pipeline in a synchronized mode Launch a job on the existing running instance Run a tool Generate pipeline launch command via the GUI Runs sharing View sharing status Share a run Unshare a run Run a single command or an interactive session over the SSH protocol Stop a pipeline execution Terminate a node Cloud Pipeline CLI has to be installed. See 14. Command-line interface (CLI) . View pipeline runs The command to view runs info: pipe view-runs [OPTIONS] [RUN_ID] Options Description Non-required options -s / --status [ANY | FAILURE | PAUSED | PAUSING | RESUMING | RUNNING | STOPPED | SUCCESS] List pipeline runs with a specific status -df / --date-from List pipeline runs started after specified date -dt / --date-to List pipeline runs completed before specified date -p / --pipeline List history of runs for a specific pipeline -pid / --parent-id List runs for a specific parent pipeline run -f / --find Search runs with a specific substring in run parameters values -t / --top Display top N records -nd / --node-details Display node details -pd / --parameters-details Display parameters -td / --tasks-details Display tasks That command without any arguments will list all active runs available for the current user (more about permissions see here ): To view information about specific run - enter its RunID at the end of the command, e.g.: You can additionally display information about instance used by a specific run - by the -nd ( --node-details ) flag: You can additionally display information about pipeline parameters used by a specific run - by the -pd ( --parameters-details ) flag: You can additionally display information about run tasks - by the -td ( --tasks-details ) flag: Note : by default, the view-runs command outputs information about active runs. To view a list of all runs with a specific status (see possible values in the table above) - use the -s ( --status ) option, e.g. to view a list of failed runs: pipe view-runs -s FAILURE By default, only last 100 records are displayed. If you want to change this count, use the -t ( --top ) option with specifying a number of records to show, e.g.: Option -df ( --date-from ) allows to output runs started after specified datetime. Datetime shall be specified in one of the following formats: yyyy-MM-dd HH:mm:ss or yyyy-MM-dd , e.g.: Option -dt ( --date-to ) allows to output runs completed before specified datetime. Note : since the view-runs command outputs by default information about active runs, then for using that command with the -dt option one of the completed statuses must be specified, e.g.: To list history of runs for a specific pipeline use the -p ( --pipeline ) option and a pipeline name. By default it will print a list of active runs for the latest pipeline version. If you want to launch non-latest pipeline version, specify version name after the pipeline name using the @ symbol, e.g. to list STOPPED runs for the v1 version of the simplepipeline pipeline: pipe view-runs -p simplepipeline@v1 -s STOPPED To list child runs use the -pid ( --parent-id ) option and a parent run ID, e.g.: To find runs with a specific substring in run parameters values use the -f ( --find ) option. E.g. to find among all stopped runs only those which parameter values contain the word \"Changed\": pipe view-runs -s STOPPED -f Changed Schedule a pipeline execution The command to shedule a pipeline/version execution: pipe run [OPTIONS] [RUN_PARAMETERS] Options Description Non-required options -n / --pipeline Pipeline name or ID -c / --config Pipeline configuration name -di / --docker-image Docker image -it / --instance-type Node type in terms of the Cloud Provider -id / --instance-disk Instance disk size -ic / --instance-count Number of worker instances to launch in a cluster -nc / --cores Number of cores that a cluster shall contain. This option will be ignored if -ic ( --instance-count ) option was specified -r / --region-id Instance Cloud region -pt / --price-type [spot | on-demand] Price type -t / --timeout Timeout (in minutes), when elapsed - run will be stopped -cmd / --cmd-template Command template -p / --parameters Returns a parameter list for the specified pipeline -pn / --parent-node Parent instance Run ID. Allows to run a pipeline as a child job on the existing running instance -s / --sync Allows a pipeline to be run in a sync mode. When set - terminal will be blocked until the pipeline won't finish the run. After that the pipeline's finish status will be returned into the terminal output and the terminal will be unblocked -np / --non-pause Allows to switch off the auto-pause option. Note : this option is supported for on-demand runs only -y / --yes Do not ask confirmation -q / --quiet Quiet mode RUN_PARAMETERS - list of the pipeline parameters to set. Can be specified as a single -- Parameter name value pair or a list of them. In the example below the pipeline with the name simplepipeline will be launched: pipe run -n simplepipeline -y And then we'll check that it was launched by the view-runs command: If not additionally specified, described command will launch default configuration of the pipeline latest version. If you want to launch non-default pipeline configuration, specify its name with the -c ( --config ) option, e.g.: pipe run -n simplepipeline -c new-config -y In the example above the configuration with the name new-config of the pipeline simplepipeline was launched. If you want to launch non-latest pipeline version, specify version name after the pipeline name using the @ symbol, e.g.: pipe run -n simplepipeline@v1 -y In the example above default configuration of the version with the name v1 of the pipeline simplepipeline was launched. Change execution environment You can change execution environment for a pipeline run by setting the following options: -di ( --docker-image ), -it ( --instance-type ), -id ( --instance-disk ), -r ( --region-id ). In the example below the pipeline simplepipeline will be launched on the instance n1-highcpu-2 with the disk size 25 Gb: pipe run -n simplepipeline --instance-type n1-highcpu-2 --instance-disk 25 -y Note : for the -di ( --docker-image ) option shall specify a full path to the Docker image including its version. Change advanced options You can change advanced options for a pipeline execution by setting the following options: -pt ( --price-type ), -t ( --timeout ), -cmd ( --cmd-template ). E.g. the command below will launch the pipeline simplepipeline with the spot price type and changed command template: pipe run -n simplepipeline -pt spot -cmd sleep 1d Note : -pt flag could take only one from two values - on-demand and spot , independent on the Cloud Provider. Setting parameters for a launch To view all parameter list for a pipeline use -p ( --parameters ) flag, e.g. to view parameters of the simplepipeline pipeline: pipe run -n simplepipeline -p As you can see, that flag allows to view all parameters with their types. If parameters have default values they will be also printed. To set parameters for a pipeline launch, enter them after all options in the following manner: -- Parameter1 name value -- Parameter2 name value ... , e.g.: pipe run -n simplepipeline --booleanParamExample false --stringParamExample Changed test value -y Launch a cluster You can launch a cluster using -ic ( --instance-count ) option. It sets a number of worker instances. In the example below the cluster with two child nodes will be launched: pipe run -n simplepipeline -ic 2 Also you can launch a cluster using the -nc ( --number-cores ) option. With this option you specify a number of cores that a cluster shall contain. In that case worker instance count of the cluster will be calculated automatically based on the supported instance types. Note : This option will be ignored if -ic ( --instance-count ) option was specified. In the example below the cluster runs with 16 cores will be launched: pipe run -n simplepipeline -nc 16 As you can see, for that run single instance with 16 cores was scheduled. Launch a pipeline in a synchronized mode You can launch a pipeline in a synchronized mode by setting the -s ( --sync ) flag. In that mode terminal will be blocked until the pipeline won't finish the run. After the pipeline's finish, its status will be returned into the terminal output and the terminal will be unblocked. Example of the pipeline simplepipeline execution in a sync mode: pipe run -n simplepipeline --sync -y Launch a job on the existing running instance You can launch a pipeline as a child job on the existing running instance by setting the -pn ( --parent-node ) option. It allows not to initialize a new node for a such job but use already existing one. Example of the launching the pipeline simplepipeline as a child of the previously launched and initialized instance: pipe run -n simplepipeline --parent-node Parent RunID -q -y Run a tool Also you can launch a tool - without providing a pipeline name/ID. In that case Docker image shall be mandatory specified. If instance type, instance disk and cmd template aren't specified, default tool settings for them will be used. In the example below we will launch the Docker image ubuntu:latest on the n1-highcpu-2 instance with the disk size 27 GB and sleep infinity command template: pipe run -di Docker image path -it n1-highcpu-2 -id 27 -cmd sleep infinity -y Note : Docker image path shall be specified in a full manner including version. Generate pipeline launch command via the GUI The construction of the correct run command sometimes could be hard for users, so users can generate necessary launch commands via the GUI. For that: Via the GUI open the Launch page of a pipeline/tool you want to run At the Launch page, configure all settings for the run as you want and then click the \" CLI command \" button in the right-upper corner: In the appeared popup copy the pipe run command, e.g.: Paste the copied command into the terminal and perform it Note : instead of step 2 described above, you may open the Run logs page of any active/completed run and click the LAUNCH COMMAND button in the right-upper corner to view the launch command of the selected run: Also, user can select the API tab in the \"Launch commands\" popup and get the POST request for a job launch: Runs sharing Note : to share a run with other users/groups, user shall be the OWNER of that run or has an ADMIN role. About sharing run via the GUI see here . View sharing status The command to view for whom a run is shared: pipe share get RUN_ID The example below lists users/groups for whom the run with ID 38081 is shared: pipe share get 38081 Share a run The command to share a running job with other users/groups: pipe share add RUN_ID [OPTIONS] Options Description -su / --shared-user Specifies the user for whom the run will be shared. Multiple options are supported -sg / --shared-group Specifies the group/role for which the run will be shared. Multiple options are supported -ssh / --share-ssh Share SSH-session of the run. Non-required option for the runs with endpoints One of the option -su or -sg shall be necessarily specified. Both options also can be used simultaneously. -ssh option is necessarily required only in cases when the run hasn't endpoints. In the example below we will share the SSH-session of the run with ID 38081 with the user USER1 and then check it by the command pipe share get : pipe share add 38081 -ssh -su USER1 Unshare a run The command to unshare a running job from other users/groups for whom the access was shared before: pipe share remove RUN_ID [OPTIONS] Options Description Non-required -su / --shared-user Specifies the user for whom the sharing of the run will be disabled. Multiple options are supported -sg / --shared-group Specifies the group/role for which the sharing of the run will be disabled. Multiple options are supported -ssh / --share-ssh Remove only SSH-sharing for all users/groups for runs with endpoint(s) or remove all users/groups of the run shared list for runs without endpoints Without any additional options the command will remove all sharings of the run. In the example below we will unshare the run with ID 38081 with the role ROLE_USER and check it by the command pipe share get : pipe share remove 38081 -sg ROLE_USER Run a single command or an interactive session over the SSH protocol Note : to perform a command or run an interactive session over the running job, user shall be the OWNER of that job or has an ADMIN role. The command to run a single command or an interactive session over the SSH protocol for the specified job run: pipe ssh RUN_ID [COMMAND] COMMAND - a single command to execute over the running instance with the specified Run ID using the SSH protocol. In the example below we will list the content of the root directory of the job run with ID 12370 : pipe ssh 12370 'ls -l /root' If COMMAND isn't specified, it will start an interactive session over the running instance with the specified Run ID using the SSH protocol. To exit from the interactive session use commands exit or logout . We will perform the same command as was in the example above but in the interactive session: Stop a pipeline execution The command to stop a specific running pipeline: pipe stop [OPTIONS] RUN_ID Options Description Non-required options -y / --yes Do not ask confirmation In the example below we will run a pipeline and then stop it: Terminate a node The command to terminate a specific calculation node: pipe terminate-node [OPTIONS] NODE_NAME Options Description Non-required options -y / --yes Do not ask confirmation NODE_NAME - calculation node name (ID). You can know it, for example, by the view-runs command with the -nd option, the view-cluster command or via the GUI. In the example below we will terminate a node of the running pipeline:","title":"14.5. Manage pipeline executions"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#145-manage-pipeline-executions-via-cli","text":"View pipeline runs Schedule a pipeline execution Change execution environment Change advanced options Setting parameters for a launch Launch a cluster Launch a pipeline in a synchronized mode Launch a job on the existing running instance Run a tool Generate pipeline launch command via the GUI Runs sharing View sharing status Share a run Unshare a run Run a single command or an interactive session over the SSH protocol Stop a pipeline execution Terminate a node Cloud Pipeline CLI has to be installed. See 14. Command-line interface (CLI) .","title":"14.5. Manage pipeline executions via CLI"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#view-pipeline-runs","text":"The command to view runs info: pipe view-runs [OPTIONS] [RUN_ID] Options Description Non-required options -s / --status [ANY | FAILURE | PAUSED | PAUSING | RESUMING | RUNNING | STOPPED | SUCCESS] List pipeline runs with a specific status -df / --date-from List pipeline runs started after specified date -dt / --date-to List pipeline runs completed before specified date -p / --pipeline List history of runs for a specific pipeline -pid / --parent-id List runs for a specific parent pipeline run -f / --find Search runs with a specific substring in run parameters values -t / --top Display top N records -nd / --node-details Display node details -pd / --parameters-details Display parameters -td / --tasks-details Display tasks That command without any arguments will list all active runs available for the current user (more about permissions see here ): To view information about specific run - enter its RunID at the end of the command, e.g.: You can additionally display information about instance used by a specific run - by the -nd ( --node-details ) flag: You can additionally display information about pipeline parameters used by a specific run - by the -pd ( --parameters-details ) flag: You can additionally display information about run tasks - by the -td ( --tasks-details ) flag: Note : by default, the view-runs command outputs information about active runs. To view a list of all runs with a specific status (see possible values in the table above) - use the -s ( --status ) option, e.g. to view a list of failed runs: pipe view-runs -s FAILURE By default, only last 100 records are displayed. If you want to change this count, use the -t ( --top ) option with specifying a number of records to show, e.g.: Option -df ( --date-from ) allows to output runs started after specified datetime. Datetime shall be specified in one of the following formats: yyyy-MM-dd HH:mm:ss or yyyy-MM-dd , e.g.: Option -dt ( --date-to ) allows to output runs completed before specified datetime. Note : since the view-runs command outputs by default information about active runs, then for using that command with the -dt option one of the completed statuses must be specified, e.g.: To list history of runs for a specific pipeline use the -p ( --pipeline ) option and a pipeline name. By default it will print a list of active runs for the latest pipeline version. If you want to launch non-latest pipeline version, specify version name after the pipeline name using the @ symbol, e.g. to list STOPPED runs for the v1 version of the simplepipeline pipeline: pipe view-runs -p simplepipeline@v1 -s STOPPED To list child runs use the -pid ( --parent-id ) option and a parent run ID, e.g.: To find runs with a specific substring in run parameters values use the -f ( --find ) option. E.g. to find among all stopped runs only those which parameter values contain the word \"Changed\": pipe view-runs -s STOPPED -f Changed","title":"View pipeline runs"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#schedule-a-pipeline-execution","text":"The command to shedule a pipeline/version execution: pipe run [OPTIONS] [RUN_PARAMETERS] Options Description Non-required options -n / --pipeline Pipeline name or ID -c / --config Pipeline configuration name -di / --docker-image Docker image -it / --instance-type Node type in terms of the Cloud Provider -id / --instance-disk Instance disk size -ic / --instance-count Number of worker instances to launch in a cluster -nc / --cores Number of cores that a cluster shall contain. This option will be ignored if -ic ( --instance-count ) option was specified -r / --region-id Instance Cloud region -pt / --price-type [spot | on-demand] Price type -t / --timeout Timeout (in minutes), when elapsed - run will be stopped -cmd / --cmd-template Command template -p / --parameters Returns a parameter list for the specified pipeline -pn / --parent-node Parent instance Run ID. Allows to run a pipeline as a child job on the existing running instance -s / --sync Allows a pipeline to be run in a sync mode. When set - terminal will be blocked until the pipeline won't finish the run. After that the pipeline's finish status will be returned into the terminal output and the terminal will be unblocked -np / --non-pause Allows to switch off the auto-pause option. Note : this option is supported for on-demand runs only -y / --yes Do not ask confirmation -q / --quiet Quiet mode RUN_PARAMETERS - list of the pipeline parameters to set. Can be specified as a single -- Parameter name value pair or a list of them. In the example below the pipeline with the name simplepipeline will be launched: pipe run -n simplepipeline -y And then we'll check that it was launched by the view-runs command: If not additionally specified, described command will launch default configuration of the pipeline latest version. If you want to launch non-default pipeline configuration, specify its name with the -c ( --config ) option, e.g.: pipe run -n simplepipeline -c new-config -y In the example above the configuration with the name new-config of the pipeline simplepipeline was launched. If you want to launch non-latest pipeline version, specify version name after the pipeline name using the @ symbol, e.g.: pipe run -n simplepipeline@v1 -y In the example above default configuration of the version with the name v1 of the pipeline simplepipeline was launched.","title":"Schedule a pipeline execution"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#change-execution-environment","text":"You can change execution environment for a pipeline run by setting the following options: -di ( --docker-image ), -it ( --instance-type ), -id ( --instance-disk ), -r ( --region-id ). In the example below the pipeline simplepipeline will be launched on the instance n1-highcpu-2 with the disk size 25 Gb: pipe run -n simplepipeline --instance-type n1-highcpu-2 --instance-disk 25 -y Note : for the -di ( --docker-image ) option shall specify a full path to the Docker image including its version.","title":"Change execution environment"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#change-advanced-options","text":"You can change advanced options for a pipeline execution by setting the following options: -pt ( --price-type ), -t ( --timeout ), -cmd ( --cmd-template ). E.g. the command below will launch the pipeline simplepipeline with the spot price type and changed command template: pipe run -n simplepipeline -pt spot -cmd sleep 1d Note : -pt flag could take only one from two values - on-demand and spot , independent on the Cloud Provider.","title":"Change advanced options"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#setting-parameters-for-a-launch","text":"To view all parameter list for a pipeline use -p ( --parameters ) flag, e.g. to view parameters of the simplepipeline pipeline: pipe run -n simplepipeline -p As you can see, that flag allows to view all parameters with their types. If parameters have default values they will be also printed. To set parameters for a pipeline launch, enter them after all options in the following manner: -- Parameter1 name value -- Parameter2 name value ... , e.g.: pipe run -n simplepipeline --booleanParamExample false --stringParamExample Changed test value -y","title":"Setting parameters for a launch"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#launch-a-cluster","text":"You can launch a cluster using -ic ( --instance-count ) option. It sets a number of worker instances. In the example below the cluster with two child nodes will be launched: pipe run -n simplepipeline -ic 2 Also you can launch a cluster using the -nc ( --number-cores ) option. With this option you specify a number of cores that a cluster shall contain. In that case worker instance count of the cluster will be calculated automatically based on the supported instance types. Note : This option will be ignored if -ic ( --instance-count ) option was specified. In the example below the cluster runs with 16 cores will be launched: pipe run -n simplepipeline -nc 16 As you can see, for that run single instance with 16 cores was scheduled.","title":"Launch a cluster"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#launch-a-pipeline-in-a-synchronized-mode","text":"You can launch a pipeline in a synchronized mode by setting the -s ( --sync ) flag. In that mode terminal will be blocked until the pipeline won't finish the run. After the pipeline's finish, its status will be returned into the terminal output and the terminal will be unblocked. Example of the pipeline simplepipeline execution in a sync mode: pipe run -n simplepipeline --sync -y","title":"Launch a pipeline in a synchronized mode"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#launch-a-job-on-the-existing-running-instance","text":"You can launch a pipeline as a child job on the existing running instance by setting the -pn ( --parent-node ) option. It allows not to initialize a new node for a such job but use already existing one. Example of the launching the pipeline simplepipeline as a child of the previously launched and initialized instance: pipe run -n simplepipeline --parent-node Parent RunID -q -y","title":"Launch a job on the existing running instance"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#run-a-tool","text":"Also you can launch a tool - without providing a pipeline name/ID. In that case Docker image shall be mandatory specified. If instance type, instance disk and cmd template aren't specified, default tool settings for them will be used. In the example below we will launch the Docker image ubuntu:latest on the n1-highcpu-2 instance with the disk size 27 GB and sleep infinity command template: pipe run -di Docker image path -it n1-highcpu-2 -id 27 -cmd sleep infinity -y Note : Docker image path shall be specified in a full manner including version.","title":"Run a tool"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#generate-pipeline-launch-command-via-the-gui","text":"The construction of the correct run command sometimes could be hard for users, so users can generate necessary launch commands via the GUI. For that: Via the GUI open the Launch page of a pipeline/tool you want to run At the Launch page, configure all settings for the run as you want and then click the \" CLI command \" button in the right-upper corner: In the appeared popup copy the pipe run command, e.g.: Paste the copied command into the terminal and perform it Note : instead of step 2 described above, you may open the Run logs page of any active/completed run and click the LAUNCH COMMAND button in the right-upper corner to view the launch command of the selected run: Also, user can select the API tab in the \"Launch commands\" popup and get the POST request for a job launch:","title":"Generate pipeline launch command via the GUI"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#runs-sharing","text":"Note : to share a run with other users/groups, user shall be the OWNER of that run or has an ADMIN role. About sharing run via the GUI see here .","title":"Runs sharing"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#view-sharing-status","text":"The command to view for whom a run is shared: pipe share get RUN_ID The example below lists users/groups for whom the run with ID 38081 is shared: pipe share get 38081","title":"View sharing status"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#share-a-run","text":"The command to share a running job with other users/groups: pipe share add RUN_ID [OPTIONS] Options Description -su / --shared-user Specifies the user for whom the run will be shared. Multiple options are supported -sg / --shared-group Specifies the group/role for which the run will be shared. Multiple options are supported -ssh / --share-ssh Share SSH-session of the run. Non-required option for the runs with endpoints One of the option -su or -sg shall be necessarily specified. Both options also can be used simultaneously. -ssh option is necessarily required only in cases when the run hasn't endpoints. In the example below we will share the SSH-session of the run with ID 38081 with the user USER1 and then check it by the command pipe share get : pipe share add 38081 -ssh -su USER1","title":"Share a run"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#unshare-a-run","text":"The command to unshare a running job from other users/groups for whom the access was shared before: pipe share remove RUN_ID [OPTIONS] Options Description Non-required -su / --shared-user Specifies the user for whom the sharing of the run will be disabled. Multiple options are supported -sg / --shared-group Specifies the group/role for which the sharing of the run will be disabled. Multiple options are supported -ssh / --share-ssh Remove only SSH-sharing for all users/groups for runs with endpoint(s) or remove all users/groups of the run shared list for runs without endpoints Without any additional options the command will remove all sharings of the run. In the example below we will unshare the run with ID 38081 with the role ROLE_USER and check it by the command pipe share get : pipe share remove 38081 -sg ROLE_USER","title":"Unshare a run"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#run-a-single-command-or-an-interactive-session-over-the-ssh-protocol","text":"Note : to perform a command or run an interactive session over the running job, user shall be the OWNER of that job or has an ADMIN role. The command to run a single command or an interactive session over the SSH protocol for the specified job run: pipe ssh RUN_ID [COMMAND] COMMAND - a single command to execute over the running instance with the specified Run ID using the SSH protocol. In the example below we will list the content of the root directory of the job run with ID 12370 : pipe ssh 12370 'ls -l /root' If COMMAND isn't specified, it will start an interactive session over the running instance with the specified Run ID using the SSH protocol. To exit from the interactive session use commands exit or logout . We will perform the same command as was in the example above but in the interactive session:","title":"Run a single command or an interactive session over the SSH protocol"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#stop-a-pipeline-execution","text":"The command to stop a specific running pipeline: pipe stop [OPTIONS] RUN_ID Options Description Non-required options -y / --yes Do not ask confirmation In the example below we will run a pipeline and then stop it:","title":"Stop a pipeline execution"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#terminate-a-node","text":"The command to terminate a specific calculation node: pipe terminate-node [OPTIONS] NODE_NAME Options Description Non-required options -y / --yes Do not ask confirmation NODE_NAME - calculation node name (ID). You can know it, for example, by the view-runs command with the -nd option, the view-cluster command or via the GUI. In the example below we will terminate a node of the running pipeline:","title":"Terminate a node"},{"location":"manual/14_CLI/14.6._View_cluster_nodes_via_CLI/","text":"14.6. View cluster nodes via CLI The command to view working nodes info: pipe view-cluster [NODE_NAME] NODE_NAME - calculation node name (ID). That command without any arguments will list all working nodes: Or you can view full information about specific node by specifying its name/ID at the end of the command, e.g.:","title":"14.6. View cluster nodes"},{"location":"manual/14_CLI/14.6._View_cluster_nodes_via_CLI/#146-view-cluster-nodes-via-cli","text":"The command to view working nodes info: pipe view-cluster [NODE_NAME] NODE_NAME - calculation node name (ID). That command without any arguments will list all working nodes: Or you can view full information about specific node by specifying its name/ID at the end of the command, e.g.:","title":"14.6. View cluster nodes via CLI"},{"location":"manual/14_CLI/14.7._View_and_manage_Permissions_via_CLI/","text":"14.7. View and manage Permissions via CLI View permissions Manage permissions Example: set permissions for a folder Example: set permissions for a pipeline View the list of objects accessible by a user View the list of objects accessible by a group Change OWNER property Cloud Pipeline CLI has to be installed. See 14. Command-line interface (CLI) . View permissions To view permissions for the object you need READ permission for the object. See 13. Permissions . Command to list all permissions for a specific object: pipe view-acl -t|--object-type Object type Object id/name Two parameters are required: Object type - defines a name of the object class. Possible values: pipeline , folder , data_storage . Object id/name - defines a name of an object of a specified class. Note : full path to the object has to be specified. In the example below we check permissions for the folder \" workfolder/manage-permissions-folder \": pipe view-acl -t folder workfolder/manage-permissions-folder To check permissions for the Data Storage with ID 3976 : pipe view-acl --object-type data_storage 3976 Manage permissions To manage permissions for the object you need to be an OWNER of that object or you need to have the ADMIN role. See 13. Permissions . Command to set permissions for the object: pipe set-acl -t|--object-type Object type -s|--sid User/Group name [-g|--group] -a|--allow/-d|--deny/-i|--inherit w / x / r Object id/name The following parameters are required: Object type - defines a name of the object class. Possible values: pipeline , folder , data_storage . User or Group name - defines a name of an user or a group (role) for whom permissions will be set. Note : the option -g (or --group ) shall be necessarily specified when permissions are being set for a group (role) Allow ( -a or --allow ), Deny ( -d or --deny ), Inherit ( -i or --inherit ) - actions that could be performed with permissions. WRITE ( w ), READ ( r ) and EXECUTE ( x ) - permissions for setting. Object id or name - defines an ID or name of an object of the specified class to set permissions for. Note : full path to the object has to be specified if the name is not unique (in cases for Data Storage , Pipeline ). Note : permissions and actions over them could be written in command in any combinations. See examples below. Example: set permissions for a folder Here we demonstrate how to set permissions for a folder . You can set permissions for other CP objects in the same way. In the example below we grant the user USER3 READ access and deny WRITE and EXECUTE access to the directory \" workfolder/manage-permissions-folder \". pipe set-acl -t folder -s USER3 -d wx -a r workfolder/manage-permissions-folder Example: set permissions for a pipeline In the example below we grant the role ROLE_USER READ and WRITE access to the pipeline with ID 5937 . pipe set-acl --object-type pipeline --sid ROLE_USER --group --allow rw 5937 View the list of objects accessible by a user To view objects accessible for a user you shall have the ROLE_ADMIN role. The command to view the full list of objects accessible by a user: pipe view-user-objects Username [OPTIONS] Where Username defines the name of the user for which you wish to view the accessible object list. Options Description Non-required options -t / --object-type OBJECT_TYPE Defines a name of the object class. If specified, the command output will contain only the list of accessible objects of the specific type by a user. Possible values: pipeline , folder , data_storage , configuration , docker_registry , tool , tool_group In the example below we'll print the list of objects accessible to the user user3 : pipe view-user-objects user3 In the example below we'll print the list of pipelines accessible to the user demo : pipe view-user-objects -t pipeline demo View the list of objects accessible by a group To view objects accessible for a group you shall have the ROLE_ADMIN role. The command to view the full list of objects accessible by a user group/role: pipe view-group-objects Groupname [OPTIONS] Where Groupname defines the name of the user group/role for which you wish to view the accessible object list. Options Description Non-required options -t / --object-type OBJECT_TYPE Defines a name of the object class. If specified, the command output will contain only the list of accessible objects of the specific type by a user group. Possible values: pipeline , folder , data_storage , configuration , docker_registry , tool , tool_group In the example below we'll print the list of objects accessible to the role ROLE_USER : pipe view-group-objects ROLE_USER In the example below we'll print the list of pipelines accessible to the group LIBRARY : pipe view-group-objects -t pipeline ROLE_LIBRARY Change OWNER property Each object has a mandatory OWNER property. You can change an owner of the Cloud Pipeline object via CLI. Please note, for do that, you shall be an object OWNER or have the ROLE_ADMIN role. Note : how to change an object owner via the GUI see here . Command to change an owner of the object: pipe chown User name Object class Object id/name Three parameters shall be specified: User name - defines a user name of a desired object owner. Object class - defines a name of the object class. Possible values: data_storage , docker_registry , folder , pipeline , tool , tool_group , configuration . Object id/name - defines an ID or name of an object of the specified object class. Note : full path to the object has to be specified. Paths to Docker registry and Tool objects should include registry IP address. The example below will change an owner to USER3 for the pipeline with ID 5937 : pipe chown USER3 pipeline 5937","title":"14.7. View and manage Permissions"},{"location":"manual/14_CLI/14.7._View_and_manage_Permissions_via_CLI/#147-view-and-manage-permissions-via-cli","text":"View permissions Manage permissions Example: set permissions for a folder Example: set permissions for a pipeline View the list of objects accessible by a user View the list of objects accessible by a group Change OWNER property Cloud Pipeline CLI has to be installed. See 14. Command-line interface (CLI) .","title":"14.7. View and manage Permissions via CLI"},{"location":"manual/14_CLI/14.7._View_and_manage_Permissions_via_CLI/#view-permissions","text":"To view permissions for the object you need READ permission for the object. See 13. Permissions . Command to list all permissions for a specific object: pipe view-acl -t|--object-type Object type Object id/name Two parameters are required: Object type - defines a name of the object class. Possible values: pipeline , folder , data_storage . Object id/name - defines a name of an object of a specified class. Note : full path to the object has to be specified. In the example below we check permissions for the folder \" workfolder/manage-permissions-folder \": pipe view-acl -t folder workfolder/manage-permissions-folder To check permissions for the Data Storage with ID 3976 : pipe view-acl --object-type data_storage 3976","title":"View permissions"},{"location":"manual/14_CLI/14.7._View_and_manage_Permissions_via_CLI/#manage-permissions","text":"To manage permissions for the object you need to be an OWNER of that object or you need to have the ADMIN role. See 13. Permissions . Command to set permissions for the object: pipe set-acl -t|--object-type Object type -s|--sid User/Group name [-g|--group] -a|--allow/-d|--deny/-i|--inherit w / x / r Object id/name The following parameters are required: Object type - defines a name of the object class. Possible values: pipeline , folder , data_storage . User or Group name - defines a name of an user or a group (role) for whom permissions will be set. Note : the option -g (or --group ) shall be necessarily specified when permissions are being set for a group (role) Allow ( -a or --allow ), Deny ( -d or --deny ), Inherit ( -i or --inherit ) - actions that could be performed with permissions. WRITE ( w ), READ ( r ) and EXECUTE ( x ) - permissions for setting. Object id or name - defines an ID or name of an object of the specified class to set permissions for. Note : full path to the object has to be specified if the name is not unique (in cases for Data Storage , Pipeline ). Note : permissions and actions over them could be written in command in any combinations. See examples below.","title":"Manage permissions"},{"location":"manual/14_CLI/14.7._View_and_manage_Permissions_via_CLI/#example-set-permissions-for-a-folder","text":"Here we demonstrate how to set permissions for a folder . You can set permissions for other CP objects in the same way. In the example below we grant the user USER3 READ access and deny WRITE and EXECUTE access to the directory \" workfolder/manage-permissions-folder \". pipe set-acl -t folder -s USER3 -d wx -a r workfolder/manage-permissions-folder","title":"Example: set permissions for a folder"},{"location":"manual/14_CLI/14.7._View_and_manage_Permissions_via_CLI/#example-set-permissions-for-a-pipeline","text":"In the example below we grant the role ROLE_USER READ and WRITE access to the pipeline with ID 5937 . pipe set-acl --object-type pipeline --sid ROLE_USER --group --allow rw 5937","title":"Example: set permissions for a pipeline"},{"location":"manual/14_CLI/14.7._View_and_manage_Permissions_via_CLI/#view-the-list-of-objects-accessible-by-a-user","text":"To view objects accessible for a user you shall have the ROLE_ADMIN role. The command to view the full list of objects accessible by a user: pipe view-user-objects Username [OPTIONS] Where Username defines the name of the user for which you wish to view the accessible object list. Options Description Non-required options -t / --object-type OBJECT_TYPE Defines a name of the object class. If specified, the command output will contain only the list of accessible objects of the specific type by a user. Possible values: pipeline , folder , data_storage , configuration , docker_registry , tool , tool_group In the example below we'll print the list of objects accessible to the user user3 : pipe view-user-objects user3 In the example below we'll print the list of pipelines accessible to the user demo : pipe view-user-objects -t pipeline demo","title":"View the list of objects accessible by a user"},{"location":"manual/14_CLI/14.7._View_and_manage_Permissions_via_CLI/#view-the-list-of-objects-accessible-by-a-group","text":"To view objects accessible for a group you shall have the ROLE_ADMIN role. The command to view the full list of objects accessible by a user group/role: pipe view-group-objects Groupname [OPTIONS] Where Groupname defines the name of the user group/role for which you wish to view the accessible object list. Options Description Non-required options -t / --object-type OBJECT_TYPE Defines a name of the object class. If specified, the command output will contain only the list of accessible objects of the specific type by a user group. Possible values: pipeline , folder , data_storage , configuration , docker_registry , tool , tool_group In the example below we'll print the list of objects accessible to the role ROLE_USER : pipe view-group-objects ROLE_USER In the example below we'll print the list of pipelines accessible to the group LIBRARY : pipe view-group-objects -t pipeline ROLE_LIBRARY","title":"View the list of objects accessible by a group"},{"location":"manual/14_CLI/14.7._View_and_manage_Permissions_via_CLI/#change-owner-property","text":"Each object has a mandatory OWNER property. You can change an owner of the Cloud Pipeline object via CLI. Please note, for do that, you shall be an object OWNER or have the ROLE_ADMIN role. Note : how to change an object owner via the GUI see here . Command to change an owner of the object: pipe chown User name Object class Object id/name Three parameters shall be specified: User name - defines a user name of a desired object owner. Object class - defines a name of the object class. Possible values: data_storage , docker_registry , folder , pipeline , tool , tool_group , configuration . Object id/name - defines an ID or name of an object of the specified object class. Note : full path to the object has to be specified. Paths to Docker registry and Tool objects should include registry IP address. The example below will change an owner to USER3 for the pipeline with ID 5937 : pipe chown USER3 pipeline 5937","title":"Change OWNER property"},{"location":"manual/14_CLI/14.8._View_tools_definitions_via_CLI/","text":"14.8. View tools definitions via CLI View a list of groups in the registry View a list of tools in the group View a tool definition View tool version's details Using the \"path\" to view the object Via the CLI users can view details of a tool/specific tool version or tools groups. The general command to perform these operations: pipe view-tools [OPTIONS] Options Description Non-required options -r / --registry Defines a specific Docker registry -g / --group Defines a specific tool group in a registry -t / --tool Defines a specific tool in a tool group -v / --version Defines a specific version of a tool Without any arguments that command will output a list of the tools contained in: In a personal tool group If not personal group is available - in the library or default tool group If none of the above is available - a corresponding warning will be printed Note : If more than one registry exists on the current Cloud Pipeline deployment - Docker registry shall be forcibly specified ( -r becomes a mandatory option) or the corresponding error message will be printed. View a list of groups in the registry With specifying a Docker registry that command will output a list of tools groups of that registry. Note : Docker registry shall be specified as registry_name : port . E.g.: pipe view-tools --registry registry_name : port View a list of tools in the group With specifying a tools group that command will output a list of tools in the specific group. Note : If more than one registry exists in the current Cloud Pipeline deployment - Docker registry shall be also specified. E.g.: pipe view-tools [--registry registry_name ] --group group_name In the example above, the list of tools in user's personal tools group was printed. View a tool definition To view a specific tool's definition and the list of tool versions - use the tool name together with its group name. Note : If more than one registry exists in the current Cloud Pipeline deployment - Docker registry shall be also specified. E.g.: pipe view-tools [--registry registry_name ] --group group_name --tool tool_name View tool version's details To view details of a specific tool version - use the tool name together with the version and the group name. Note : If more than one registry exists in the current Cloud Pipeline deployment - Docker registry shall be also specified. E.g.: pipe view-tools [--registry registry_name ] --group group_name --tool tool_name --version version_name Details of a specific tool version contain: tool definition list of the tool version execution settings (if specified) list of the tool version vulnerabilities list of the tool version packages Using the \"path\" to view the object Users also can view definitions via the \"path\" to the object (registry/group/tool). The \"full path\" format is: registry_name : port / group_name / tool_name : verion_name . In that case, the specifying of command options ( -r / -g / -t / -v ) is not required. So: pipe view-tools registry_name : port will show a list of tools groups in the specified registry pipe view-tools registry_name : port / group_name will show a list of tools in the specified group pipe view-tools registry_name : port / group_name / tool_name will show a definition of the specified tool pipe view-tools registry_name : port / group_name / tool_name : verion_name will show details of the specified tool version","title":"14.8. View tools definitions"},{"location":"manual/14_CLI/14.8._View_tools_definitions_via_CLI/#148-view-tools-definitions-via-cli","text":"View a list of groups in the registry View a list of tools in the group View a tool definition View tool version's details Using the \"path\" to view the object Via the CLI users can view details of a tool/specific tool version or tools groups. The general command to perform these operations: pipe view-tools [OPTIONS] Options Description Non-required options -r / --registry Defines a specific Docker registry -g / --group Defines a specific tool group in a registry -t / --tool Defines a specific tool in a tool group -v / --version Defines a specific version of a tool Without any arguments that command will output a list of the tools contained in: In a personal tool group If not personal group is available - in the library or default tool group If none of the above is available - a corresponding warning will be printed Note : If more than one registry exists on the current Cloud Pipeline deployment - Docker registry shall be forcibly specified ( -r becomes a mandatory option) or the corresponding error message will be printed.","title":"14.8. View tools definitions via CLI"},{"location":"manual/14_CLI/14.8._View_tools_definitions_via_CLI/#view-a-list-of-groups-in-the-registry","text":"With specifying a Docker registry that command will output a list of tools groups of that registry. Note : Docker registry shall be specified as registry_name : port . E.g.: pipe view-tools --registry registry_name : port","title":"View a list of groups in the registry"},{"location":"manual/14_CLI/14.8._View_tools_definitions_via_CLI/#view-a-list-of-tools-in-the-group","text":"With specifying a tools group that command will output a list of tools in the specific group. Note : If more than one registry exists in the current Cloud Pipeline deployment - Docker registry shall be also specified. E.g.: pipe view-tools [--registry registry_name ] --group group_name In the example above, the list of tools in user's personal tools group was printed.","title":"View a list of tools in the group"},{"location":"manual/14_CLI/14.8._View_tools_definitions_via_CLI/#view-a-tool-definition","text":"To view a specific tool's definition and the list of tool versions - use the tool name together with its group name. Note : If more than one registry exists in the current Cloud Pipeline deployment - Docker registry shall be also specified. E.g.: pipe view-tools [--registry registry_name ] --group group_name --tool tool_name","title":"View a tool definition"},{"location":"manual/14_CLI/14.8._View_tools_definitions_via_CLI/#view-tool-versions-details","text":"To view details of a specific tool version - use the tool name together with the version and the group name. Note : If more than one registry exists in the current Cloud Pipeline deployment - Docker registry shall be also specified. E.g.: pipe view-tools [--registry registry_name ] --group group_name --tool tool_name --version version_name Details of a specific tool version contain: tool definition list of the tool version execution settings (if specified) list of the tool version vulnerabilities list of the tool version packages","title":"View tool version's details"},{"location":"manual/14_CLI/14.8._View_tools_definitions_via_CLI/#using-the-path-to-view-the-object","text":"Users also can view definitions via the \"path\" to the object (registry/group/tool). The \"full path\" format is: registry_name : port / group_name / tool_name : verion_name . In that case, the specifying of command options ( -r / -g / -t / -v ) is not required. So: pipe view-tools registry_name : port will show a list of tools groups in the specified registry pipe view-tools registry_name : port / group_name will show a list of tools in the specified group pipe view-tools registry_name : port / group_name / tool_name will show a definition of the specified tool pipe view-tools registry_name : port / group_name / tool_name : verion_name will show details of the specified tool version","title":"Using the \"path\" to view the object"},{"location":"manual/14_CLI/14._Command-line_interface/","text":"14. Command-line interface (CLI) Introduction Working with CLI CLI options and commands Examples Cloud Pipeline CLI has to be installed. See 14.1. Install and setup CLI . Introduction Working with a Cloud Pipeline from CLI has numerous benefits compared to GUI: It has extra features which are not accessible from GUI such copying or moving files from one storage to another. The uploading file size could exceed 10 Mb. It is more convenient for System administrators. Working with CLI All CLI commands have to be typed into the command line (Terminal) of the computer. pipe [OPTIONS] COMMAND [ARGS]... The pipe is a command-line interface for Cloud Pipeline. It has a number of available commands. Each command has a number of arguments as an input. To get a list of available options and commands type pipe or pipe --help into the command line. CLI options and commands Options --help Show help message and exit --version Show CLI version and exit. Details of the command using see here Commands chown Changes current owner to specified. Details of the command using see here . configure Configures CLI parameters. This command can be automatically generated . Details of the command using see here . run Schedules a pipeline execution. Details of the command using see here . set-acl Set object permissions. Details of the command using see here . share Allows to share launched run with users/groups. Details of the command using see here . ssh Runs a single command or an interactive session over the SSH protocol for the specified job run. Details of the command using see here . stop Stops a running pipeline. Details of the command using see here . storage Storage operations. Details of the command using see here . tag Operations with attributes. Details of the command using see here . terminate-node Terminates calculation node. Details of the command using see here . token Prints the authentication token for a specified user. Details of the command using see here . update Checks the Cloud Pipeline CLI version and updates it to the latest one if required. Details of the command using see here . view-acl View object permissions. Details of the command using see here . view-cluster Lists cluster nodes. Details of the command using see here . view-group-objects Lists the objects accessible to the specific users group. Details of the command using see here . view-pipes Lists pipelines definitions. Details of the command using see here . view-runs Lists pipeline runs. Details of the command using see here . view-tools Displays details of a tool/tool version. Details of the command using see here . view-user-objects Lists the objects accessible to the specific user. Details of the command using see here . Note : To see command's arguments and options type pipe command --help . Examples To see a list of available CLI commands type pipe or pipe --help in the terminal. Note : each command might have its own set of commands that consequently might have their own set of commands... To learn more about a specific command, type the following in the terminal: pipe COMMAND --help . For instance, we can list a number of pipe storage commands with pipe storage --help . Another example - a user can see a list of pipelines runs by pipe view-runs command.","title":"14.0. Overview"},{"location":"manual/14_CLI/14._Command-line_interface/#14-command-line-interface-cli","text":"Introduction Working with CLI CLI options and commands Examples Cloud Pipeline CLI has to be installed. See 14.1. Install and setup CLI .","title":"14. Command-line interface (CLI)"},{"location":"manual/14_CLI/14._Command-line_interface/#introduction","text":"Working with a Cloud Pipeline from CLI has numerous benefits compared to GUI: It has extra features which are not accessible from GUI such copying or moving files from one storage to another. The uploading file size could exceed 10 Mb. It is more convenient for System administrators.","title":"Introduction"},{"location":"manual/14_CLI/14._Command-line_interface/#working-with-cli","text":"All CLI commands have to be typed into the command line (Terminal) of the computer. pipe [OPTIONS] COMMAND [ARGS]... The pipe is a command-line interface for Cloud Pipeline. It has a number of available commands. Each command has a number of arguments as an input. To get a list of available options and commands type pipe or pipe --help into the command line.","title":"Working with CLI"},{"location":"manual/14_CLI/14._Command-line_interface/#cli-options-and-commands","text":"Options --help Show help message and exit --version Show CLI version and exit. Details of the command using see here Commands chown Changes current owner to specified. Details of the command using see here . configure Configures CLI parameters. This command can be automatically generated . Details of the command using see here . run Schedules a pipeline execution. Details of the command using see here . set-acl Set object permissions. Details of the command using see here . share Allows to share launched run with users/groups. Details of the command using see here . ssh Runs a single command or an interactive session over the SSH protocol for the specified job run. Details of the command using see here . stop Stops a running pipeline. Details of the command using see here . storage Storage operations. Details of the command using see here . tag Operations with attributes. Details of the command using see here . terminate-node Terminates calculation node. Details of the command using see here . token Prints the authentication token for a specified user. Details of the command using see here . update Checks the Cloud Pipeline CLI version and updates it to the latest one if required. Details of the command using see here . view-acl View object permissions. Details of the command using see here . view-cluster Lists cluster nodes. Details of the command using see here . view-group-objects Lists the objects accessible to the specific users group. Details of the command using see here . view-pipes Lists pipelines definitions. Details of the command using see here . view-runs Lists pipeline runs. Details of the command using see here . view-tools Displays details of a tool/tool version. Details of the command using see here . view-user-objects Lists the objects accessible to the specific user. Details of the command using see here . Note : To see command's arguments and options type pipe command --help .","title":"CLI options and commands"},{"location":"manual/14_CLI/14._Command-line_interface/#examples","text":"To see a list of available CLI commands type pipe or pipe --help in the terminal. Note : each command might have its own set of commands that consequently might have their own set of commands... To learn more about a specific command, type the following in the terminal: pipe COMMAND --help . For instance, we can list a number of pipe storage commands with pipe storage --help . Another example - a user can see a list of pipelines runs by pipe view-runs command.","title":"Examples"},{"location":"manual/15_Interactive_services/15.1._Starting_an_Interactive_application/","text":"15.1. Starting an Interactive application Starting a service Terminating a service To run a Tool or a Pipeline as an Interactive service you need to have EXECUTE permissions for that Tool/Pipeline. For more information see 13. Permissions . On this page, you'll find an example of launching an Interactive application. Launching steps remain the same for all applications ( Jupiter Notebook , Rstudio , etc.). All launching steps on this page are illustrated with the example of the Rstudio application. Rstudio is a popular IDE for R language that: is designed to make it easy to write scripts; makes it easy to set your working directory and access files on your computer; makes graphics much more accessible to a casual user. Starting a service Both Pipelines and Tools can be run as interactive services. The example below shows launching Tool scenario. Search for a Tool that implements a service (type a service name in the search box - list will be filtered). Navigate to the Tool information page by clicking the Tool's name and click the Run button then press the OK (for more information see 10.5. Launch a Tool ). Service will begin to set up. A service instance will be shown in the same way as batch jobs - within Active Runs menu. Once an instance is created for a service, a link(s) to the web GUI will be shown within \"Run Log\" form. Clicking the link will load the application web interface. In this example, we show the Rstudio web interface. Service configuration includes the following items: For all out of the box services in the Cloud Pipeline, a user will be automatically authenticated within a service. Note : authentication within all new services added by the users shall be configured by themselves. Below is an example of the authentication within the Rstudio service. Besides, for all out of the box services, you'll also find that all STS data storages, that are available to the user, are available within a service. Data from the STS storages will be available as a local file system and you will be able to work with it just as you do on your laptop. Note : In case of the Rstudio application, you can find all available data storages in the Home/cloud-data directory in the bottom-right corner of the screen. Only a user that launched a service can access it. Other users (even if a direct link to the service's GUI is known) will have 401 - Unauthorized error . Terminating a service Stopping a service is performed in the same manner as with a batch job. Option 1 . Load a list of \" Active Runs \" and click the STOP button. Option 2 . Load \"Run Logs\" form and click the STOP button. More information about runs lifecycle see here .","title":"15.1. Starting an interactive application"},{"location":"manual/15_Interactive_services/15.1._Starting_an_Interactive_application/#151-starting-an-interactive-application","text":"Starting a service Terminating a service To run a Tool or a Pipeline as an Interactive service you need to have EXECUTE permissions for that Tool/Pipeline. For more information see 13. Permissions . On this page, you'll find an example of launching an Interactive application. Launching steps remain the same for all applications ( Jupiter Notebook , Rstudio , etc.). All launching steps on this page are illustrated with the example of the Rstudio application. Rstudio is a popular IDE for R language that: is designed to make it easy to write scripts; makes it easy to set your working directory and access files on your computer; makes graphics much more accessible to a casual user.","title":"15.1. Starting an Interactive application"},{"location":"manual/15_Interactive_services/15.1._Starting_an_Interactive_application/#starting-a-service","text":"Both Pipelines and Tools can be run as interactive services. The example below shows launching Tool scenario. Search for a Tool that implements a service (type a service name in the search box - list will be filtered). Navigate to the Tool information page by clicking the Tool's name and click the Run button then press the OK (for more information see 10.5. Launch a Tool ). Service will begin to set up. A service instance will be shown in the same way as batch jobs - within Active Runs menu. Once an instance is created for a service, a link(s) to the web GUI will be shown within \"Run Log\" form. Clicking the link will load the application web interface. In this example, we show the Rstudio web interface. Service configuration includes the following items: For all out of the box services in the Cloud Pipeline, a user will be automatically authenticated within a service. Note : authentication within all new services added by the users shall be configured by themselves. Below is an example of the authentication within the Rstudio service. Besides, for all out of the box services, you'll also find that all STS data storages, that are available to the user, are available within a service. Data from the STS storages will be available as a local file system and you will be able to work with it just as you do on your laptop. Note : In case of the Rstudio application, you can find all available data storages in the Home/cloud-data directory in the bottom-right corner of the screen. Only a user that launched a service can access it. Other users (even if a direct link to the service's GUI is known) will have 401 - Unauthorized error .","title":"Starting a service"},{"location":"manual/15_Interactive_services/15.1._Starting_an_Interactive_application/#terminating-a-service","text":"Stopping a service is performed in the same manner as with a batch job. Option 1 . Load a list of \" Active Runs \" and click the STOP button. Option 2 . Load \"Run Logs\" form and click the STOP button. More information about runs lifecycle see here .","title":"Terminating a service"},{"location":"manual/15_Interactive_services/15.2._Using_Terminal_access/","text":"15.2. Using Terminal access Using Terminal access Example: using of Environment Modules for the Cloud Pipeline runs Example: using of Slurm for the Cloud Pipeline's clusters Terminal access is available to the OWNER of the running job and users with ADMIN role. With sufficient permissions, Terminal access can be achieved to any running job. For more information see 13. Permissions . Also you can get a terminal access to the running job using the pipe CLI. For more details see here . All software in the Cloud Pipeline is located in Docker containers , and we can use Terminal access to the Docker container via the Interactive services . This can be useful when: usage of a new bioinformatics tool shall be tested; batch job scripts shall be tested within a real execution environment; docker image shall be extended and saved (install more packages/bioinformatics tools) - see 10.4. Edit a Tool . Using Terminal access Both Pipelines and Tools can be run as interactive services . The example below shows launching tool scenario: Navigate to the list of registered Tools and search for the Tool required (e.g. \"base-generic-centos7\" ). Go to the Tool page and click the arrow near the Run button \u2192 Select \"Custom Settings\" . Launch Tool page form will load (it's the same form that is used to configure a batch run). The following fields shall be filled: Node type Disk size Cloud Region \" Start idle \" box should be chosen. Click the Launch button when all above parameters are set. Once a run is scheduled and configured SSH hyperlink will appear in the \"Run Log\" form in the right upper corner of the form. Note : This link is only visible to the owner of the run and users with ROLE_ADMIN role assigned. Note : Also you can find this link at the Active Runs panel of the main Dashboard: Clicking the SSH link will load a new browser tab with an authenticated Terminal . Note : If an unauthorized user will load a direct link, \"Permission denied\" error will be returned. Example: using of Environment Modules for the Cloud Pipeline runs Configure of Environment Modules using is available only for users with ADMIN role. The Environment Modules package provides for the dynamic modification of a user's environment via modulefiles . In the example below, we will use Modules to switch between two versions of Java Development Kit . At the beginning we will create a storage for all JDK versions files and modulefiles . For that: open the Library , click Create + \u2192 Storages \u2192 Create new object storage While creating - specify a storage name and mount point, e.g. /opt/software : Click the Create button. Open the created storage and create two folders in it: app - here we will upload JDK files modulefiles - here we will create modulefiles for each JDK version Open the modulefiles folder, create the jdk folder in it. Open the jdk folder, create modulefile for the JDK ver. 9.0.4 - name it 9.0.4 : Click the file name, click the Fullscreen button at the file content panel: At the popup click the EDIT button and input the modulefile content, e.g. for the JDK ver. 9.0.4 : Save it. Repeat steps 5-7 for the JDK ver. 11.0.2 . At the end you will have two modulefiles in the jdk folder: Open System Settings popup, click the Preferences tab, select Launch section. Into the launch.env.properties field add a new variable - CP_CAP_MODULES_FILES_DIR . That variable specify path to the source modulefiles . As you can see - during the run, when the storage created at step 2 will be mounted to the node in the specified mount-point ( /opt/software ), created above JDK modulefiles will be available in the modulefiles folder created at step 3 - by the path /opt/software/modulefiles . Save and close the Settings popup. Go to the Tool page, open the tool page you want to use the Environment Modules with and click the arrow near the Run button \u2192 Select \"Custom Settings\" . At the Launch page expand Advanced section. In the Limit mounts field select the storage created at step 2 (see more details here ). Click the Add system parameter button In the popup select the CP_CAP_MODULES item and click the OK button: CP_CAP_MODULES parameter enables installation and using the Modules for the current run. While installing, Modules will be configured to the source modulefiles path from the CP_CAP_MODULES_FILES_DIR launch environment variable (that was set at step 9). If CP_CAP_MODULES_FILES_DIR is not set - default modulefiles location will be used. Launch the tool. Open Run logs page, wait until InstallEnvironmentModules task will appear and check that the Modules was installed successfully: Wait until SSH hyperlink will appear in the right upper corner. Click it. In the terminal run the command module use to check the ource path to the modulefiles : Now, we will install JDK . For the ver. 9.0.4 run the following commands: # Download jdk 9.0.4 archive wget https://download.java.net/java/GA/jdk9/9.0.4/binaries/openjdk-9.0.4_linux-x64_bin.tar.gz # Extract archive content tar -zxf openjdk-9.0.4_linux-x64_bin.tar.gz # Copy jdk 9.0.4 files into the mounted data storage cp -r jdk-9.0.4 /opt/software/app/jdk-9.0.4 For the ver. 11.0.2 run the following commands: # Download jdk 11.0.2 archive wget https://download.java.net/java/GA/jdk11/9/GPL/openjdk-11.0.2_linux-x64_bin.tar.gz # Extract archive content tar -zxf openjdk-11.0.2_linux-x64_bin.tar.gz # Copy jdk 11.0.2 files into the mounted data storage cp -r jdk-11.0.2 /opt/software/app/jdk-11.0.2 Now, you can check the facilities of the Environment Modules package. Load the available modulefiles list: Load the JDK ver. 11.0.2 : Switch to the JDK ver. 9.0.4 : Unload all JDK versions: Example: using of Slurm for the Cloud Pipeline's clusters Slurm is an open source, highly scalable cluster management and job scheduling system for large and small Linux clusters. In the example below, we will use Slurm for performing the simplest batch job. Open the Tools page, select a tool and its version ( Note : in our example we will use Ubuntu 18.04 ). Hover over the v button near the Run button and click the \"Custom settings\" item in the dropdown list. At the Launch page expand \"Exec environment\" section and click the \" Configure cluster \" button: In the appeared popup click the Cluster tab. Set the count of \"child\" nodes, tick the \"Enable Slurm\" checkbox and click the OK button to confirm: Launch the tool: Open the main Dashboard and wait until the SSH hyperlink will appear at the Active Runs panel for the just-launched tool, then click it: The terminal web GUI will appear. At the beginning, let's check general system state, view existing partitions in the system and the list of available nodes. For that, perform the sinfo command: Only main.q partition is created. All cluster nodes are attached to this partition. To report more detailed information about partition - the scontrol command can be used: And to display detailed information about one of the nodes, e.g.: Now, we'll parallely execute /bin/hostname on all three nodes ( -N3 option) and include task numbers in the output ( -l option) via the srun command. The default partition will be used. One task per node will be used by default: For the batch job, create the following script: This script contains a timelimit for the job embedded within itself (via the --time option after the #SBATCH prefix). Script contains the command /bin/hostname that will be executed on the first node in the allocation (where the script runs) plus two job steps initiated using the srun command and executed sequentially. To submit a job script for execution over all three nodes use the sbatch command, result will be written to the file ( -o option): During the script execution you can check the queue of running jobs in priority order via the squeue command: The result of the sbatch command performing:","title":"15.2. Using terminal access"},{"location":"manual/15_Interactive_services/15.2._Using_Terminal_access/#152-using-terminal-access","text":"Using Terminal access Example: using of Environment Modules for the Cloud Pipeline runs Example: using of Slurm for the Cloud Pipeline's clusters Terminal access is available to the OWNER of the running job and users with ADMIN role. With sufficient permissions, Terminal access can be achieved to any running job. For more information see 13. Permissions . Also you can get a terminal access to the running job using the pipe CLI. For more details see here . All software in the Cloud Pipeline is located in Docker containers , and we can use Terminal access to the Docker container via the Interactive services . This can be useful when: usage of a new bioinformatics tool shall be tested; batch job scripts shall be tested within a real execution environment; docker image shall be extended and saved (install more packages/bioinformatics tools) - see 10.4. Edit a Tool .","title":"15.2. Using Terminal access"},{"location":"manual/15_Interactive_services/15.2._Using_Terminal_access/#using-terminal-access","text":"Both Pipelines and Tools can be run as interactive services . The example below shows launching tool scenario: Navigate to the list of registered Tools and search for the Tool required (e.g. \"base-generic-centos7\" ). Go to the Tool page and click the arrow near the Run button \u2192 Select \"Custom Settings\" . Launch Tool page form will load (it's the same form that is used to configure a batch run). The following fields shall be filled: Node type Disk size Cloud Region \" Start idle \" box should be chosen. Click the Launch button when all above parameters are set. Once a run is scheduled and configured SSH hyperlink will appear in the \"Run Log\" form in the right upper corner of the form. Note : This link is only visible to the owner of the run and users with ROLE_ADMIN role assigned. Note : Also you can find this link at the Active Runs panel of the main Dashboard: Clicking the SSH link will load a new browser tab with an authenticated Terminal . Note : If an unauthorized user will load a direct link, \"Permission denied\" error will be returned.","title":"Using Terminal access"},{"location":"manual/15_Interactive_services/15.2._Using_Terminal_access/#example-using-of-environment-modules-for-the-cloud-pipeline-runs","text":"Configure of Environment Modules using is available only for users with ADMIN role. The Environment Modules package provides for the dynamic modification of a user's environment via modulefiles . In the example below, we will use Modules to switch between two versions of Java Development Kit . At the beginning we will create a storage for all JDK versions files and modulefiles . For that: open the Library , click Create + \u2192 Storages \u2192 Create new object storage While creating - specify a storage name and mount point, e.g. /opt/software : Click the Create button. Open the created storage and create two folders in it: app - here we will upload JDK files modulefiles - here we will create modulefiles for each JDK version Open the modulefiles folder, create the jdk folder in it. Open the jdk folder, create modulefile for the JDK ver. 9.0.4 - name it 9.0.4 : Click the file name, click the Fullscreen button at the file content panel: At the popup click the EDIT button and input the modulefile content, e.g. for the JDK ver. 9.0.4 : Save it. Repeat steps 5-7 for the JDK ver. 11.0.2 . At the end you will have two modulefiles in the jdk folder: Open System Settings popup, click the Preferences tab, select Launch section. Into the launch.env.properties field add a new variable - CP_CAP_MODULES_FILES_DIR . That variable specify path to the source modulefiles . As you can see - during the run, when the storage created at step 2 will be mounted to the node in the specified mount-point ( /opt/software ), created above JDK modulefiles will be available in the modulefiles folder created at step 3 - by the path /opt/software/modulefiles . Save and close the Settings popup. Go to the Tool page, open the tool page you want to use the Environment Modules with and click the arrow near the Run button \u2192 Select \"Custom Settings\" . At the Launch page expand Advanced section. In the Limit mounts field select the storage created at step 2 (see more details here ). Click the Add system parameter button In the popup select the CP_CAP_MODULES item and click the OK button: CP_CAP_MODULES parameter enables installation and using the Modules for the current run. While installing, Modules will be configured to the source modulefiles path from the CP_CAP_MODULES_FILES_DIR launch environment variable (that was set at step 9). If CP_CAP_MODULES_FILES_DIR is not set - default modulefiles location will be used. Launch the tool. Open Run logs page, wait until InstallEnvironmentModules task will appear and check that the Modules was installed successfully: Wait until SSH hyperlink will appear in the right upper corner. Click it. In the terminal run the command module use to check the ource path to the modulefiles : Now, we will install JDK . For the ver. 9.0.4 run the following commands: # Download jdk 9.0.4 archive wget https://download.java.net/java/GA/jdk9/9.0.4/binaries/openjdk-9.0.4_linux-x64_bin.tar.gz # Extract archive content tar -zxf openjdk-9.0.4_linux-x64_bin.tar.gz # Copy jdk 9.0.4 files into the mounted data storage cp -r jdk-9.0.4 /opt/software/app/jdk-9.0.4 For the ver. 11.0.2 run the following commands: # Download jdk 11.0.2 archive wget https://download.java.net/java/GA/jdk11/9/GPL/openjdk-11.0.2_linux-x64_bin.tar.gz # Extract archive content tar -zxf openjdk-11.0.2_linux-x64_bin.tar.gz # Copy jdk 11.0.2 files into the mounted data storage cp -r jdk-11.0.2 /opt/software/app/jdk-11.0.2 Now, you can check the facilities of the Environment Modules package. Load the available modulefiles list: Load the JDK ver. 11.0.2 : Switch to the JDK ver. 9.0.4 : Unload all JDK versions:","title":"Example: using of Environment Modules for the Cloud Pipeline runs"},{"location":"manual/15_Interactive_services/15.2._Using_Terminal_access/#example-using-of-slurm-for-the-cloud-pipelines-clusters","text":"Slurm is an open source, highly scalable cluster management and job scheduling system for large and small Linux clusters. In the example below, we will use Slurm for performing the simplest batch job. Open the Tools page, select a tool and its version ( Note : in our example we will use Ubuntu 18.04 ). Hover over the v button near the Run button and click the \"Custom settings\" item in the dropdown list. At the Launch page expand \"Exec environment\" section and click the \" Configure cluster \" button: In the appeared popup click the Cluster tab. Set the count of \"child\" nodes, tick the \"Enable Slurm\" checkbox and click the OK button to confirm: Launch the tool: Open the main Dashboard and wait until the SSH hyperlink will appear at the Active Runs panel for the just-launched tool, then click it: The terminal web GUI will appear. At the beginning, let's check general system state, view existing partitions in the system and the list of available nodes. For that, perform the sinfo command: Only main.q partition is created. All cluster nodes are attached to this partition. To report more detailed information about partition - the scontrol command can be used: And to display detailed information about one of the nodes, e.g.: Now, we'll parallely execute /bin/hostname on all three nodes ( -N3 option) and include task numbers in the output ( -l option) via the srun command. The default partition will be used. One task per node will be used by default: For the batch job, create the following script: This script contains a timelimit for the job embedded within itself (via the --time option after the #SBATCH prefix). Script contains the command /bin/hostname that will be executed on the first node in the allocation (where the script runs) plus two job steps initiated using the srun command and executed sequentially. To submit a job script for execution over all three nodes use the sbatch command, result will be written to the file ( -o option): During the script execution you can check the queue of running jobs in priority order via the squeue command: The result of the sbatch command performing:","title":"Example: using of Slurm for the Cloud Pipeline's clusters"},{"location":"manual/15_Interactive_services/15.3._Expose_node_filesystem/","text":"15.3. Expose node filesystem Upload files through node's file browser Download files/directories through node's file browser Delete files/directories through node's file browser Search files/directories at node's file browser Filesystem browser is available for the Active runs only. The system preference storage.fsbrowser.enabled should be set. To browse node filesystem go to the Run logs page. After a job had been initialized the Browse hyperlink become available to the users: Click the hyperlink to view the node's filesystem. By default, the \"root\" directory for the FSBrowser is / directory of the node: Admin can change the \"root\" directory for the FSBrowser by the system preference storage.fsbrowser.wd . Other abilities of FSBrowser behavior management see here . User has an ability to view, download, upload, delete and search files and directories. To transfer between folders just click the folder and the system will open this folder to browse: Note : Storage has an auto-cleanup policy (to remove the temp files). Upload files through node's file browser To upload files to the node filesystem press the Upload button: The local File Manager window will be opened. Select files that you want to upload and press the Open button: The Cloud Pipeline will display Operations list with all selected files and uploading statuses: After the uploading will be completed the system will update files statuses: To collapse the Operations window - press the cross button in the right-upper corner. The window will be collapsed: Download files/directories through node's file browser Download operation supports both files and directories. If a directory is requested for a download the result of such operation will be a gzipped tarball. To download files or directories on your local machine from the node filesystem press the Download button: The Operations window will be open: The file or directory will be downloaded to your local machine after it's status in Operations window became Downloaded : If some directory was downloaded it will be saved as a gzipped tarball file: Delete files/directories through node's file browser To delete file or directory user needs to press the Delete button: The confirmation window will be displayed. Press OK to delete file or directory: Search files/directories at node's file browser To search inside the current directory through files and directories - click the address panel and type desired file or directory name: The result will be displayed to you. The string in the address panel is a path to the searched file or directory. So you can type a path to the file or directory and the result will be displayed to you right away.","title":"15.3. Expose node filesystem"},{"location":"manual/15_Interactive_services/15.3._Expose_node_filesystem/#153-expose-node-filesystem","text":"Upload files through node's file browser Download files/directories through node's file browser Delete files/directories through node's file browser Search files/directories at node's file browser Filesystem browser is available for the Active runs only. The system preference storage.fsbrowser.enabled should be set. To browse node filesystem go to the Run logs page. After a job had been initialized the Browse hyperlink become available to the users: Click the hyperlink to view the node's filesystem. By default, the \"root\" directory for the FSBrowser is / directory of the node: Admin can change the \"root\" directory for the FSBrowser by the system preference storage.fsbrowser.wd . Other abilities of FSBrowser behavior management see here . User has an ability to view, download, upload, delete and search files and directories. To transfer between folders just click the folder and the system will open this folder to browse: Note : Storage has an auto-cleanup policy (to remove the temp files).","title":"15.3. Expose node filesystem"},{"location":"manual/15_Interactive_services/15.3._Expose_node_filesystem/#upload-files-through-nodes-file-browser","text":"To upload files to the node filesystem press the Upload button: The local File Manager window will be opened. Select files that you want to upload and press the Open button: The Cloud Pipeline will display Operations list with all selected files and uploading statuses: After the uploading will be completed the system will update files statuses: To collapse the Operations window - press the cross button in the right-upper corner. The window will be collapsed:","title":"Upload files through node's file browser"},{"location":"manual/15_Interactive_services/15.3._Expose_node_filesystem/#download-filesdirectories-through-nodes-file-browser","text":"Download operation supports both files and directories. If a directory is requested for a download the result of such operation will be a gzipped tarball. To download files or directories on your local machine from the node filesystem press the Download button: The Operations window will be open: The file or directory will be downloaded to your local machine after it's status in Operations window became Downloaded : If some directory was downloaded it will be saved as a gzipped tarball file:","title":"Download files/directories through node's file browser"},{"location":"manual/15_Interactive_services/15.3._Expose_node_filesystem/#delete-filesdirectories-through-nodes-file-browser","text":"To delete file or directory user needs to press the Delete button: The confirmation window will be displayed. Press OK to delete file or directory:","title":"Delete files/directories through node's file browser"},{"location":"manual/15_Interactive_services/15.3._Expose_node_filesystem/#search-filesdirectories-at-nodes-file-browser","text":"To search inside the current directory through files and directories - click the address panel and type desired file or directory name: The result will be displayed to you. The string in the address panel is a path to the searched file or directory. So you can type a path to the file or directory and the result will be displayed to you right away.","title":"Search files/directories at node's file browser"},{"location":"manual/15_Interactive_services/15.4._Interactive_service_examples/","text":"15.4. Interactive service examples Running Apache Spark cluster with RStudio Web GUI Launch the RStudio tool with Apache Cluster Example of sparklyr script Monitoring execution via Spark UI To run a Tool or a Pipeline as an Interactive service you need to have EXECUTE permissions for that Tool/Pipeline. For more information see 13. Permissions . On this page, you'll find examples of launching Interactive applications with various features. Running Apache Spark cluster with RStudio Web GUI Launch the RStudio tool with Apache Cluster Navigate to the list of registered Tools and search for the RStudio Tool: Go to the Tool page and click the arrow near the Run button \u2192 Select \"Custom Settings\" . At the Launch page select Node type (from which the subsequent cluster will consist). Note : it is recommended to select nodes with more memory volume, because it is critical for Spark's in-memory processing Click the Configure cluster button: At the popup select the Cluster tab, set the number of Child nodes to create (e.g. 2 or more), then tick the Enable Apache Spark checkbox and confirm by the OK button: Or you could enable Apache Spark by manually adding the system parameter CP_CAP_SPARK with true value: Click the Launch button: At the popup confirm the launch. Check that the cluster has appeared in the ACTIVE RUNS tab of the Runs page: Wait until all components are initialized. The cluster tile in the ACTIVE RUNS tab will turn into yellow. Click on the parent run to open the Run logs page: At the Run logs page there are two endpoints: RStudio - it exposes RStudio's Web IDE SparkUI - it exposes Web GUI of the Spark. It allows to monitor Spark master/workers/application via the web-browser. Details are available in the Spark UI manual Click the RStudio endpoint. This will load RStudio Web GUI with the pre-installed sparklyr package in the new tab of the web-browser: From here one can start coding in R using sparklyr to run the workload over the cluster. Example of sparklyr script It is assumed that a Spark cluster with the RStudio Web GUI and sparklyr package is up and running, as shown in the previous section. Accessing datasets from the Cloud Pipeline's Spark Access to data via File Storages If the user has an access to the FS storage - then datasets from such storage can be accessed via the file:// schema or without a schema at all. But this approach may start to degrade once there is 100+ cores cluster with a lot of I/O operations. Access to data via Object Storages Note : this feature is available only for AWS Cloud Provider. Spark cluster configuration uses the stable version of the Hadoop-AWS module , that allows to directly access (read/write) the datasets in the S3 buckets using Spark jobs. The only difference with the filesystem access - is the URL schema. s3a:// prefix shall be used instead of s3:// . E.g. if there is a user's S3 bucket named \" test-object-storage \" with the \" test_dataset.parquet \" dataset - then it can be accessed as \" s3a://test-object-storage/test_dataset.parquet \" from the sparklyr code (or any other Spark job). Prepare and run sparklyr script This section provides prepare and run of R script that shows how to connect to the Spark cluster, deployed in the Cloud Pipeline, and read/write the data from/to object data storage. Note : we will use S3 bucket with directly access to data as described above. This script will not work for other Cloud Providers. For this example, a small public VCF file will be used. It is located in some S3 bucket e.g.: So, prepare the following script: library(sparklyr) # Cloud Pipeline provides the SPARK's master URL in SPARK_MASTER variable # SPARK_HOME variabe is set by the Cloud Pipeline and will be used by sparklyr - no need to specify it explicitly # Spark version will be retrieved by sparklyr from the $SPARK_HOME/RELEASE file - no need to specify it explicitly master - Sys.getenv( SPARK_MASTER ) sc - spark_connect(master=master) # Get the current Cloud Pipeline's unique Run ID to write the results into the unique directory unique_id - Sys.getenv( RUN_ID ) # Setup input VCF (tab-delimited) file location and the resulting parquet file # Note that both input and output are located in the S3 bucket and are addressed via s3a:// schema example_data_vcf_path - s3a://objstor/TestSample.vcf example_data_parquet_path - paste( s3a://objstor/results , unique_id, example_data.parquet , sep= / ) # Read VCF from the storage and convert to the DataFrame example_data_df - spark_read_csv(sc = sc, name = example_data_vcf , path = example_data_vcf_path, header = F, delimiter = \\t ) # Write DataFrame as a parquet to the storage spark_write_parquet(example_data_df, path = example_data_parquet_path) Paste that script into the RStudio console and launch it: Once script is finished - resulting parquet will be written to the storage. To check it open in the Library the storage, that was specified for output results: Open the path for output results - you will see the directory with the name equal to the Run ID that contains resulting files: Monitoring execution via Spark UI To view the details of the jobs being executed in Spark, how the memory is used and get other useful information - the SparkUI endpoint from the Run logs page shall be opened. While executing the example script , open the Spark UI endpoint: A list of active applications and workers will be shown: To get the details of the underlying jobs, executed by the Spark instance, click the application name: The following page will be opened: For more details about Spark UI opportunities see here .","title":"15.4. Interactive service examples"},{"location":"manual/15_Interactive_services/15.4._Interactive_service_examples/#154-interactive-service-examples","text":"Running Apache Spark cluster with RStudio Web GUI Launch the RStudio tool with Apache Cluster Example of sparklyr script Monitoring execution via Spark UI To run a Tool or a Pipeline as an Interactive service you need to have EXECUTE permissions for that Tool/Pipeline. For more information see 13. Permissions . On this page, you'll find examples of launching Interactive applications with various features.","title":"15.4. Interactive service examples"},{"location":"manual/15_Interactive_services/15.4._Interactive_service_examples/#running-apache-spark-cluster-with-rstudio-web-gui","text":"","title":"Running Apache Spark cluster with RStudio Web GUI"},{"location":"manual/15_Interactive_services/15.4._Interactive_service_examples/#launch-the-rstudio-tool-with-apache-cluster","text":"Navigate to the list of registered Tools and search for the RStudio Tool: Go to the Tool page and click the arrow near the Run button \u2192 Select \"Custom Settings\" . At the Launch page select Node type (from which the subsequent cluster will consist). Note : it is recommended to select nodes with more memory volume, because it is critical for Spark's in-memory processing Click the Configure cluster button: At the popup select the Cluster tab, set the number of Child nodes to create (e.g. 2 or more), then tick the Enable Apache Spark checkbox and confirm by the OK button: Or you could enable Apache Spark by manually adding the system parameter CP_CAP_SPARK with true value: Click the Launch button: At the popup confirm the launch. Check that the cluster has appeared in the ACTIVE RUNS tab of the Runs page: Wait until all components are initialized. The cluster tile in the ACTIVE RUNS tab will turn into yellow. Click on the parent run to open the Run logs page: At the Run logs page there are two endpoints: RStudio - it exposes RStudio's Web IDE SparkUI - it exposes Web GUI of the Spark. It allows to monitor Spark master/workers/application via the web-browser. Details are available in the Spark UI manual Click the RStudio endpoint. This will load RStudio Web GUI with the pre-installed sparklyr package in the new tab of the web-browser: From here one can start coding in R using sparklyr to run the workload over the cluster.","title":"Launch the RStudio tool with Apache Cluster"},{"location":"manual/15_Interactive_services/15.4._Interactive_service_examples/#example-of-sparklyr-script","text":"It is assumed that a Spark cluster with the RStudio Web GUI and sparklyr package is up and running, as shown in the previous section.","title":"Example of sparklyr script"},{"location":"manual/15_Interactive_services/15.4._Interactive_service_examples/#accessing-datasets-from-the-cloud-pipelines-spark","text":"Access to data via File Storages If the user has an access to the FS storage - then datasets from such storage can be accessed via the file:// schema or without a schema at all. But this approach may start to degrade once there is 100+ cores cluster with a lot of I/O operations. Access to data via Object Storages Note : this feature is available only for AWS Cloud Provider. Spark cluster configuration uses the stable version of the Hadoop-AWS module , that allows to directly access (read/write) the datasets in the S3 buckets using Spark jobs. The only difference with the filesystem access - is the URL schema. s3a:// prefix shall be used instead of s3:// . E.g. if there is a user's S3 bucket named \" test-object-storage \" with the \" test_dataset.parquet \" dataset - then it can be accessed as \" s3a://test-object-storage/test_dataset.parquet \" from the sparklyr code (or any other Spark job).","title":"Accessing datasets from the Cloud Pipeline's Spark"},{"location":"manual/15_Interactive_services/15.4._Interactive_service_examples/#prepare-and-run-sparklyr-script","text":"This section provides prepare and run of R script that shows how to connect to the Spark cluster, deployed in the Cloud Pipeline, and read/write the data from/to object data storage. Note : we will use S3 bucket with directly access to data as described above. This script will not work for other Cloud Providers. For this example, a small public VCF file will be used. It is located in some S3 bucket e.g.: So, prepare the following script: library(sparklyr) # Cloud Pipeline provides the SPARK's master URL in SPARK_MASTER variable # SPARK_HOME variabe is set by the Cloud Pipeline and will be used by sparklyr - no need to specify it explicitly # Spark version will be retrieved by sparklyr from the $SPARK_HOME/RELEASE file - no need to specify it explicitly master - Sys.getenv( SPARK_MASTER ) sc - spark_connect(master=master) # Get the current Cloud Pipeline's unique Run ID to write the results into the unique directory unique_id - Sys.getenv( RUN_ID ) # Setup input VCF (tab-delimited) file location and the resulting parquet file # Note that both input and output are located in the S3 bucket and are addressed via s3a:// schema example_data_vcf_path - s3a://objstor/TestSample.vcf example_data_parquet_path - paste( s3a://objstor/results , unique_id, example_data.parquet , sep= / ) # Read VCF from the storage and convert to the DataFrame example_data_df - spark_read_csv(sc = sc, name = example_data_vcf , path = example_data_vcf_path, header = F, delimiter = \\t ) # Write DataFrame as a parquet to the storage spark_write_parquet(example_data_df, path = example_data_parquet_path) Paste that script into the RStudio console and launch it: Once script is finished - resulting parquet will be written to the storage. To check it open in the Library the storage, that was specified for output results: Open the path for output results - you will see the directory with the name equal to the Run ID that contains resulting files:","title":"Prepare and run sparklyr script"},{"location":"manual/15_Interactive_services/15.4._Interactive_service_examples/#monitoring-execution-via-spark-ui","text":"To view the details of the jobs being executed in Spark, how the memory is used and get other useful information - the SparkUI endpoint from the Run logs page shall be opened. While executing the example script , open the Spark UI endpoint: A list of active applications and workers will be shown: To get the details of the underlying jobs, executed by the Spark instance, click the application name: The following page will be opened: For more details about Spark UI opportunities see here .","title":"Monitoring execution via Spark UI"},{"location":"manual/15_Interactive_services/15._Interactive_services/","text":"15. Interactive services Overview Interactive services - a feature of the Cloud Pipeline that allows to set up an interactive application in a cloud infrastructure and access it via the web interface, leveraging cloud large instances. This is useful when some analysis steps shall be performed in interactive mode. It can also be useful for navigating through the execution environment and for testing purposes. Examples: Debug scripts, that shall be used in batch jobs Perform data post-processing using IDE Install/Delete/Update software. In the Cloud Pipeline, both Tools and Pipelines can be run as interactive services. Supported services Out of the box, Cloud Pipeline provides the following interactive services: RStudio - IDE for R language that helps to make work with R a great deal more convenient. For details see https://www.rstudio.com/ . Jupiter Notebook - a web application that allows creating documents with code pieces, visualizations and narrative text inside. For more information see http://jupyter.org/ . Terminal - a window with a command line (shell). Note : Terminal access is available for all Tools or Pipelines with these Tools. On the other side, you can't get an access to the e.g. Rstudio application if a Tool doesn't contain it. List of services can be extended by users.","title":"15.0. Overview"},{"location":"manual/15_Interactive_services/15._Interactive_services/#15-interactive-services","text":"","title":"15. Interactive services"},{"location":"manual/15_Interactive_services/15._Interactive_services/#overview","text":"Interactive services - a feature of the Cloud Pipeline that allows to set up an interactive application in a cloud infrastructure and access it via the web interface, leveraging cloud large instances. This is useful when some analysis steps shall be performed in interactive mode. It can also be useful for navigating through the execution environment and for testing purposes. Examples: Debug scripts, that shall be used in batch jobs Perform data post-processing using IDE Install/Delete/Update software. In the Cloud Pipeline, both Tools and Pipelines can be run as interactive services.","title":"Overview"},{"location":"manual/15_Interactive_services/15._Interactive_services/#supported-services","text":"Out of the box, Cloud Pipeline provides the following interactive services: RStudio - IDE for R language that helps to make work with R a great deal more convenient. For details see https://www.rstudio.com/ . Jupiter Notebook - a web application that allows creating documents with code pieces, visualizations and narrative text inside. For more information see http://jupyter.org/ . Terminal - a window with a command line (shell). Note : Terminal access is available for all Tools or Pipelines with these Tools. On the other side, you can't get an access to the e.g. Rstudio application if a Tool doesn't contain it. List of services can be extended by users.","title":"Supported services"},{"location":"manual/16_Issues/16._Issues/","text":"16. Issues Open an issue Change an issue title Leave a comment Edit a comment Delete a comment Delete an issue Issues is a great tool to share results with other users or get feedback. It allows keeping the discussion in one place - traceable and linked to specific data. The feature is available for: Folders, including Projects Pipelines Tools. Open an issue To open an issue, a user shall have READ permissions for a discussed object. For more information see 13. Permissions . To open an issue the following steps shall be performed: Navigate to the object you want to discuss and click icon \u2192 Issues . Note : the second way is to navigate to a folder that contains the object you want to discuss and click icon in the desired object's line. The Issues pane will be opened. Click button to create a new issue. Fill up the open form: Title (e.g. new issue ), Description (e.g. error ). Description supports MARKDOWN formatting, thus you can write your text in the Write tab with special symbols and then preview it in the Preview tab. Note : you can address your topic to a specific user if you put @ symbol and start to write username. The system will suggest you choose from the list. After you save your topic, the user will receive e-mail notification. When you finish, press the Create button - the topic will be created. You'll see topic title, the author and how much time past since the topic was created. You can click on it to open and see description. If you want to go back to the list of all discussions, click . Change an issue title To edit an issue title a user shall be OWNER of the discussion. For more information see 13. Permissions . To change an issue title the following steps shall be performed: Navigate to the issue which title you want to change and open it. Click the issue title - the Title field will be open for editing. Enter new title and click out of the field. The new title will be saved. Leave a comment To leave a comment a user shall have READ permissions for a discussioned object. For more information see 13. Permissions . To leave a comment on an issue the following steps shall be performed: Navigate to the object you want to discuss and click icon \u2192 Issues . Note : the second way is to navigate to a folder that contains the object you want to discuss and click icon in the desired object's line. The Issues pane will be opened. Click on an issue you interested in. Fill up the Comment form. Comment supports MARKDOWN formatting, thus you can write your text in the Write tab with special symbols and then preview it in the Preview tab. Note : you can address your comment to a specific user if you put @ symbol and start to write username. The system will suggest you choose from the list. After you save your comment, the user will receive an e-mail notification. Note : you can also drag and drop pictures here, so that everyone can see the issue more clearly: When you finish, press the Send button - the comment will be created. You'll see your comment, the author and how much time past since the comment was created. Edit a comment To edit a comment a user shall be OWNER of the comment. For more information see 13. Permissions . Note : to edit a topic description the same steps should be performed. To edit a comment the following steps shall be performed: Navigate to the comment you want to edit. Press icon - the editing form will be shown. Change your comment and click to save your changes. Note : won't be available until you change something. Note : if you change your mind and want to leave your comment as is, click . The changes will be saved. Delete a comment To delete a comment a user shall be OWNER of the comment. For more information see 13. Permissions . To delete a comment the following steps shall be performed: Navigate to the comment you want to delete. Click icon. Confirm your action in the dialog window. The comment is deleted. Delete an issue To delete an issue a user shall be OWNER of the issue. For more information see 13. Permissions . To delete an issue the following steps shall be performed: Navigate to the issue you want to delete and open it. Click icon. Confirm your action in the dialog window. The issue is deleted.","title":"16. Issues"},{"location":"manual/16_Issues/16._Issues/#16-issues","text":"Open an issue Change an issue title Leave a comment Edit a comment Delete a comment Delete an issue Issues is a great tool to share results with other users or get feedback. It allows keeping the discussion in one place - traceable and linked to specific data. The feature is available for: Folders, including Projects Pipelines Tools.","title":"16. Issues"},{"location":"manual/16_Issues/16._Issues/#open-an-issue","text":"To open an issue, a user shall have READ permissions for a discussed object. For more information see 13. Permissions . To open an issue the following steps shall be performed: Navigate to the object you want to discuss and click icon \u2192 Issues . Note : the second way is to navigate to a folder that contains the object you want to discuss and click icon in the desired object's line. The Issues pane will be opened. Click button to create a new issue. Fill up the open form: Title (e.g. new issue ), Description (e.g. error ). Description supports MARKDOWN formatting, thus you can write your text in the Write tab with special symbols and then preview it in the Preview tab. Note : you can address your topic to a specific user if you put @ symbol and start to write username. The system will suggest you choose from the list. After you save your topic, the user will receive e-mail notification. When you finish, press the Create button - the topic will be created. You'll see topic title, the author and how much time past since the topic was created. You can click on it to open and see description. If you want to go back to the list of all discussions, click .","title":"Open an issue"},{"location":"manual/16_Issues/16._Issues/#change-an-issue-title","text":"To edit an issue title a user shall be OWNER of the discussion. For more information see 13. Permissions . To change an issue title the following steps shall be performed: Navigate to the issue which title you want to change and open it. Click the issue title - the Title field will be open for editing. Enter new title and click out of the field. The new title will be saved.","title":"Change an issue title"},{"location":"manual/16_Issues/16._Issues/#leave-a-comment","text":"To leave a comment a user shall have READ permissions for a discussioned object. For more information see 13. Permissions . To leave a comment on an issue the following steps shall be performed: Navigate to the object you want to discuss and click icon \u2192 Issues . Note : the second way is to navigate to a folder that contains the object you want to discuss and click icon in the desired object's line. The Issues pane will be opened. Click on an issue you interested in. Fill up the Comment form. Comment supports MARKDOWN formatting, thus you can write your text in the Write tab with special symbols and then preview it in the Preview tab. Note : you can address your comment to a specific user if you put @ symbol and start to write username. The system will suggest you choose from the list. After you save your comment, the user will receive an e-mail notification. Note : you can also drag and drop pictures here, so that everyone can see the issue more clearly: When you finish, press the Send button - the comment will be created. You'll see your comment, the author and how much time past since the comment was created.","title":"Leave a comment"},{"location":"manual/16_Issues/16._Issues/#edit-a-comment","text":"To edit a comment a user shall be OWNER of the comment. For more information see 13. Permissions . Note : to edit a topic description the same steps should be performed. To edit a comment the following steps shall be performed: Navigate to the comment you want to edit. Press icon - the editing form will be shown. Change your comment and click to save your changes. Note : won't be available until you change something. Note : if you change your mind and want to leave your comment as is, click . The changes will be saved.","title":"Edit a comment"},{"location":"manual/16_Issues/16._Issues/#delete-a-comment","text":"To delete a comment a user shall be OWNER of the comment. For more information see 13. Permissions . To delete a comment the following steps shall be performed: Navigate to the comment you want to delete. Click icon. Confirm your action in the dialog window. The comment is deleted.","title":"Delete a comment"},{"location":"manual/16_Issues/16._Issues/#delete-an-issue","text":"To delete an issue a user shall be OWNER of the issue. For more information see 13. Permissions . To delete an issue the following steps shall be performed: Navigate to the issue you want to delete and open it. Click icon. Confirm your action in the dialog window. The issue is deleted.","title":"Delete an issue"},{"location":"manual/17_Tagging_by_attributes/17._CP_objects_tagging_by_additional_attributes/","text":"17. CP objects tagging by additional attributes Add attributes Add JSON object into the attribute value Edit attributes Delete attributes Automatic tagging A user can manage custom sets of \" key-values \" attributes for data storage and files. These custom attributes could be used for an additional description of the object and make the search process easier by using attributes as tags. To edit object's attributes, you need to be an OWNER of the object. For more information see 13. Permissions . You can also manage attributes via CLI. See 14.2. View and manage Attributes via CLI . How to navigate to Attributes panel of different objects: Folder Metadata Pipeline Data storage Tool groups and tools User Group of users/role Note : if you were changing the data storage file's attributes, you could return to data storage's attribute by clicking control. Add attributes Navigate to the Attributes panel of a selected object. Click the + Add button. Enter an attribute key and value. Click the Add button: Added attribute will appear at the Attributes panel: Add JSON object into the attribute value Also, you can add more complex attributes than just strings. In the \"Value\" field you can specify a raw JSON object, that will be transformed into the pretty-view table. View an example: Navigate to the Attributes panel of a selected object. Click the + Add button. Enter an attribute key. Enter the JSON object as the attribute value, click the Add button: Added attribute will appear at the Attributes panel: As the value - the link will be displayed that shows the summary count of first-level JSON records. Click the value link - the pretty-view detailed table will be opened for the added attribute: If the raw JSON has more than one level, downstream records will be shown as the link Object . You can hover over it and view the downstream records: If you want to edit such attribute value - click the EDIT button. The raw JSON will be opened: You can edit it and click the SAVE button to save changes. Edit attributes Navigate to the Attributes panel of a selected object. Click the attribute key or value field: Change the attribute key or value: Press the Enter key or just click out of the active field. Edited attribute will appear at the Attributes panel: Delete attributes Navigate to the Attributes panel of a selected object. Click the Trash icon to delete a particular attribute. Note : click the Remove all button to delete all attributes. Automatic tagging In the Cloud Pipeline files are automatically tagged with the following attributes when uploading them to the data storage via CLI/GUI (see a CLI example 14.3. Manage Storage via CLI ): Name in GUI Name in CLI Value Owner CP_OWNER The value of the attribute will be set as a user ID. Source CP_SOURCE The value of the attribute will be set as a local path used to upload. Note : this attribute is set only if a file is uploaded through CLI. Note : The exception is that the storage is based on FS share. Files in such data storage don't have attributes at all. Besides, files are automatically tagged with the following attributes when uploading them to the data storage as a result of a pipeline run: Name in GUI Name in CLI Value Owner CP_OWNER User ID Source CP_SOURCE Local path used to upload data RunID CP_RUN_ID Run ID Pipeline CP_JOB_NAME Pipeline name CP_JOB_ID Pipeline ID Pipeline version CP_JOB_VERSION Pipeline version Pipeline configuration CP_JOB_CONFIGURATION Pipeline configuration Docker image CP_DOCKER_IMAGE Tool (docker image) that was used Compute CP_CALC_CONFIG Instance type Example of the Attributes panel for the such file:","title":"17. CP objects tagging by additional attributes"},{"location":"manual/17_Tagging_by_attributes/17._CP_objects_tagging_by_additional_attributes/#17-cp-objects-tagging-by-additional-attributes","text":"Add attributes Add JSON object into the attribute value Edit attributes Delete attributes Automatic tagging A user can manage custom sets of \" key-values \" attributes for data storage and files. These custom attributes could be used for an additional description of the object and make the search process easier by using attributes as tags. To edit object's attributes, you need to be an OWNER of the object. For more information see 13. Permissions . You can also manage attributes via CLI. See 14.2. View and manage Attributes via CLI . How to navigate to Attributes panel of different objects: Folder Metadata Pipeline Data storage Tool groups and tools User Group of users/role Note : if you were changing the data storage file's attributes, you could return to data storage's attribute by clicking control.","title":"17. CP objects tagging by additional attributes"},{"location":"manual/17_Tagging_by_attributes/17._CP_objects_tagging_by_additional_attributes/#add-attributes","text":"Navigate to the Attributes panel of a selected object. Click the + Add button. Enter an attribute key and value. Click the Add button: Added attribute will appear at the Attributes panel:","title":"Add attributes"},{"location":"manual/17_Tagging_by_attributes/17._CP_objects_tagging_by_additional_attributes/#add-json-object-into-the-attribute-value","text":"Also, you can add more complex attributes than just strings. In the \"Value\" field you can specify a raw JSON object, that will be transformed into the pretty-view table. View an example: Navigate to the Attributes panel of a selected object. Click the + Add button. Enter an attribute key. Enter the JSON object as the attribute value, click the Add button: Added attribute will appear at the Attributes panel: As the value - the link will be displayed that shows the summary count of first-level JSON records. Click the value link - the pretty-view detailed table will be opened for the added attribute: If the raw JSON has more than one level, downstream records will be shown as the link Object . You can hover over it and view the downstream records: If you want to edit such attribute value - click the EDIT button. The raw JSON will be opened: You can edit it and click the SAVE button to save changes.","title":"Add JSON object into the attribute value"},{"location":"manual/17_Tagging_by_attributes/17._CP_objects_tagging_by_additional_attributes/#edit-attributes","text":"Navigate to the Attributes panel of a selected object. Click the attribute key or value field: Change the attribute key or value: Press the Enter key or just click out of the active field. Edited attribute will appear at the Attributes panel:","title":"Edit attributes"},{"location":"manual/17_Tagging_by_attributes/17._CP_objects_tagging_by_additional_attributes/#delete-attributes","text":"Navigate to the Attributes panel of a selected object. Click the Trash icon to delete a particular attribute. Note : click the Remove all button to delete all attributes.","title":"Delete attributes"},{"location":"manual/17_Tagging_by_attributes/17._CP_objects_tagging_by_additional_attributes/#automatic-tagging","text":"In the Cloud Pipeline files are automatically tagged with the following attributes when uploading them to the data storage via CLI/GUI (see a CLI example 14.3. Manage Storage via CLI ): Name in GUI Name in CLI Value Owner CP_OWNER The value of the attribute will be set as a user ID. Source CP_SOURCE The value of the attribute will be set as a local path used to upload. Note : this attribute is set only if a file is uploaded through CLI. Note : The exception is that the storage is based on FS share. Files in such data storage don't have attributes at all. Besides, files are automatically tagged with the following attributes when uploading them to the data storage as a result of a pipeline run: Name in GUI Name in CLI Value Owner CP_OWNER User ID Source CP_SOURCE Local path used to upload data RunID CP_RUN_ID Run ID Pipeline CP_JOB_NAME Pipeline name CP_JOB_ID Pipeline ID Pipeline version CP_JOB_VERSION Pipeline version Pipeline configuration CP_JOB_CONFIGURATION Pipeline configuration Docker image CP_DOCKER_IMAGE Tool (docker image) that was used Compute CP_CALC_CONFIG Instance type Example of the Attributes panel for the such file:","title":"Automatic tagging"},{"location":"manual/18_Home_page/18._Home_page/","text":"18. Home page Home page widgets Activities Data Notifications Tools Pipelines Projects Recently completed runs Active runs Services Adjust Home page view Start a Run from the Home tab Home page widgets Activities This widget lists recent comments/issues/posts that occured for the items that you own (e.g. models, pipelines, projects, folders, etc.). Data List of the data storages that are available to you for READ/WRITE operations. These data storages are available from the Platform GUI. Click an item to navigate to the data storage contents. Note : personal Data storages (i.e. a user is an OWNER of this Storage) will be shown on top, WRITE - second priority, READ - third priority. Note : data storages are tagged with the Cloud Region flag to visually distinguish storage locations. Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - corresponding auxiliary Cloud Provider icons are additionally displayed, e.g.: Notifications List of system-wide notifications from administrators posted. These are the same notifications as shown at a login time in the top right corner of the main page. Tools Tools/Compute stacks/Docker images that are added in your PERSONAL repository or available to your group. To run a Tool - please use a RUN button that appears when hovering an item with a mouse. Use a search bar to find tools that are shared by other users and groups. To get a full list of available stacks - please use the Tools menu item in the left toolbar. Note : group-level Tools will be shown on the top of the Tools list. Pipelines Pipelines that are available to you for READ/WRITE operations. This is the same list, as available in the Library hierarchy. Pipeline can be run right from this widget using a RUN button that appears when hovering an item with a mouse. Press the HISTORY control to view runs history of a chosen pipeline. Projects Projects that are available to you. This is the same list, as available in the Library hierarchy. Projects could be opened from this widget. Press the HISTORY control to view runs history of a chosen Project. Press the DATA STORAGE control to open a data storage included into a chosen Project. Recently completed runs This widget lists your runs that were recently completed. Click a corresponding entry in this widget to navigate to the run details/logs page. Use the Rerun control to rerun selected item. Note : completed runs are tagged with the Cloud Region flag to visually distinguish compute instance locations. Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - corresponding auxiliary Cloud Provider icons are additionally displayed, e.g.: Help tooltips are provided when hovering a run state icon, e.g.: Active runs List of the jobs/tools that are currently in RUNNING or PAUSED state with some information about instance, elapsed time and estimated price, that is calculated based on the run duration and instance type. If this list is empty - start a new run from the Tools or Pipelines widgets. Hover a run item to view a list of available action. Use STOP / PAUSE / RESUME / TERMINATE actions to change the state of the run, use OPEN to navigate to the GUI of the interactive job or use SSH to open SSH session over the running container. Select the LINKS button to view/navigate the run input/output parameters. Click a run item to navigate to the details and logs page. Note : active runs are tagged with the Cloud Region flag to visually distinguish compute instance locations. Only top 50 active runs will be shown, if more than 50 jobs/tools are running - use Explore all active runs link. Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - corresponding auxiliary Cloud Provider icons are additionally displayed, e.g.: Note : if specific run CPU/Memory/Disk consumption is lower or higher specified in the configurations values, the IDLE or PRESSURE labels will be displayed respectively: Help tooltips are provided when hovering a run state icon, e.g.: Services This widget lists direct links to the Interactive services, that are exposed by the launched Tools. Compared to the ACTIVE RUNS widget - this one does NOT show all the active jobs/tools, only links to the web/desktop GUI. If this list is empty - start a new run of an interactive compute stack from the Tools widget. Click an item within this widget to navigate to the corresponding service. Adjust Home page view Dashboard view can be adjusted by clicking the Configure button. In the opened window a user can adjust view of the Home tab by selecting widgets. Selected items will be displayed after you click OK : Show only favourites checkbox allows to restrict displaying items in selected widgets and show only ones ticked by favourite mark (\"star\" icon in the left side of the items - ). This feature works only for DATA , PIPELINES , PROJECTS and TOOLS widgets. Restore default layout control is used to restore default tab configuration. Also, user can remove widgets by clicking the \"Delete\" icon on them. Start a Run from the Home tab User shall have EXECUTE rights to run selected Tool/pipeline. Although Runs can be started from the Tools or Library tabs respectively (see 10.5. Launch a Tool and 6.2. Launch a pipeline ), we can start them from the Home tab as well. In the example below we will start a Run from the Tools widget of the Home tab: Navigate to the Home tab. Select a Tool in the Tools widget. Click Run . Run a Tool with custom settings with Run custom control or run them with default settings with Run button. Note : Active runs with endpoints are highlighted in yellow. They expose the Open button that shows endpoints. Clicking them will navigate you to the endpoint URL: Note : all widgets that display Runs show input/output links via LINKS control. Clicking a link will navigate you to the appropriate Data Storage.","title":"18. Home page"},{"location":"manual/18_Home_page/18._Home_page/#18-home-page","text":"Home page widgets Activities Data Notifications Tools Pipelines Projects Recently completed runs Active runs Services Adjust Home page view Start a Run from the Home tab","title":"18. Home page"},{"location":"manual/18_Home_page/18._Home_page/#home-page-widgets","text":"","title":"Home page widgets"},{"location":"manual/18_Home_page/18._Home_page/#activities","text":"This widget lists recent comments/issues/posts that occured for the items that you own (e.g. models, pipelines, projects, folders, etc.).","title":"Activities"},{"location":"manual/18_Home_page/18._Home_page/#data","text":"List of the data storages that are available to you for READ/WRITE operations. These data storages are available from the Platform GUI. Click an item to navigate to the data storage contents. Note : personal Data storages (i.e. a user is an OWNER of this Storage) will be shown on top, WRITE - second priority, READ - third priority. Note : data storages are tagged with the Cloud Region flag to visually distinguish storage locations. Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - corresponding auxiliary Cloud Provider icons are additionally displayed, e.g.:","title":"Data"},{"location":"manual/18_Home_page/18._Home_page/#notifications","text":"List of system-wide notifications from administrators posted. These are the same notifications as shown at a login time in the top right corner of the main page.","title":"Notifications"},{"location":"manual/18_Home_page/18._Home_page/#tools","text":"Tools/Compute stacks/Docker images that are added in your PERSONAL repository or available to your group. To run a Tool - please use a RUN button that appears when hovering an item with a mouse. Use a search bar to find tools that are shared by other users and groups. To get a full list of available stacks - please use the Tools menu item in the left toolbar. Note : group-level Tools will be shown on the top of the Tools list.","title":"Tools"},{"location":"manual/18_Home_page/18._Home_page/#pipelines","text":"Pipelines that are available to you for READ/WRITE operations. This is the same list, as available in the Library hierarchy. Pipeline can be run right from this widget using a RUN button that appears when hovering an item with a mouse. Press the HISTORY control to view runs history of a chosen pipeline.","title":"Pipelines"},{"location":"manual/18_Home_page/18._Home_page/#projects","text":"Projects that are available to you. This is the same list, as available in the Library hierarchy. Projects could be opened from this widget. Press the HISTORY control to view runs history of a chosen Project. Press the DATA STORAGE control to open a data storage included into a chosen Project.","title":"Projects"},{"location":"manual/18_Home_page/18._Home_page/#recently-completed-runs","text":"This widget lists your runs that were recently completed. Click a corresponding entry in this widget to navigate to the run details/logs page. Use the Rerun control to rerun selected item. Note : completed runs are tagged with the Cloud Region flag to visually distinguish compute instance locations. Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - corresponding auxiliary Cloud Provider icons are additionally displayed, e.g.: Help tooltips are provided when hovering a run state icon, e.g.:","title":"Recently completed runs"},{"location":"manual/18_Home_page/18._Home_page/#active-runs","text":"List of the jobs/tools that are currently in RUNNING or PAUSED state with some information about instance, elapsed time and estimated price, that is calculated based on the run duration and instance type. If this list is empty - start a new run from the Tools or Pipelines widgets. Hover a run item to view a list of available action. Use STOP / PAUSE / RESUME / TERMINATE actions to change the state of the run, use OPEN to navigate to the GUI of the interactive job or use SSH to open SSH session over the running container. Select the LINKS button to view/navigate the run input/output parameters. Click a run item to navigate to the details and logs page. Note : active runs are tagged with the Cloud Region flag to visually distinguish compute instance locations. Only top 50 active runs will be shown, if more than 50 jobs/tools are running - use Explore all active runs link. Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - corresponding auxiliary Cloud Provider icons are additionally displayed, e.g.: Note : if specific run CPU/Memory/Disk consumption is lower or higher specified in the configurations values, the IDLE or PRESSURE labels will be displayed respectively: Help tooltips are provided when hovering a run state icon, e.g.:","title":"Active runs"},{"location":"manual/18_Home_page/18._Home_page/#services","text":"This widget lists direct links to the Interactive services, that are exposed by the launched Tools. Compared to the ACTIVE RUNS widget - this one does NOT show all the active jobs/tools, only links to the web/desktop GUI. If this list is empty - start a new run of an interactive compute stack from the Tools widget. Click an item within this widget to navigate to the corresponding service.","title":"Services"},{"location":"manual/18_Home_page/18._Home_page/#adjust-home-page-view","text":"Dashboard view can be adjusted by clicking the Configure button. In the opened window a user can adjust view of the Home tab by selecting widgets. Selected items will be displayed after you click OK : Show only favourites checkbox allows to restrict displaying items in selected widgets and show only ones ticked by favourite mark (\"star\" icon in the left side of the items - ). This feature works only for DATA , PIPELINES , PROJECTS and TOOLS widgets. Restore default layout control is used to restore default tab configuration. Also, user can remove widgets by clicking the \"Delete\" icon on them.","title":"Adjust Home page view"},{"location":"manual/18_Home_page/18._Home_page/#start-a-run-from-the-home-tab","text":"User shall have EXECUTE rights to run selected Tool/pipeline. Although Runs can be started from the Tools or Library tabs respectively (see 10.5. Launch a Tool and 6.2. Launch a pipeline ), we can start them from the Home tab as well. In the example below we will start a Run from the Tools widget of the Home tab: Navigate to the Home tab. Select a Tool in the Tools widget. Click Run . Run a Tool with custom settings with Run custom control or run them with default settings with Run button. Note : Active runs with endpoints are highlighted in yellow. They expose the Open button that shows endpoints. Clicking them will navigate you to the endpoint URL: Note : all widgets that display Runs show input/output links via LINKS control. Clicking a link will navigate you to the appropriate Data Storage.","title":"Start a Run from the Home tab"},{"location":"manual/Appendix_A/Appendix_A._Instance_and_Docker_container_lifecycles/","text":"Instance and Docker container lifecycles Overview Instance lifecycle stages Docker container lifecycle stages Overview In the context of Cloud Pipeline , both life cycles are tied together. A user is charged for instance usage. Note : to run a container you need a launched instance. In the Cloud Pipeline , instances are bought for hourly rates with a minimum of one hour. This is due to the following reasons: It helps to decrease the time for node relaunch. It helps to decrease the time for Docker image and data (\"type\": \"common\") download. Most pipelines will take much longer than 1 hour to complete. Note : same instance configuration must be used in order to reuse currently active nodes. If the node has no running jobs 10 minutes before the new hour of payment begins, it will be terminated. Instance lifecycle stages Note : instance lifecycle stages are presented on the example of one of the supported instances - EC2 of AWS Cloud Provider. The general overview of the EC2 instance lifecycle - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-lifecycle.html . In the Cloud Pipeline there are 3 stages: Pending state - no billing. Running state - you're billed for each second, with a one-minute minimum, that you keep the instance running. Terminated state - no billing. Docker container lifecycle stages A general overview of the Docker container lifecycle - https://medium.com/@nagarwal/lifecycle-of-docker-container-d2da9f85959 . In the Cloud Pipeline there are 2 stages: Running state - possible to execute some commands inside the container. Terminated state - the container is not accessible.","title":"Appendix A. Instance and Docker container lifecycles"},{"location":"manual/Appendix_A/Appendix_A._Instance_and_Docker_container_lifecycles/#instance-and-docker-container-lifecycles","text":"Overview Instance lifecycle stages Docker container lifecycle stages","title":"Instance and Docker container lifecycles"},{"location":"manual/Appendix_A/Appendix_A._Instance_and_Docker_container_lifecycles/#overview","text":"In the context of Cloud Pipeline , both life cycles are tied together. A user is charged for instance usage. Note : to run a container you need a launched instance. In the Cloud Pipeline , instances are bought for hourly rates with a minimum of one hour. This is due to the following reasons: It helps to decrease the time for node relaunch. It helps to decrease the time for Docker image and data (\"type\": \"common\") download. Most pipelines will take much longer than 1 hour to complete. Note : same instance configuration must be used in order to reuse currently active nodes. If the node has no running jobs 10 minutes before the new hour of payment begins, it will be terminated.","title":"Overview"},{"location":"manual/Appendix_A/Appendix_A._Instance_and_Docker_container_lifecycles/#instance-lifecycle-stages","text":"Note : instance lifecycle stages are presented on the example of one of the supported instances - EC2 of AWS Cloud Provider. The general overview of the EC2 instance lifecycle - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-lifecycle.html . In the Cloud Pipeline there are 3 stages: Pending state - no billing. Running state - you're billed for each second, with a one-minute minimum, that you keep the instance running. Terminated state - no billing.","title":"Instance lifecycle stages"},{"location":"manual/Appendix_A/Appendix_A._Instance_and_Docker_container_lifecycles/#docker-container-lifecycle-stages","text":"A general overview of the Docker container lifecycle - https://medium.com/@nagarwal/lifecycle-of-docker-container-d2da9f85959 . In the Cloud Pipeline there are 2 stages: Running state - possible to execute some commands inside the container. Terminated state - the container is not accessible.","title":"Docker container lifecycle stages"},{"location":"manual/Appendix_B/Appendix_B._Working_with_a_Project/","text":"Working with a Project Project is a special type of Folder . Projects might be used to organize data and metadata and simplify analysis runs for a large data set. Also, you can set a project attributes as parameters of analysis method configuration. Note : learn more about metadata here . Create a project To create a project you need WRITE permissions for the parent folder and the ROLE_FOLDER_MANAGER role. For more information see 13. Permissions . To create a project in the system, the following steps shall be performed: Navigate to a folder of a future-project destination. Click + Create \u2192 PROJECT . Note PROJECT is an oncology project template. It supports the default structure of an oncology project. The system suggests that you name a new project. Enter a name. Click OK button. You'll be fetched into the new project's parent folder page automatically. The folder has a default attribute: type = project (see picture above, 2 ). Note : It's an essential attribute for a project. Based on this attribute, the system recognizes a folder as a project. If the attribute is removed, the folder is no longer a project. The new project contains (see picture above, 1 ): Method-Configuration folder. It will be a container for all your methods to run. Storage . The storage will be empty. The name of the storage will be set as a default. Here you can see default settings of new storage. History . This is a table that contains all at any time scheduled runs of a project's methods. For now, it's empty. The picture below illustrated how the table looks with an existing run's history. The History GUI repeats the Run space. For more details see 11. Manage Runs . How to add metadata in the project, see here .","title":"Appendix B. Working with a Project"},{"location":"manual/Appendix_B/Appendix_B._Working_with_a_Project/#working-with-a-project","text":"Project is a special type of Folder . Projects might be used to organize data and metadata and simplify analysis runs for a large data set. Also, you can set a project attributes as parameters of analysis method configuration. Note : learn more about metadata here .","title":"Working with a Project"},{"location":"manual/Appendix_B/Appendix_B._Working_with_a_Project/#create-a-project","text":"To create a project you need WRITE permissions for the parent folder and the ROLE_FOLDER_MANAGER role. For more information see 13. Permissions . To create a project in the system, the following steps shall be performed: Navigate to a folder of a future-project destination. Click + Create \u2192 PROJECT . Note PROJECT is an oncology project template. It supports the default structure of an oncology project. The system suggests that you name a new project. Enter a name. Click OK button. You'll be fetched into the new project's parent folder page automatically. The folder has a default attribute: type = project (see picture above, 2 ). Note : It's an essential attribute for a project. Based on this attribute, the system recognizes a folder as a project. If the attribute is removed, the folder is no longer a project. The new project contains (see picture above, 1 ): Method-Configuration folder. It will be a container for all your methods to run. Storage . The storage will be empty. The name of the storage will be set as a default. Here you can see default settings of new storage. History . This is a table that contains all at any time scheduled runs of a project's methods. For now, it's empty. The picture below illustrated how the table looks with an existing run's history. The History GUI repeats the Run space. For more details see 11. Manage Runs . How to add metadata in the project, see here .","title":"Create a project"},{"location":"manual/Appendix_C/Appendix_C._Working_with_autoscaled_cluster_runs/","text":"Working with autoscaled cluster runs Overview Limitations How it works Finding expired jobs Launching workers Determing worker instance types Killing excessive workers Preventing deadlocks Configurations Examples Homogeneous cluster Hybrid cluster Overview Cloud Pipeline has support for launching runs using a single machine or a cluster of machines. Moreover Cloud Pipeline allows to launch so-called autoscaled clusters which are basically clusters of machines with dynamic size. It means that during the run execution additional worker machines can be attached to the cluster as well as removed from it. By the way an autoscaled cluster can have so-called persistent workers which cannot be affected by the autoscaler. Limitations Currently only the SGE-based runs can be used efficiently with an autoscaled cluster capability. And only the CPU requirements of the SGE jobs are considered while calculating cluster unsatisfied resources. How it works The cluster autoscaling tries to be intuitive and is pretty straightforward in most cases. Nevertheless it can get cumbersome with all the allowed customization parameters. The overall autoscaling approach is briefly described below. Please notice that all the described steps are executed repeatedly for all the cluster lifetime depending on the current situation. Finding expired jobs Autoscaler is a background process which constantly watches the SGE queues in order to find expired jobs . Basically expired jobs are the jobs that wait in any queue for more than some predefined time. Launching workers Once expired jobs are found the autoscaler tries to launch an additional worker. The only case additional worker won't be launched is if all allowed additional workers are already set up. Determing worker instance types Additional worker instance types can vary from only a master instance type up to some instance types family. Therefore there are homogeneous clusters that launches only the master instance type machines and hybrid clusters which launches instance types from some instance type family. By default all autoscaled clusters are homogeneous. If an autoscaled cluster is hybrid and it is launching an additional worker then its instance type will be resolved based on the amount of unsatisfied CPU requirements of all pending jobs. The autoscaler will try to launch the smallest allowed instance from a specific instance type family that can process all the pending jobs simultaneously. For instance if there are two pending jobs with the CPU requirements of 4 and 8 then the autoscaler will try to launch the instance which has at least 12 CPUs. Killing excessive workers Once required additional workers are set up the cluster gets bigger and more jobs can be executed simultaneously. At some point while jobs finish their execution some additional workers may become excessive. In this case the autoscaler will check if all the queues were empty for at least some predefined time and try to remove excessive additional workers from the cluster. Preventing deadlocks In some specific cases autoscaled clusters may enter the deadlock situation. In terms of autoscaled clusters a deadlock is a situation when submitted jobs cannot be executed in the best allowed cluster configuration. For example if a hybrid autoscaled cluster which can have at most a machine with 64 CPUs is used to submit a job with a requirement of 100 CPUs then the job won't be ever executed and will stuck in queue forever. Fortunately the autoscaler can detect such deadlocks and prevent them simply killing jobs that cannot be executed anyway. It most likely will fail the run execution but it is reasonable since the cluster usage is not properly configured in the run. Besides an actual deadlock situation where are some similar cases when the cluster has reached its full capacity but some of the jobs are still pending. It is not the deadlock situation since the jobs can be still executed but in the different cluster configuration. The autoscaler will detect such situations and replace weak addition workers with a better ones. Configurations System parameter Description CP_CAP_AUTOSCALE_HYBRID Enables hybrid cluster mode. It means that additional worker type can vary within either master instance type family or CP_CAP_AUTOSCALE_HYBRID_FAMILY if specified. CP_CAP_AUTOSCALE_HYBRID_FAMILY Hybrid cluster additional worker instance type family. For example c5 or r5 instance families can be used for AWS. CP_CAP_AUTOSCALE_HYBRID_MAX_CORE_PER_NODE The maximum number of cores that hybrid cluster additional worker instances can have. CP_CAP_AUTOSCALE_VERBOSE Enables verbose logging. CP_CAP_AUTOSCALE_PRICE_TYPE Cluster additional worker instance price type. Defaults to master instance price type. CP_CAP_SGE_MASTER_CORES The number of cores that master run can use for job submissions. If set to 0 then no jobs will be executed on the master. Defaults to all the master instance cores. CP_CAP_SGE_WORKER_FREE_CORES The number of cores that all worker and master runs have to reserve from job submissions. Defaults to 0 which means that all instance cores will be used for job submissions. Examples Homogeneous cluster In this example we will see how an autoscaled cluster behaves on different conditions. Let's say we have launch an autoscaled cluster of m5.large instances (2 CPUs per instance) with no persistent workers which can scale up to 2 workers. You can find information on how to do that in the corresponding page . On the Runs page we can see that the launched run doesn't have any workers. On the Run logs page click the SSH button when it will become available. In the opened terminal submit 10 single-core jobs to SGE queue using the following command qsub -b y -t 1:10 sleep 10m . Check that the jobs have been successfully submitted using qstat command: Within several minutes an additional worker will be automatically added to out cluster: Once the additional worker is fully initialized we can see that it takes several jobs from the queue: A few moments later if there are still pending jobs in the queue a second additional worker will be created: And once it is initialized the worker will grab remaining jobs from the queue as well: As long as the most of the jobs are finishing and the workers become excessive they will be removed from the cluster: At first one of the workers becomes stopped: And then the last one becomes stopped too: From this point the cluster can scale up and down again and again depending on the workload. Hybrid cluster In this example we will see how a hybrid cluster behaves on different workloads. To start a hybrid cluster you have to configure a regular autoscaling cluster as described here . Only the additional system parameter CP_CAP_AUTOSCALE_HYBRID have to be specified. Open the Advanced tab on the launch page and click Add system parameter button. Then type down HYBRID into the search field of the opened popup and select CP_CAP_AUTOSCALE_HYBRID system parameter. Once the parameter is selected click OK (1) button. Configure an autoscaled cluster with the same configuration as in the example above and launch the run. On the Runs page click on the launched run. Wait for the SSH button to appear and click on it. In the opened terminal submit 10 single-core jobs to SGE queue using the following command qsub -b y -t 1:10 sleep 10m . Check that the jobs have been successfully submitted using qstat command: If you go back to the run page and wait for a few minutes then you will see that an additional worker is launched. Click on the additional worker run link to find out what instance type it uses. The additional worker instance type in this case is m5.2xlarge which has 8 CPUs and 32GB of RAM. Please notice that the instance type selecting mechanism is highly situational and the resulting instance type can be different between launches. Once the additional worker is initialized it will grab all the remaining jobs from the queue. And after about 10 minutes an additional worker will be scaled down since there are no pending jobs anymore. From this point the cluster can scale up and down again and again depending on the workload.","title":"Appendix C. Working with autoscaled cluster runs"},{"location":"manual/Appendix_C/Appendix_C._Working_with_autoscaled_cluster_runs/#working-with-autoscaled-cluster-runs","text":"Overview Limitations How it works Finding expired jobs Launching workers Determing worker instance types Killing excessive workers Preventing deadlocks Configurations Examples Homogeneous cluster Hybrid cluster","title":"Working with autoscaled cluster runs"},{"location":"manual/Appendix_C/Appendix_C._Working_with_autoscaled_cluster_runs/#overview","text":"Cloud Pipeline has support for launching runs using a single machine or a cluster of machines. Moreover Cloud Pipeline allows to launch so-called autoscaled clusters which are basically clusters of machines with dynamic size. It means that during the run execution additional worker machines can be attached to the cluster as well as removed from it. By the way an autoscaled cluster can have so-called persistent workers which cannot be affected by the autoscaler.","title":"Overview"},{"location":"manual/Appendix_C/Appendix_C._Working_with_autoscaled_cluster_runs/#limitations","text":"Currently only the SGE-based runs can be used efficiently with an autoscaled cluster capability. And only the CPU requirements of the SGE jobs are considered while calculating cluster unsatisfied resources.","title":"Limitations"},{"location":"manual/Appendix_C/Appendix_C._Working_with_autoscaled_cluster_runs/#how-it-works","text":"The cluster autoscaling tries to be intuitive and is pretty straightforward in most cases. Nevertheless it can get cumbersome with all the allowed customization parameters. The overall autoscaling approach is briefly described below. Please notice that all the described steps are executed repeatedly for all the cluster lifetime depending on the current situation.","title":"How it works"},{"location":"manual/Appendix_C/Appendix_C._Working_with_autoscaled_cluster_runs/#finding-expired-jobs","text":"Autoscaler is a background process which constantly watches the SGE queues in order to find expired jobs . Basically expired jobs are the jobs that wait in any queue for more than some predefined time.","title":"Finding expired jobs"},{"location":"manual/Appendix_C/Appendix_C._Working_with_autoscaled_cluster_runs/#launching-workers","text":"Once expired jobs are found the autoscaler tries to launch an additional worker. The only case additional worker won't be launched is if all allowed additional workers are already set up.","title":"Launching workers"},{"location":"manual/Appendix_C/Appendix_C._Working_with_autoscaled_cluster_runs/#determing-worker-instance-types","text":"Additional worker instance types can vary from only a master instance type up to some instance types family. Therefore there are homogeneous clusters that launches only the master instance type machines and hybrid clusters which launches instance types from some instance type family. By default all autoscaled clusters are homogeneous. If an autoscaled cluster is hybrid and it is launching an additional worker then its instance type will be resolved based on the amount of unsatisfied CPU requirements of all pending jobs. The autoscaler will try to launch the smallest allowed instance from a specific instance type family that can process all the pending jobs simultaneously. For instance if there are two pending jobs with the CPU requirements of 4 and 8 then the autoscaler will try to launch the instance which has at least 12 CPUs.","title":"Determing worker instance types"},{"location":"manual/Appendix_C/Appendix_C._Working_with_autoscaled_cluster_runs/#killing-excessive-workers","text":"Once required additional workers are set up the cluster gets bigger and more jobs can be executed simultaneously. At some point while jobs finish their execution some additional workers may become excessive. In this case the autoscaler will check if all the queues were empty for at least some predefined time and try to remove excessive additional workers from the cluster.","title":"Killing excessive workers"},{"location":"manual/Appendix_C/Appendix_C._Working_with_autoscaled_cluster_runs/#preventing-deadlocks","text":"In some specific cases autoscaled clusters may enter the deadlock situation. In terms of autoscaled clusters a deadlock is a situation when submitted jobs cannot be executed in the best allowed cluster configuration. For example if a hybrid autoscaled cluster which can have at most a machine with 64 CPUs is used to submit a job with a requirement of 100 CPUs then the job won't be ever executed and will stuck in queue forever. Fortunately the autoscaler can detect such deadlocks and prevent them simply killing jobs that cannot be executed anyway. It most likely will fail the run execution but it is reasonable since the cluster usage is not properly configured in the run. Besides an actual deadlock situation where are some similar cases when the cluster has reached its full capacity but some of the jobs are still pending. It is not the deadlock situation since the jobs can be still executed but in the different cluster configuration. The autoscaler will detect such situations and replace weak addition workers with a better ones.","title":"Preventing deadlocks"},{"location":"manual/Appendix_C/Appendix_C._Working_with_autoscaled_cluster_runs/#configurations","text":"System parameter Description CP_CAP_AUTOSCALE_HYBRID Enables hybrid cluster mode. It means that additional worker type can vary within either master instance type family or CP_CAP_AUTOSCALE_HYBRID_FAMILY if specified. CP_CAP_AUTOSCALE_HYBRID_FAMILY Hybrid cluster additional worker instance type family. For example c5 or r5 instance families can be used for AWS. CP_CAP_AUTOSCALE_HYBRID_MAX_CORE_PER_NODE The maximum number of cores that hybrid cluster additional worker instances can have. CP_CAP_AUTOSCALE_VERBOSE Enables verbose logging. CP_CAP_AUTOSCALE_PRICE_TYPE Cluster additional worker instance price type. Defaults to master instance price type. CP_CAP_SGE_MASTER_CORES The number of cores that master run can use for job submissions. If set to 0 then no jobs will be executed on the master. Defaults to all the master instance cores. CP_CAP_SGE_WORKER_FREE_CORES The number of cores that all worker and master runs have to reserve from job submissions. Defaults to 0 which means that all instance cores will be used for job submissions.","title":"Configurations"},{"location":"manual/Appendix_C/Appendix_C._Working_with_autoscaled_cluster_runs/#examples","text":"","title":"Examples"},{"location":"manual/Appendix_C/Appendix_C._Working_with_autoscaled_cluster_runs/#homogeneous-cluster","text":"In this example we will see how an autoscaled cluster behaves on different conditions. Let's say we have launch an autoscaled cluster of m5.large instances (2 CPUs per instance) with no persistent workers which can scale up to 2 workers. You can find information on how to do that in the corresponding page . On the Runs page we can see that the launched run doesn't have any workers. On the Run logs page click the SSH button when it will become available. In the opened terminal submit 10 single-core jobs to SGE queue using the following command qsub -b y -t 1:10 sleep 10m . Check that the jobs have been successfully submitted using qstat command: Within several minutes an additional worker will be automatically added to out cluster: Once the additional worker is fully initialized we can see that it takes several jobs from the queue: A few moments later if there are still pending jobs in the queue a second additional worker will be created: And once it is initialized the worker will grab remaining jobs from the queue as well: As long as the most of the jobs are finishing and the workers become excessive they will be removed from the cluster: At first one of the workers becomes stopped: And then the last one becomes stopped too: From this point the cluster can scale up and down again and again depending on the workload.","title":"Homogeneous cluster"},{"location":"manual/Appendix_C/Appendix_C._Working_with_autoscaled_cluster_runs/#hybrid-cluster","text":"In this example we will see how a hybrid cluster behaves on different workloads. To start a hybrid cluster you have to configure a regular autoscaling cluster as described here . Only the additional system parameter CP_CAP_AUTOSCALE_HYBRID have to be specified. Open the Advanced tab on the launch page and click Add system parameter button. Then type down HYBRID into the search field of the opened popup and select CP_CAP_AUTOSCALE_HYBRID system parameter. Once the parameter is selected click OK (1) button. Configure an autoscaled cluster with the same configuration as in the example above and launch the run. On the Runs page click on the launched run. Wait for the SSH button to appear and click on it. In the opened terminal submit 10 single-core jobs to SGE queue using the following command qsub -b y -t 1:10 sleep 10m . Check that the jobs have been successfully submitted using qstat command: If you go back to the run page and wait for a few minutes then you will see that an additional worker is launched. Click on the additional worker run link to find out what instance type it uses. The additional worker instance type in this case is m5.2xlarge which has 8 CPUs and 32GB of RAM. Please notice that the instance type selecting mechanism is highly situational and the resulting instance type can be different between launches. Once the additional worker is initialized it will grab all the remaining jobs from the queue. And after about 10 minutes an additional worker will be scaled down since there are no pending jobs anymore. From this point the cluster can scale up and down again and again depending on the workload.","title":"Hybrid cluster"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/","text":"Costs management rationale and concepts Restricting the size of the compute cluster Compute nodes size Number of compute nodes Spot/Preemptible compute instances Instance PAUSE/RESUME IDLE instances Scheduled instances PAUSE/RESUME Spending quotas Billing reports General costs report User report Billing center/group report Resources reports Storages report Compute instances report Report aggregation level according to the permissions Any job, run in the Cloud Pipeline, or any file placed into the Cloud Storage - cost money. These costs are incurred by the underlying provider: AWS/GCP/Azure/etc. At a scale, when hundreds of the platform users submit the jobs and utilize the storage capacity, the cloud bill may grow to a quite high level. Even more, some of the users may not be aware of the underlying billing and keep the instances up and running for months, but not performing any productive work. To address these issues, Cloud Pipeline provides a number of features, that allow to reduce and control the costs. This machinery can be split into separate sections: Features that allow to minimize the costs , e.g. by limiting the size of the compute nodes or automatically stopping IDLE jobs Reporting features , that make users aware of the workload costs, e.g. by notifying the users on the IDLE jobs or providing interactive spending dashboard This document lists the concepts and corresponding features (either Ready and WIP as well). Restricting the size of the compute cluster From the overall Cloud usage experience - the largest bills are generated by the compute resources, not the storages. And the key driver of the compute cost is the size/shape of the compute nodes used. Compute nodes size Each of the Cloud Providers offers lots of different compute instances families and types and most likely the users won't need that variety. Users may even choose huge GPU-enabled nodes by mistake, which are the most expensive. Cloud Pipeline allows to restrict which sizes are available to the users for a specific platform deployment, thus reducing a chance of spending money unused resources of a huge node. These restrictions can be applied to the overall platform and than fine-tuned for a specific users' group, specific user or a docker image. Number of compute nodes While we can restrict the size of each single compute node - it is still possible to spin up an uncontrolled number of smaller instances, which still costs a lot. Cloud Pipeline offers a way to restrict the number of simultaneously running Cloud instances in the following manner: Administrator may configure the overall number of compute nodes, that may be created for this particular platform deployment. This restriction will be in effect across all the users/groups/tools. E.g. if the administrators sets this parameter to 50 nodes and we already have 50 nodes running jobs - the 51st node won't be created. The corresponding job will sit in the queue until the previous runs finish and free the space for the new instance. Users may also spin-up on-demand clusters , i.e. a single job run which require more than a single compute node (e.g. a molecular dynamics job, which needs a 200 of CPU cores interconnected with MPI). In this case - administrator may limit the size of such on-demand clusters . E.g. limiting this parameter to 5 will allow user to launch several jobs in a cluster mode, but each cluster will be limited to max 5 hosts. Spot/Preemptible compute instances One of the very first cost reduction options to consider is the usage of Spot/Preemptible compute nodes. The behavior and savings are a bit different across the underlying Cloud providers, but in general all of them follow the strategy: allow to use the compute resources for a limited time at a greatly reduced costs. It's quite hard to predict the savings in general, but typically this will be ~twice cheaper than general on-demand instance type. For more details on the Spot/Preemptible compute instances details, please review the corresponding provider's documentation: AWS Spot instances GCP Preemptible VMs Azure Preemptible VMs While being quite cost-effective, such type of compute instances are not reliable for the long-running or stateful jobs. Cloud Pipeline encourages users to leverage the Spots/Preemptibles for: The Batch jobs, e.g. NGS pipelines which can be easily restarted Testing/Proofing tasks, when some script shall be debugged or a new package tested For the interactive tasks, e.g. Jupyter notebooks , which require online access from the users - such type of instance does not fit well. Another limitation of the Spots/Preemptibles is that such jobs cannot be Paused (i.e. stop consuming money, but keep the job's environment). Such kind of compute nodes can be fully terminated only. From the Cloud Pipeline's point of view this option is considered a Price Type and can be enabled at different levels: Globally, at a platform level (if the Spot/Preemptible can be used by any job in the platform) User group or a specific user levels (if can be used by the specific users) Pipeline level (if a particular pipeline can tolerate Spots/Preemptibles restarts) Docker image/version (if a particular image can be launched using a reduced cost instances) Instance PAUSE/RESUME From the existing usage scenarios the best compute costs reduction was observed, when the interactive tools were stopped while not used. The typical use case here is: User launches some IDE (e.g. RStudio/Jupyter) Works during the day Keeps the instance running over the night or weekends This introduces really high spendings, but without any actual outcome, as the instance is doing nothing. To address this, Cloud Pipeline allows to PAUSE and RESUME any instance/job, which is created using on-demand price type. While the instance is paused - compute is not charged, but the job's environment and filesystem is persisted. Once required - the instance can be resumed . Under the hood, the \"real\" compute instances are stopped/deallocated and than restarted. Cloud Pipeline takes care of the software state persistance and restore. In general, this feature is available via the Web GUI and API and the users are in charge of performing this pause/resume operation. API allows to automate this procedure (e.g. based on the schedule or resources usage) and the subsequent sections describe this approach. See Manage runs lifecycles for more details. IDLE instances This section extends the \"plain\" PAUSE/RESUME by managing the IDLE instances. Besides the describe above night-time/weekends cases, here we also consider under-utilization of the compute resource. E.g. if the user selects 96-cored instance (maybe by mistake), but runs a single-threaded application. In this case lots of CPU resources are just wasted. Cloud Pipeline mixes together the PAUSE/RESUME and instances workload monitoring and offers a set of policies, which can be applied to the compute instances to take care of such IDLE instances. These policies can be used in the following manner: Platform administrators can define thresholds for the hardware utilization (CPU/GPU usage) and overall run duration. If some job is considered as IDLE (the hardware utilization is below the threshold for the configured period of time) \u2013 a number of actions can be performed (a single or a mixture of them): Job is marked as IDLE in the GUI Email Notification is sent (to the Adminstrators and the Owner of the instance) to make user aware of the event Job can be automatically paused (if it\u2019s price type and cluster mode are compatible with the PAUSE operation) Job can be terminated Scheduled instances PAUSE/RESUME For certain use cases (e.g. when then user leverages Cloud Pipeline as a development/research environment) users launch runs and keep them running all the time, including weekends and holidays. As shown in the examples above. One can use the IDLE policy to PAUSE such jobs, but the IDLE status is set only when the threshold is exceeded. This time (while the platform will decide to treat a job as IDLE ) also costs some money. So to manage the jobs that shall be stopped for the non-working hours - a PAUSE/RESUME schedule is used. User (who have permissions to pause/resume a run) is able to set a schedule for an active run or a run being launched Schedule is defined as a list of rules (user shall be able to specify any number of them): Action: PAUSE or RESUME Recurrence: Daily: every N days, time Weekly: every N weeks, weekday, time User is able to create/view/modify/delete schedule rules anytime run is active (i.e. running or paused) This is applied only to the \"Pauseable\" runs (i.e. On-demand/Non-cluster) Spending quotas While the described options are mostly focused on the soft cost reduction (e.g. help the user to decide on the instance type), the platform shall be also capable of enforcing certain policies if the soft restrictions didn't work. This kind of restriction is controlled by the spending quotas . This functionality allows to apply policies to the platform's entities: User - quotas can be applied to a specific user Users group - group of users can be restricted separately as well Billing/Cost center - this is a different dimension of the users grouping. Typically this is a meta-group , which is not used to apply security permissions. Such groups describe, e.g. the departments, which have separate budget and can manage it Global - administrator can define what is the overall budget for the platform Each of those user groupings can be managed by the platform administrator or an authorized manager (who is assigned a corresponding platform role). For each of the quotas configured \u2013 there is an option to specify the thresholds (e.g. 50% / 75% / 100%), when a specific action shall be performed by the platform: \"Notify\" (default) - this action will only notify corresponding users that a limited is exceeded. The access to the platform shall not be restricted \"Read-only keep jobs\" - corresponding users that a limited is exceeded and then will have the \"read-only\" access to the platform, they won't be able to launch new jobs. All active runs will be kept \"Read-only stop jobs\" \u2013 same as previous, but all the active runs will be stopped \"Block\" - corresponding users will be blocked and won\u2019t have any access to the platform. After a time period for which the limit is set, the access to the platform shall be restored to users according to their permissions. Billing reports To make the users aware of the current bills and quota policy attached - platform logs the information on the compute and storage costs. This reporting feature is available in two flavors: When a compute job is launched - user is notified on the hourly cost of the chosen hardware configuration Compute and storage costs are aggregated into the ElasticSearch index on a daily basis and can be queried to build the historical reports General costs report The latter one offers graphical/tabular visualization options to get the insights on a current or a previous period bills By these forms users can view the whole system spendings, or spendings divided by the specific resources. Presented metrics (resources): costs of launching compute instances, used for tools/pipelines runs. There could be: CPU instances GPU instances costs of storing user data in storages. There could be costs of storing: in Object storages in File storages info about auxiliary costs isn't supported yet All costs are aggregated and displayed for the specific period (by default - the current calendar month). Selected specific period, for which the costs should be calculated and displayed, is called \" current \". Also, for comparison, the costs for the analogical previous period are displayed in diagrams/charts (where this data is available). The user can select the desired (current) period to view the costs incurred: from one of predefined periods. For each of them there will be a specific \"previous\" period: Month . If the month is selected as current period - the \"previous\" period will be the previous calendar month of the selected one Quarter . If the quarter is selected as current period - the \"previous\" period will be the same quarter in the previous calendar year Year . If the year is selected as current period - the \"previous\" period will be the previous calendar year custom period. That period is configured by the user manually in months and can have any duration. The \"previous\" period isn't displayed in this case By default, the \"Billing Visualization\" form looks like on the picture above. It contains: General report table with the following data: current and previous periods, for which the costs are calculated summary spendings according to selected configurations in the toolbar for the current and previous periods the difference between the costs of current period and the previous one in percentage with a certain mark to determine whether spendings grow or reduce The main summary costs diagram. On this diagram, the user can see summary spendings over the current and the previous time periods according to selected configurations in the toolbar. This data could be displayed with the accumulation (as line chart ) or as fact (as bar chart with actual spending values in each time point of the period without accumulation) if the selected \"current\" period coincides with the current corresponding calendar period - there a slice line is displayed on the current calendar date: on its intersection with the \"current\" chart line, the amount of money which was spent from the beginning of the period to the current date is displayed and on its intersection with the \"previous\" chart line - the amount of money which was spent for the same previous time period is displayed the user can hover any point of the \"current\"/\"previous\" line of the diagram - the summary spendings at the corresponding datetime appear in the tooltip the aggregation value of timeline division for selected periods less than 4 months is 1 day, in other cases - 1 month Note : costs of the current calendar day is being aggregated in the following day, so they aren't displayed in diagrams With accumulation: And actual values (without accumulation): The spendings bar chart with the Resources division. On this chart, the user can view the division of summary spendings in a selected time period over the resource groups - Storages (with division on Object and File storages) and Compute Instances (with division on CPU and GPU ). Also, for each resource, the summary spendings for the same previous period are presented as well: The spendings bar chart with the Billing centers division. On this chart, the user can view the division of summary spendings in a selected time period over the billing centers (the system displays only top N most costly billing centers). For each displayed center/group, the summary spendings for the same previous period is presented as well: The report toolbar with the following controls: the \"period selector\" to select a current time period for the report - from one of predefined periods or the manual custom period the additional calendar control - to select another \"current\" period that doesn't coincide with the current calendar period (for predefined periods) or manually select desired period duration (for custom period) the control to select the specific billing center (user group) or user. By default, all available centers/users are selected the export control to download a data of displayed report in *.csv format to the local workstation The billing report form described above is the general. To get info/charts of summary spendings in any desired period (from available) you should select the corresponding period via the Period selector and Calendar control (if necessary). To get info/charts of summary spendings in the selected period only for the specific resource group you can click that resource in the Resources chart or use the corresponding item of the menu in the left side of the page (for more details see sections below ). User report To get info/charts of summary spendings in the selected period only for the specific user you can select the desired user from the dropdown list in the main toolbar. In this case: the costs data will be calculated and displayed only for the selected user (in the general report table, in the main diagram and in the resources chart) the spendings bar chart with the Billing centers division will disappear, e.g.: Billing center/group report To get info/charts of summary spendings in the selected period only for the specific Billing center you can select the desired one from the dropdown list in the main toolbar or by click the corresponding Billing Center's bar in the Billing centers division chart. In this case: the costs data will be calculated and displayed only for the selected Billing center (summary for all its users - in the general report table, in the main diagram and in the resources chart) the bar chart with the top N of users' spendings in the selected period (among all users in that Billing center) will appear the table with short info about summary spendings of each user of that Billing center in the selected period (also info contains summary duration and count of the runs launched by the user, used storages volume) will appear, e.g.: Resources reports If you wish - you may get info/charts of summary spendings not for all resources but for the specific resource group. Also, for these charts you may select the desired time period and specific user/Billing center (group) to view costs in the way analogical as described above. Storages report To get info/charts of summary spendings in the selected period only for the Storages resource group (costs of the storing data in storages) you can click the corresponding item of the menu in the left side of the page: In this case, the summary costs for all used storages during the selected period by selected users/Billing center will be calculated and displayed (both types - Object/File storages). In the appeared page, you can see: General report table with summary costs for all used storages during the selected and previous ( if it's available ) periods The summary Storages spendings diagram over the current and the previous time periods according to selected configurations in the toolbar. This data could be displayed with the accumulation (as line chart ) or as fact (as bar chart with actual spending values in each time point of the period without accumulation) The spendings bar chart with top N most costly storages used during the selected period compared with the previous one ( if it's available ) The detailed spendings table under the chart with the full list of storages used during the selected period by selected user/Billing center Example: By default, in this form there isn't the division by storage type (Object/File). All data is calculated and displayed summary for both types. If you want to view costs only for storages with the specific type - you can select the corresponding one in the menu at the left side of the page. E.g., for Object storages: Compute instances report To get info/charts of summary spendings in the selected period only for the Compute instances resource group (costs of the launching instances for running tools/pipelines) you can click the corresponding item of the menu in the left side of the page: In this case, the summary costs for all launched instances during the selected period by selected users/Billing center will be calculated and displayed (launched tools/pipelines and both instance types - CPU/GPU). In the appeared page, you can see: General report table with summary costs for all runs launched in the selected and previous ( if it's available ) periods The summary Compute instances spendings diagram over the current and the previous time periods according to selected configurations in the toolbar. This data could be displayed with the accumulation (as line chart ) or as fact (as bar chart with actual spending values in each time point of the period without accumulation) The spendings bar chart with top N most costly instance types launched in the selected period compared to the previous one ( if it's available ) The detailed spendings table under that chart with the full list of launched instance types in the selected period The spendings bar chart with top N most costly tools launched in the selected period compared to the previous one ( if it's available ) The detailed spendings table under that chart with the full list of launched tools in the selected period The spendings bar chart with top N most costly pipelines launched in the selected period compared to the previous one ( if it's available ) The detailed spendings table under that chart with the full list of launched pipelines in the selected period Additional control that allows to change displaying of spendings bar charts with the following possible values: cost (default) - on 3 above described bar charts top N most costly objects (instance types, tools, pipelines) are displayed. Data Unit - currency usage - on 3 above described bar charts top N most involved objects (instance types, tools, pipelines) are displayed. Data Unit - usage hours runs - on 3 above described bar charts top N objects (instance types, tools, pipelines) with the largest runs count are displayed. Data Unit - runs count Example: By default, in this form there isn't the division by workload type (CPU/GPU). All data is calculated and displayed summary for both types. If you want to view costs only for instances with the specific workload type - you can select the corresponding one in the menu at the left side of the page. E.g., for CPU: Report aggregation level according to the permissions Available reports are calculated and displayed according to the user permissions: general user can view: general report with only own costs (User report, without Billing center report) resources report with only own costs Billing center (group) leader can view: general report with costs of that Billing center Billing center report user report for any user of that Billing center resources report with costs of that Billing center on the whole and for any user of that Billing center separately platform admin can view all available reports","title":"Appendix D. Costs management"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#costs-management-rationale-and-concepts","text":"Restricting the size of the compute cluster Compute nodes size Number of compute nodes Spot/Preemptible compute instances Instance PAUSE/RESUME IDLE instances Scheduled instances PAUSE/RESUME Spending quotas Billing reports General costs report User report Billing center/group report Resources reports Storages report Compute instances report Report aggregation level according to the permissions Any job, run in the Cloud Pipeline, or any file placed into the Cloud Storage - cost money. These costs are incurred by the underlying provider: AWS/GCP/Azure/etc. At a scale, when hundreds of the platform users submit the jobs and utilize the storage capacity, the cloud bill may grow to a quite high level. Even more, some of the users may not be aware of the underlying billing and keep the instances up and running for months, but not performing any productive work. To address these issues, Cloud Pipeline provides a number of features, that allow to reduce and control the costs. This machinery can be split into separate sections: Features that allow to minimize the costs , e.g. by limiting the size of the compute nodes or automatically stopping IDLE jobs Reporting features , that make users aware of the workload costs, e.g. by notifying the users on the IDLE jobs or providing interactive spending dashboard This document lists the concepts and corresponding features (either Ready and WIP as well).","title":"Costs management rationale and concepts"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#restricting-the-size-of-the-compute-cluster","text":"From the overall Cloud usage experience - the largest bills are generated by the compute resources, not the storages. And the key driver of the compute cost is the size/shape of the compute nodes used.","title":"Restricting the size of the compute cluster"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#compute-nodes-size","text":"Each of the Cloud Providers offers lots of different compute instances families and types and most likely the users won't need that variety. Users may even choose huge GPU-enabled nodes by mistake, which are the most expensive. Cloud Pipeline allows to restrict which sizes are available to the users for a specific platform deployment, thus reducing a chance of spending money unused resources of a huge node. These restrictions can be applied to the overall platform and than fine-tuned for a specific users' group, specific user or a docker image.","title":"Compute nodes size"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#number-of-compute-nodes","text":"While we can restrict the size of each single compute node - it is still possible to spin up an uncontrolled number of smaller instances, which still costs a lot. Cloud Pipeline offers a way to restrict the number of simultaneously running Cloud instances in the following manner: Administrator may configure the overall number of compute nodes, that may be created for this particular platform deployment. This restriction will be in effect across all the users/groups/tools. E.g. if the administrators sets this parameter to 50 nodes and we already have 50 nodes running jobs - the 51st node won't be created. The corresponding job will sit in the queue until the previous runs finish and free the space for the new instance. Users may also spin-up on-demand clusters , i.e. a single job run which require more than a single compute node (e.g. a molecular dynamics job, which needs a 200 of CPU cores interconnected with MPI). In this case - administrator may limit the size of such on-demand clusters . E.g. limiting this parameter to 5 will allow user to launch several jobs in a cluster mode, but each cluster will be limited to max 5 hosts.","title":"Number of compute nodes"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#spotpreemptible-compute-instances","text":"One of the very first cost reduction options to consider is the usage of Spot/Preemptible compute nodes. The behavior and savings are a bit different across the underlying Cloud providers, but in general all of them follow the strategy: allow to use the compute resources for a limited time at a greatly reduced costs. It's quite hard to predict the savings in general, but typically this will be ~twice cheaper than general on-demand instance type. For more details on the Spot/Preemptible compute instances details, please review the corresponding provider's documentation: AWS Spot instances GCP Preemptible VMs Azure Preemptible VMs While being quite cost-effective, such type of compute instances are not reliable for the long-running or stateful jobs. Cloud Pipeline encourages users to leverage the Spots/Preemptibles for: The Batch jobs, e.g. NGS pipelines which can be easily restarted Testing/Proofing tasks, when some script shall be debugged or a new package tested For the interactive tasks, e.g. Jupyter notebooks , which require online access from the users - such type of instance does not fit well. Another limitation of the Spots/Preemptibles is that such jobs cannot be Paused (i.e. stop consuming money, but keep the job's environment). Such kind of compute nodes can be fully terminated only. From the Cloud Pipeline's point of view this option is considered a Price Type and can be enabled at different levels: Globally, at a platform level (if the Spot/Preemptible can be used by any job in the platform) User group or a specific user levels (if can be used by the specific users) Pipeline level (if a particular pipeline can tolerate Spots/Preemptibles restarts) Docker image/version (if a particular image can be launched using a reduced cost instances)","title":"Spot/Preemptible compute instances"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#instance-pauseresume","text":"From the existing usage scenarios the best compute costs reduction was observed, when the interactive tools were stopped while not used. The typical use case here is: User launches some IDE (e.g. RStudio/Jupyter) Works during the day Keeps the instance running over the night or weekends This introduces really high spendings, but without any actual outcome, as the instance is doing nothing. To address this, Cloud Pipeline allows to PAUSE and RESUME any instance/job, which is created using on-demand price type. While the instance is paused - compute is not charged, but the job's environment and filesystem is persisted. Once required - the instance can be resumed . Under the hood, the \"real\" compute instances are stopped/deallocated and than restarted. Cloud Pipeline takes care of the software state persistance and restore. In general, this feature is available via the Web GUI and API and the users are in charge of performing this pause/resume operation. API allows to automate this procedure (e.g. based on the schedule or resources usage) and the subsequent sections describe this approach. See Manage runs lifecycles for more details.","title":"Instance PAUSE/RESUME"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#idle-instances","text":"This section extends the \"plain\" PAUSE/RESUME by managing the IDLE instances. Besides the describe above night-time/weekends cases, here we also consider under-utilization of the compute resource. E.g. if the user selects 96-cored instance (maybe by mistake), but runs a single-threaded application. In this case lots of CPU resources are just wasted. Cloud Pipeline mixes together the PAUSE/RESUME and instances workload monitoring and offers a set of policies, which can be applied to the compute instances to take care of such IDLE instances. These policies can be used in the following manner: Platform administrators can define thresholds for the hardware utilization (CPU/GPU usage) and overall run duration. If some job is considered as IDLE (the hardware utilization is below the threshold for the configured period of time) \u2013 a number of actions can be performed (a single or a mixture of them): Job is marked as IDLE in the GUI Email Notification is sent (to the Adminstrators and the Owner of the instance) to make user aware of the event Job can be automatically paused (if it\u2019s price type and cluster mode are compatible with the PAUSE operation) Job can be terminated","title":"IDLE instances"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#scheduled-instances-pauseresume","text":"For certain use cases (e.g. when then user leverages Cloud Pipeline as a development/research environment) users launch runs and keep them running all the time, including weekends and holidays. As shown in the examples above. One can use the IDLE policy to PAUSE such jobs, but the IDLE status is set only when the threshold is exceeded. This time (while the platform will decide to treat a job as IDLE ) also costs some money. So to manage the jobs that shall be stopped for the non-working hours - a PAUSE/RESUME schedule is used. User (who have permissions to pause/resume a run) is able to set a schedule for an active run or a run being launched Schedule is defined as a list of rules (user shall be able to specify any number of them): Action: PAUSE or RESUME Recurrence: Daily: every N days, time Weekly: every N weeks, weekday, time User is able to create/view/modify/delete schedule rules anytime run is active (i.e. running or paused) This is applied only to the \"Pauseable\" runs (i.e. On-demand/Non-cluster)","title":"Scheduled instances PAUSE/RESUME"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#spending-quotas","text":"While the described options are mostly focused on the soft cost reduction (e.g. help the user to decide on the instance type), the platform shall be also capable of enforcing certain policies if the soft restrictions didn't work. This kind of restriction is controlled by the spending quotas . This functionality allows to apply policies to the platform's entities: User - quotas can be applied to a specific user Users group - group of users can be restricted separately as well Billing/Cost center - this is a different dimension of the users grouping. Typically this is a meta-group , which is not used to apply security permissions. Such groups describe, e.g. the departments, which have separate budget and can manage it Global - administrator can define what is the overall budget for the platform Each of those user groupings can be managed by the platform administrator or an authorized manager (who is assigned a corresponding platform role). For each of the quotas configured \u2013 there is an option to specify the thresholds (e.g. 50% / 75% / 100%), when a specific action shall be performed by the platform: \"Notify\" (default) - this action will only notify corresponding users that a limited is exceeded. The access to the platform shall not be restricted \"Read-only keep jobs\" - corresponding users that a limited is exceeded and then will have the \"read-only\" access to the platform, they won't be able to launch new jobs. All active runs will be kept \"Read-only stop jobs\" \u2013 same as previous, but all the active runs will be stopped \"Block\" - corresponding users will be blocked and won\u2019t have any access to the platform. After a time period for which the limit is set, the access to the platform shall be restored to users according to their permissions.","title":"Spending quotas"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#billing-reports","text":"To make the users aware of the current bills and quota policy attached - platform logs the information on the compute and storage costs. This reporting feature is available in two flavors: When a compute job is launched - user is notified on the hourly cost of the chosen hardware configuration Compute and storage costs are aggregated into the ElasticSearch index on a daily basis and can be queried to build the historical reports","title":"Billing reports"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#general-costs-report","text":"The latter one offers graphical/tabular visualization options to get the insights on a current or a previous period bills By these forms users can view the whole system spendings, or spendings divided by the specific resources. Presented metrics (resources): costs of launching compute instances, used for tools/pipelines runs. There could be: CPU instances GPU instances costs of storing user data in storages. There could be costs of storing: in Object storages in File storages info about auxiliary costs isn't supported yet All costs are aggregated and displayed for the specific period (by default - the current calendar month). Selected specific period, for which the costs should be calculated and displayed, is called \" current \". Also, for comparison, the costs for the analogical previous period are displayed in diagrams/charts (where this data is available). The user can select the desired (current) period to view the costs incurred: from one of predefined periods. For each of them there will be a specific \"previous\" period: Month . If the month is selected as current period - the \"previous\" period will be the previous calendar month of the selected one Quarter . If the quarter is selected as current period - the \"previous\" period will be the same quarter in the previous calendar year Year . If the year is selected as current period - the \"previous\" period will be the previous calendar year custom period. That period is configured by the user manually in months and can have any duration. The \"previous\" period isn't displayed in this case By default, the \"Billing Visualization\" form looks like on the picture above. It contains: General report table with the following data: current and previous periods, for which the costs are calculated summary spendings according to selected configurations in the toolbar for the current and previous periods the difference between the costs of current period and the previous one in percentage with a certain mark to determine whether spendings grow or reduce The main summary costs diagram. On this diagram, the user can see summary spendings over the current and the previous time periods according to selected configurations in the toolbar. This data could be displayed with the accumulation (as line chart ) or as fact (as bar chart with actual spending values in each time point of the period without accumulation) if the selected \"current\" period coincides with the current corresponding calendar period - there a slice line is displayed on the current calendar date: on its intersection with the \"current\" chart line, the amount of money which was spent from the beginning of the period to the current date is displayed and on its intersection with the \"previous\" chart line - the amount of money which was spent for the same previous time period is displayed the user can hover any point of the \"current\"/\"previous\" line of the diagram - the summary spendings at the corresponding datetime appear in the tooltip the aggregation value of timeline division for selected periods less than 4 months is 1 day, in other cases - 1 month Note : costs of the current calendar day is being aggregated in the following day, so they aren't displayed in diagrams With accumulation: And actual values (without accumulation): The spendings bar chart with the Resources division. On this chart, the user can view the division of summary spendings in a selected time period over the resource groups - Storages (with division on Object and File storages) and Compute Instances (with division on CPU and GPU ). Also, for each resource, the summary spendings for the same previous period are presented as well: The spendings bar chart with the Billing centers division. On this chart, the user can view the division of summary spendings in a selected time period over the billing centers (the system displays only top N most costly billing centers). For each displayed center/group, the summary spendings for the same previous period is presented as well: The report toolbar with the following controls: the \"period selector\" to select a current time period for the report - from one of predefined periods or the manual custom period the additional calendar control - to select another \"current\" period that doesn't coincide with the current calendar period (for predefined periods) or manually select desired period duration (for custom period) the control to select the specific billing center (user group) or user. By default, all available centers/users are selected the export control to download a data of displayed report in *.csv format to the local workstation The billing report form described above is the general. To get info/charts of summary spendings in any desired period (from available) you should select the corresponding period via the Period selector and Calendar control (if necessary). To get info/charts of summary spendings in the selected period only for the specific resource group you can click that resource in the Resources chart or use the corresponding item of the menu in the left side of the page (for more details see sections below ).","title":"General costs report"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#user-report","text":"To get info/charts of summary spendings in the selected period only for the specific user you can select the desired user from the dropdown list in the main toolbar. In this case: the costs data will be calculated and displayed only for the selected user (in the general report table, in the main diagram and in the resources chart) the spendings bar chart with the Billing centers division will disappear, e.g.:","title":"User report"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#billing-centergroup-report","text":"To get info/charts of summary spendings in the selected period only for the specific Billing center you can select the desired one from the dropdown list in the main toolbar or by click the corresponding Billing Center's bar in the Billing centers division chart. In this case: the costs data will be calculated and displayed only for the selected Billing center (summary for all its users - in the general report table, in the main diagram and in the resources chart) the bar chart with the top N of users' spendings in the selected period (among all users in that Billing center) will appear the table with short info about summary spendings of each user of that Billing center in the selected period (also info contains summary duration and count of the runs launched by the user, used storages volume) will appear, e.g.:","title":"Billing center/group report"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#resources-reports","text":"If you wish - you may get info/charts of summary spendings not for all resources but for the specific resource group. Also, for these charts you may select the desired time period and specific user/Billing center (group) to view costs in the way analogical as described above.","title":"Resources reports"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#storages-report","text":"To get info/charts of summary spendings in the selected period only for the Storages resource group (costs of the storing data in storages) you can click the corresponding item of the menu in the left side of the page: In this case, the summary costs for all used storages during the selected period by selected users/Billing center will be calculated and displayed (both types - Object/File storages). In the appeared page, you can see: General report table with summary costs for all used storages during the selected and previous ( if it's available ) periods The summary Storages spendings diagram over the current and the previous time periods according to selected configurations in the toolbar. This data could be displayed with the accumulation (as line chart ) or as fact (as bar chart with actual spending values in each time point of the period without accumulation) The spendings bar chart with top N most costly storages used during the selected period compared with the previous one ( if it's available ) The detailed spendings table under the chart with the full list of storages used during the selected period by selected user/Billing center Example: By default, in this form there isn't the division by storage type (Object/File). All data is calculated and displayed summary for both types. If you want to view costs only for storages with the specific type - you can select the corresponding one in the menu at the left side of the page. E.g., for Object storages:","title":"Storages report"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#compute-instances-report","text":"To get info/charts of summary spendings in the selected period only for the Compute instances resource group (costs of the launching instances for running tools/pipelines) you can click the corresponding item of the menu in the left side of the page: In this case, the summary costs for all launched instances during the selected period by selected users/Billing center will be calculated and displayed (launched tools/pipelines and both instance types - CPU/GPU). In the appeared page, you can see: General report table with summary costs for all runs launched in the selected and previous ( if it's available ) periods The summary Compute instances spendings diagram over the current and the previous time periods according to selected configurations in the toolbar. This data could be displayed with the accumulation (as line chart ) or as fact (as bar chart with actual spending values in each time point of the period without accumulation) The spendings bar chart with top N most costly instance types launched in the selected period compared to the previous one ( if it's available ) The detailed spendings table under that chart with the full list of launched instance types in the selected period The spendings bar chart with top N most costly tools launched in the selected period compared to the previous one ( if it's available ) The detailed spendings table under that chart with the full list of launched tools in the selected period The spendings bar chart with top N most costly pipelines launched in the selected period compared to the previous one ( if it's available ) The detailed spendings table under that chart with the full list of launched pipelines in the selected period Additional control that allows to change displaying of spendings bar charts with the following possible values: cost (default) - on 3 above described bar charts top N most costly objects (instance types, tools, pipelines) are displayed. Data Unit - currency usage - on 3 above described bar charts top N most involved objects (instance types, tools, pipelines) are displayed. Data Unit - usage hours runs - on 3 above described bar charts top N objects (instance types, tools, pipelines) with the largest runs count are displayed. Data Unit - runs count Example: By default, in this form there isn't the division by workload type (CPU/GPU). All data is calculated and displayed summary for both types. If you want to view costs only for instances with the specific workload type - you can select the corresponding one in the menu at the left side of the page. E.g., for CPU:","title":"Compute instances report"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#report-aggregation-level-according-to-the-permissions","text":"Available reports are calculated and displayed according to the user permissions: general user can view: general report with only own costs (User report, without Billing center report) resources report with only own costs Billing center (group) leader can view: general report with costs of that Billing center Billing center report user report for any user of that Billing center resources report with costs of that Billing center on the whole and for any user of that Billing center separately platform admin can view all available reports","title":"Report aggregation level according to the permissions"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/","text":"Pipeline objects concept In general, pipelines represent a sequence of tasks that are executed along with each other in order to process some data. They help to automate complex tasks that consist of many sub-tasks. Pipelines allow users to solve a wide range of analytical tasks. The Pipeline object in the Cloud Pipeline environment represents a workflow script with versioned files of source code, documentation, and configuration: Source code represents the code that directly shall be performed to solve tasks according to the specific pipeline needs Configuration represents the special config that contains all settings for the environment in which the pipeline shall be performed Documentation represents the set of documents that describe purposes, main solving tasks, features of the specific pipeline, etc. This is an optional part of the Pipeline and could be omitted Under the hood, each Pipeline is a GitLab repository. So, each of the constituent Pipeline parts represents one or more versioned files in that repository. The data processing via pipeline runs, in a nutshell, can be described in these several steps: The user prepares a specific calculation script that shall be registered in the Cloud Pipeline environment as the source code file of the pipeline. The user defines the environment for the pipeline execution - first of all, it is a specific package of software that is defined by a docker image. Also, the user defines the characteristics of the Cloud compute instance on which the pipeline will be run. All described definitions shall be specified in the pipeline configuration file. To store pipeline's inputs and outputs datasets the Cloud data storages shall be used. The user defines paths to these datasets as different parameters of the pipeline. Pipeline parameters shall be specified in the pipeline's configuration file. The user launches the pipeline execution in the Cloud Pipeline environment. This includes the following steps: the Cloud compute node (according to the specified characteristics in the pipeline configuration file) is being initialized on the initialized node a docker container (from the docker image specified in the pipeline's configuration file) is being launched. The subsequent execution is being continued in the docker container when the environment is set, the Git repository (corresponding to the pipeline) is being cloned to the node disk pipeline calculation script (from the cloned source code files) launches pipeline's input datasets are used for calculations (from the data storages according to the specified paths in the pipeline configuration file) output datasets are uploaded after calculations (into the data storages according to specified paths in the pipeline configuration file) After the completion of the calculation script execution, the pipeline run is being completed. The Cloud compute instance enters the pending state (no billing). The user can monitor outputs in the data storages according to specified paths in the pipeline configuration file before the pipeline run Pipeline components For more details about Pipeline components representation see here . Source code By default, when a new Pipeline object is being created from any available template - the Cloud Pipeline creates a default code file on the template's programming language. The user can manually edit this code file, add new ones or upload any count of existing code files from the local workstation. By default, all source code files are written into the /src directory of the repository. During the Pipeline execution, its corresponding repository is cloned to the compute node - so, all source code scripts will be downloaded to the node disk (more details about pipeline execution see above). Also, the user shall specify the order of scripts execution. The execution of the main (first) running script shall be specified in the Pipeline config. The execution order of other scripts should be specified in the main script or in any other, which execution is specified in the main one, otherwise, unspecified code files will be ignored. Configuration By default, when a new Pipeline object is being created from any available template - the Cloud Pipeline creates a default configuration json file. It contains the list of all general execution settings that are necessary for the pipeline run: the characteristics of the Cloud compute instance that would be initialized: type of instance (count of CPU, GPU, RAM) in terms of the Cloud Provider size of the instance disk size based docker image from which the docker container would be launched at the initialized Cloud instance the command template which would be performed at the launched docker container - here the command of the main (first) script execution shall be necessarily specified the name of the main script from source code the list of the pipeline parameters - each parameter defines the parameter for the calculation pipeline script (it could be a simple string/boolean parameter or the path to the input/output dataset) and others The user can manually change these settings via the GUI elements or via the editing of the config.json file. Documentation By default, when a new Pipeline object is being created from any available template - the Cloud Pipeline creates a default README file. The user can edit/remove this file or upload any count of existing documents from the local workstation. By default, all documents are written into the /docs directory of the pipeline repository. Schematic composition of the Pipeline object: If the user makes any changes in the Pipeline object and saves them - it causes the new commit into the corresponding Git repository of that pipeline. All commits perform automatically by the Cloud Pipeline, the user only should specify commit message. Main working scenarios Below the main scenarios of working with the Pipeline object in the Cloud Pipeline environment are described. Creation of a new Pipeline object For more details how to create and configure a Pipeline object see here . New pipeline When the user creates a new pipeline in the Cloud Pipeline environment - it automatically leads to the creation of the GitLab repository with the same name as the pipeline. On the Pipeline page, users can see general info about the repository that corresponds to the specific pipeline: the last commit checksum hash and commit message (on the picture below, it is pipeline initial commit and its automatically naming message) the info about the last repository update and the user name which did that update Just created pipeline contains: the template of the main README file the template of the main file with the calculation script the config.json file with execution environments for the pipeline run Edit and configure a pipeline After the pipeline creation, the user should configure it for his needs. Users can change the following: Source code the user can manually edit the initial template code file, add new empty files and create scripts manually or upload any count of existing code files from his local workstation Configuration the user can manually edit the full set of execution settings via the config.json file or via the GUI elements. Both variants are identical and supported by the Cloud Pipeline general settings that the user should pay attention to: the characteristics of the Cloud compute instance that would be initialized for the pipeline run: type of instance (count of CPU, GPU, RAM) in terms of the Cloud Provider size of the instance disk size the cluster configuration - if the user wants to launch pipeline tasks on several nodes he can configure the cluster based docker image from which the docker container would be launched at the initialized Cloud instance. Each docker image contains a specific software package. The user can leverage one of the existing and available docker images or also can create his own docker image with the specific software using the Cloud Pipeline features (see below), and then use it for launch pipeline command template which would be performed at the launched docker container during the pipeline run - here the command of the main (first) script execution shall be necessarily specified. If in the command template no execution is specified - no scripts from the source code will be performed if the user wants to input/output any data to/from the pipeline script during its execution - he shall configure parameters . Parameters allow varying of the set of initial data that is necessary for the pipeline run: each parameter has a name (required), a type (required) and a value (it could be optionally predefined or specified before the pipeline run) each parameter can be one of the several types (\"string\", \"boolean\", \"input path\", \"output path\" and others) the \" string \" parameter allows to transfer into the pipeline execution some string value the \" boolean \" parameter allows to transfer into the pipeline execution the boolean value the \" input path \" parameter allows to specify the path to the input dataset. The path specified as \"input path\" will be automatically mounted to the running docker container. The path where the file or directory will be mounted in the container is defined by the system Cloud Pipeline parameter - $INPUT_DIR . It has default value, but also could be redefined by the user the \" output path \" parameter allows to specify the path for data output. After the finish of the pipeline execution, all data from working directory of the docker container will be automatically uploaded to the output path. The path to the working directory in the container is defined by the system Cloud Pipeline parameter - $ANALYSIS_DIR . It has default value, but also could be redefined by the user the user can add several parameters of each type so, the user can add some necessary parameters to the pipeline, use them in the pipeline code and define parameter values directly before a specific pipeline launch Documentation the user can manually edit/delete the initial template README file or upload any count of existing documents from his local workstation Any saved changes in the Pipeline object automatically lead to the new commit into the corresponding Git repository. Short info about the last commit is always available to the user. In some cases, if users want - they can work directly with Git. Note that the repository name is the same as a pipeline name: Create a specific docker image For some specific tasks, it might be convenient to create a new docker image that will be used for the Pipeline execution. Such docker image can contain special packages that are necessary for the specific Pipeline. A new image can be based on the existing one and simply created in the Cloud Pipeline environment. For doing that: user shall launch one of the existing Docker images in the Cloud Pipeline environment (named \" Tools \"), allow just to initialize cloud compute instance and launch a Docker container on it. After that, the user can connect into the launched container via the SSH session ( SSH Web GUI terminal ), then install the necessary software and then commit the resulting container as a new Docker image or a new version of the existing one - by the Cloud Pipeline capabilities. See more details here . or user can create a Docker image without the Cloud Pipeline and then push the prepared image into the Cloud Pipeline registry. See more details here . After that, the user can leverage such image as the based docker image for his own Pipeline object. Create a multi-configuration For some reasons, the user may need to launch the Pipeline on different compute instances or, for example, with different sets of parameters, etc. For such cases, the Cloud Pipeline allows to create several configurations for one Pipeline object that all are saved in one config.json file but with the ability to specify which one directly should be used before the pipeline run. Testing an existing Pipeline object For more details how to launch a Pipeline object see here . Launch the Pipeline After the pipeline is created, configured and saved - it can be launched. The Cloud Pipeline allows to launch the pipeline with the saved execution settings and parameters (if they have default values) or manually configure execution settings and parameters before a specific run (such changes will not be saved in the pipeline configuration and will use only for one pipeline run). After the pipeline was launched all described procedures are performed - the cloud instance is initialized, the docker container is launched, input paths are mounted (if they were specified before the run via the parameters) - then the main script will start execution. As the main pipeline script is prepared manually by the user, it can be incorrect during the first runnings and contain errors. To easily debug the script the user should not launch the created pipeline and wait for the full execution, but connect to the compute node via the SSH and work with the script execution directly in the terminal. For that, the user should set the command template , e.g. sleep infinity , to start the Pipeline in \"idle\" mode (not launching any script). It allows just to initialize cloud instance, launch a Docker container on it, clone the pipeline repository and perform other initialize tasks. After that, the user can connect into the launched container via the SSH session ( SSH Web GUI terminal ), launch and debug the script manually (the script is available from the cloned repository): All tasks performed during the run and logs are available to the user, e.g.: So, the user can observe for the pipeline execution in real-time. Review the results After completing all tasks the pipeline run ends. Output data is downloaded into the output path (if it was specified before the run via the parameters). Docker container stops. Run logs page is still available, the pipeline run gets the \"Completed\" state. The user can view output data on the output path after the pipeline execution. These files can be downloaded to the local workstation or, for example, used in further calculations. Also, each output file is marked by special tags that contain general info about the pipeline run, in which the file was received: user name who launched the run run ID in the Cloud Pipeline environment link to the Pipeline object link to the Docker image used in the run main characteristics of the compute cloud instance used in the run and others E.g.: For convenience, the user can view all run history for the Pipeline object with short information about the run. Each record is the hyperlink to the specific run logs page (see above ), where user can review full information about that pipeline run's settings: Deployment of an existing Pipeline object Release the Pipeline After the pipeline was created, configured, saved and tested - the user can release it. In the Cloud Pipeline Web GUI this action looks like creating the named version of the Pipeline object. User can't change such named version, only create a new \" draft \" version over it and then work with it. Then, this \" draft \" version also can be released as another \" stable \" version, etc. Under the hood, version release is the tagging in the corresponding Git repository. When the user releases some version - he just creates the annotated Git tag. Share to other users The Cloud Pipeline has a useful RBAC model. The user can share his prepared pipeline with other users or user groups for further usage/common work. There are 3 permission settings over the Pipeline objects in the Cloud Pipeline: Read - allows to view a pipeline, its code, configuration, settings, history of runs Write - allows to edit/delete a pipeline, its code, configuration, settings Execute - allows to run a pipeline So, the user-owner of the pipeline (or system admin) can set combinations of described permissions to other users/user groups to rule their access to the Pipeline , e.g.: See a simple example of the complete Pipeline running procedure here .","title":"Appendix E. Pipeline objects concept"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/#pipeline-objects-concept","text":"In general, pipelines represent a sequence of tasks that are executed along with each other in order to process some data. They help to automate complex tasks that consist of many sub-tasks. Pipelines allow users to solve a wide range of analytical tasks. The Pipeline object in the Cloud Pipeline environment represents a workflow script with versioned files of source code, documentation, and configuration: Source code represents the code that directly shall be performed to solve tasks according to the specific pipeline needs Configuration represents the special config that contains all settings for the environment in which the pipeline shall be performed Documentation represents the set of documents that describe purposes, main solving tasks, features of the specific pipeline, etc. This is an optional part of the Pipeline and could be omitted Under the hood, each Pipeline is a GitLab repository. So, each of the constituent Pipeline parts represents one or more versioned files in that repository. The data processing via pipeline runs, in a nutshell, can be described in these several steps: The user prepares a specific calculation script that shall be registered in the Cloud Pipeline environment as the source code file of the pipeline. The user defines the environment for the pipeline execution - first of all, it is a specific package of software that is defined by a docker image. Also, the user defines the characteristics of the Cloud compute instance on which the pipeline will be run. All described definitions shall be specified in the pipeline configuration file. To store pipeline's inputs and outputs datasets the Cloud data storages shall be used. The user defines paths to these datasets as different parameters of the pipeline. Pipeline parameters shall be specified in the pipeline's configuration file. The user launches the pipeline execution in the Cloud Pipeline environment. This includes the following steps: the Cloud compute node (according to the specified characteristics in the pipeline configuration file) is being initialized on the initialized node a docker container (from the docker image specified in the pipeline's configuration file) is being launched. The subsequent execution is being continued in the docker container when the environment is set, the Git repository (corresponding to the pipeline) is being cloned to the node disk pipeline calculation script (from the cloned source code files) launches pipeline's input datasets are used for calculations (from the data storages according to the specified paths in the pipeline configuration file) output datasets are uploaded after calculations (into the data storages according to specified paths in the pipeline configuration file) After the completion of the calculation script execution, the pipeline run is being completed. The Cloud compute instance enters the pending state (no billing). The user can monitor outputs in the data storages according to specified paths in the pipeline configuration file before the pipeline run","title":"Pipeline objects concept"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/#pipeline-components","text":"For more details about Pipeline components representation see here .","title":"Pipeline components"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/#source-code","text":"By default, when a new Pipeline object is being created from any available template - the Cloud Pipeline creates a default code file on the template's programming language. The user can manually edit this code file, add new ones or upload any count of existing code files from the local workstation. By default, all source code files are written into the /src directory of the repository. During the Pipeline execution, its corresponding repository is cloned to the compute node - so, all source code scripts will be downloaded to the node disk (more details about pipeline execution see above). Also, the user shall specify the order of scripts execution. The execution of the main (first) running script shall be specified in the Pipeline config. The execution order of other scripts should be specified in the main script or in any other, which execution is specified in the main one, otherwise, unspecified code files will be ignored.","title":"Source code"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/#configuration","text":"By default, when a new Pipeline object is being created from any available template - the Cloud Pipeline creates a default configuration json file. It contains the list of all general execution settings that are necessary for the pipeline run: the characteristics of the Cloud compute instance that would be initialized: type of instance (count of CPU, GPU, RAM) in terms of the Cloud Provider size of the instance disk size based docker image from which the docker container would be launched at the initialized Cloud instance the command template which would be performed at the launched docker container - here the command of the main (first) script execution shall be necessarily specified the name of the main script from source code the list of the pipeline parameters - each parameter defines the parameter for the calculation pipeline script (it could be a simple string/boolean parameter or the path to the input/output dataset) and others The user can manually change these settings via the GUI elements or via the editing of the config.json file.","title":"Configuration"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/#documentation","text":"By default, when a new Pipeline object is being created from any available template - the Cloud Pipeline creates a default README file. The user can edit/remove this file or upload any count of existing documents from the local workstation. By default, all documents are written into the /docs directory of the pipeline repository. Schematic composition of the Pipeline object: If the user makes any changes in the Pipeline object and saves them - it causes the new commit into the corresponding Git repository of that pipeline. All commits perform automatically by the Cloud Pipeline, the user only should specify commit message.","title":"Documentation"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/#main-working-scenarios","text":"Below the main scenarios of working with the Pipeline object in the Cloud Pipeline environment are described.","title":"Main working scenarios"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/#creation-of-a-new-pipeline-object","text":"For more details how to create and configure a Pipeline object see here .","title":"Creation of a new Pipeline object"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/#new-pipeline","text":"When the user creates a new pipeline in the Cloud Pipeline environment - it automatically leads to the creation of the GitLab repository with the same name as the pipeline. On the Pipeline page, users can see general info about the repository that corresponds to the specific pipeline: the last commit checksum hash and commit message (on the picture below, it is pipeline initial commit and its automatically naming message) the info about the last repository update and the user name which did that update Just created pipeline contains: the template of the main README file the template of the main file with the calculation script the config.json file with execution environments for the pipeline run","title":"New pipeline"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/#edit-and-configure-a-pipeline","text":"After the pipeline creation, the user should configure it for his needs. Users can change the following: Source code the user can manually edit the initial template code file, add new empty files and create scripts manually or upload any count of existing code files from his local workstation Configuration the user can manually edit the full set of execution settings via the config.json file or via the GUI elements. Both variants are identical and supported by the Cloud Pipeline general settings that the user should pay attention to: the characteristics of the Cloud compute instance that would be initialized for the pipeline run: type of instance (count of CPU, GPU, RAM) in terms of the Cloud Provider size of the instance disk size the cluster configuration - if the user wants to launch pipeline tasks on several nodes he can configure the cluster based docker image from which the docker container would be launched at the initialized Cloud instance. Each docker image contains a specific software package. The user can leverage one of the existing and available docker images or also can create his own docker image with the specific software using the Cloud Pipeline features (see below), and then use it for launch pipeline command template which would be performed at the launched docker container during the pipeline run - here the command of the main (first) script execution shall be necessarily specified. If in the command template no execution is specified - no scripts from the source code will be performed if the user wants to input/output any data to/from the pipeline script during its execution - he shall configure parameters . Parameters allow varying of the set of initial data that is necessary for the pipeline run: each parameter has a name (required), a type (required) and a value (it could be optionally predefined or specified before the pipeline run) each parameter can be one of the several types (\"string\", \"boolean\", \"input path\", \"output path\" and others) the \" string \" parameter allows to transfer into the pipeline execution some string value the \" boolean \" parameter allows to transfer into the pipeline execution the boolean value the \" input path \" parameter allows to specify the path to the input dataset. The path specified as \"input path\" will be automatically mounted to the running docker container. The path where the file or directory will be mounted in the container is defined by the system Cloud Pipeline parameter - $INPUT_DIR . It has default value, but also could be redefined by the user the \" output path \" parameter allows to specify the path for data output. After the finish of the pipeline execution, all data from working directory of the docker container will be automatically uploaded to the output path. The path to the working directory in the container is defined by the system Cloud Pipeline parameter - $ANALYSIS_DIR . It has default value, but also could be redefined by the user the user can add several parameters of each type so, the user can add some necessary parameters to the pipeline, use them in the pipeline code and define parameter values directly before a specific pipeline launch Documentation the user can manually edit/delete the initial template README file or upload any count of existing documents from his local workstation Any saved changes in the Pipeline object automatically lead to the new commit into the corresponding Git repository. Short info about the last commit is always available to the user. In some cases, if users want - they can work directly with Git. Note that the repository name is the same as a pipeline name:","title":"Edit and configure a pipeline"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/#create-a-specific-docker-image","text":"For some specific tasks, it might be convenient to create a new docker image that will be used for the Pipeline execution. Such docker image can contain special packages that are necessary for the specific Pipeline. A new image can be based on the existing one and simply created in the Cloud Pipeline environment. For doing that: user shall launch one of the existing Docker images in the Cloud Pipeline environment (named \" Tools \"), allow just to initialize cloud compute instance and launch a Docker container on it. After that, the user can connect into the launched container via the SSH session ( SSH Web GUI terminal ), then install the necessary software and then commit the resulting container as a new Docker image or a new version of the existing one - by the Cloud Pipeline capabilities. See more details here . or user can create a Docker image without the Cloud Pipeline and then push the prepared image into the Cloud Pipeline registry. See more details here . After that, the user can leverage such image as the based docker image for his own Pipeline object.","title":"Create a specific docker image"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/#create-a-multi-configuration","text":"For some reasons, the user may need to launch the Pipeline on different compute instances or, for example, with different sets of parameters, etc. For such cases, the Cloud Pipeline allows to create several configurations for one Pipeline object that all are saved in one config.json file but with the ability to specify which one directly should be used before the pipeline run.","title":"Create a multi-configuration"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/#testing-an-existing-pipeline-object","text":"For more details how to launch a Pipeline object see here .","title":"Testing an existing Pipeline object"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/#launch-the-pipeline","text":"After the pipeline is created, configured and saved - it can be launched. The Cloud Pipeline allows to launch the pipeline with the saved execution settings and parameters (if they have default values) or manually configure execution settings and parameters before a specific run (such changes will not be saved in the pipeline configuration and will use only for one pipeline run). After the pipeline was launched all described procedures are performed - the cloud instance is initialized, the docker container is launched, input paths are mounted (if they were specified before the run via the parameters) - then the main script will start execution. As the main pipeline script is prepared manually by the user, it can be incorrect during the first runnings and contain errors. To easily debug the script the user should not launch the created pipeline and wait for the full execution, but connect to the compute node via the SSH and work with the script execution directly in the terminal. For that, the user should set the command template , e.g. sleep infinity , to start the Pipeline in \"idle\" mode (not launching any script). It allows just to initialize cloud instance, launch a Docker container on it, clone the pipeline repository and perform other initialize tasks. After that, the user can connect into the launched container via the SSH session ( SSH Web GUI terminal ), launch and debug the script manually (the script is available from the cloned repository): All tasks performed during the run and logs are available to the user, e.g.: So, the user can observe for the pipeline execution in real-time.","title":"Launch the Pipeline"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/#review-the-results","text":"After completing all tasks the pipeline run ends. Output data is downloaded into the output path (if it was specified before the run via the parameters). Docker container stops. Run logs page is still available, the pipeline run gets the \"Completed\" state. The user can view output data on the output path after the pipeline execution. These files can be downloaded to the local workstation or, for example, used in further calculations. Also, each output file is marked by special tags that contain general info about the pipeline run, in which the file was received: user name who launched the run run ID in the Cloud Pipeline environment link to the Pipeline object link to the Docker image used in the run main characteristics of the compute cloud instance used in the run and others E.g.: For convenience, the user can view all run history for the Pipeline object with short information about the run. Each record is the hyperlink to the specific run logs page (see above ), where user can review full information about that pipeline run's settings:","title":"Review the results"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/#deployment-of-an-existing-pipeline-object","text":"","title":"Deployment of an existing Pipeline object"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/#release-the-pipeline","text":"After the pipeline was created, configured, saved and tested - the user can release it. In the Cloud Pipeline Web GUI this action looks like creating the named version of the Pipeline object. User can't change such named version, only create a new \" draft \" version over it and then work with it. Then, this \" draft \" version also can be released as another \" stable \" version, etc. Under the hood, version release is the tagging in the corresponding Git repository. When the user releases some version - he just creates the annotated Git tag.","title":"Release the Pipeline"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/#share-to-other-users","text":"The Cloud Pipeline has a useful RBAC model. The user can share his prepared pipeline with other users or user groups for further usage/common work. There are 3 permission settings over the Pipeline objects in the Cloud Pipeline: Read - allows to view a pipeline, its code, configuration, settings, history of runs Write - allows to edit/delete a pipeline, its code, configuration, settings Execute - allows to run a pipeline So, the user-owner of the pipeline (or system admin) can set combinations of described permissions to other users/user groups to rule their access to the Pipeline , e.g.: See a simple example of the complete Pipeline running procedure here .","title":"Share to other users"},{"location":"manual/Appendix_F/Appendix_F._\u0421omparison_of_using_different_FS_storages_(FSx_for_Lustre_vs_EFS_in_AWS)/","text":"\u0421omparison of using different FS storage types in Cloud Pipeline environment Performance Synthetic data experiment Real data experiment Costs Performance comparison The performance was measured for different AWS file systems are using in Cloud Pipeline: filesystems are managed by S3 EFS in AWS FSx for Lustre local filesystems BTRFS on EBS LizardFS on EBS A performance comparison of Cloud Pipeline storages was conducted against synthetic and real data. All experiments were carried out on c5.2xlarge ( 8 CPU, 16 RAM) AWS instance. Synthetic data experiment In this experiment we have generated two types of data: 100Gb large file 100 000 small files by 500Kb With this data, we have measured the creation and read times using the time command of Unix and Unix-like systems. The command that was used to create a 100Gb large file by 100Mb chunks: dd if=/dev/urandom of=/path-to-storage/large_100gb.txt iflag=fullblock bs=100M count=1024 The command that was used to create a 100 000 small files by 500Kb: for j in {1..100000}; do head -c 500kB /dev/urandom /path-to-storage/small/randfile$j.txt done The command that was used to read a large file: dd if=/path-to-storage/large_100gb.txt of=/dev/null conv=fdatasync The command that was used to read small files: for file in /path-to-storage/small/* ; do dd if=$file of=/dev/null done The synthetic data experiment was initially carried out in one thread and then 4 and 8 threads. The experimental results on synthetic data are presented in the following tables: 1 thread Storage Create the large file Read the large file Create many small files Read many small files EFS real 17m24.812s user 0m0.004s sys 10m34.675s real 17m18.216s user 0m48.904s sys 3m9.517s real 53m3.498s user 0m58.209s sys 5m46.386s real 27m11.831s user 2m14.295s sys 1m35.923s LUSTRE real 10m55.494s user 0m0.004s sys 9m47.326s real 13m28.536s user 0m51.675s sys 12m36.813s real 12m37.380s user 1m54.744s sys 5m13.759s real 20m40.877s user 0m44.095s sys 9m4.877s BTRFS on EBS real 11m9.866s user 0m0.032s sys 11m7.813s real 7m0.032s user 0m53.303s sys 3m47.549s real 6m48.540s user 0m47.254s sys 6m6.036s real 6m26.190s user 2m1.610s sys 1m25.726s LizardFS on EBS real 13m4.352s user 0m0.008s sys 10m57.101s real 7m54.142s user 0m53.089s sys 3m51.089s real 16m39.980s user 2m11.618s sys 6m35.383s real 10m3.791s user 2m18.924s sys 1m28.035s 4 threads Storage Create the large file Read the large file Create many small files Read many small files EFS real 69m25.583s user 0m0.015s sys 11m30.451s real 64m20.614s user 0m48.233s sys 3m15.074s real 59m18.137s user 0m37.185s sys 8m29.882s real 33m19.459s user 2m23.134s sys 2m18.345s LUSTRE real 38m32.383s user 0m0.014s sys 36m40.821s real 20m45.156s user 0m59.189s sys 19m26.054s real 25m38.531s user 0m21.820s sys 16m59.318s real 24m58.620s user 2m11.449s sys 11m57.240s BTRFS on EBS real 38m50.438s user 0m0.028s sys 38m45.451s real 27m55.173s user 0m52.903s sys 4m26.061s real 20m54.831s user 0m20.394s sys 20m34.926s real 12m50.153s user 2m5.149s sys 1m18.555s LizardFS on EBS real 48m47.367s user 0m0.020s sys 40m17.341s real 32m21.257s user 0m57.588s sys 5m32.215s real 28m12.707s user 1m30.504s sys 12m40.881s real 15m31.591s user 2m21.772s sys 2m31.211s 8 threads Storage Create the large file Read the large file Create many small files Read many small files EFS real 127m49.718s user 0m0.010s sys 14m44.358s real 122m46.188s user 1m12.786s sys 21m14.236s real 72m43.596s user 0m26.727s sys 15m0.582s real 62m46.118s user 2m31.595s sys 2m31.577s LUSTRE real 93m56.846s user 0m0.018s sys 90m30.5s real 94m1.258s user 0m48.908s sys 3m18.557s real 50m42.845s user 0m23.462s sys 35m12.511s real 30m53.199s user 0m54.020s sys 14m42.712s BTRFS on EBS real 87m48.066s user 0m0.016s sys 86m15.610s real 39m59.167s user 0m50.900s sys 4m25.582s real 44m36.847s user 0m24.491s sys 43m11.719s real 17m12.744s user 2m14.324s sys 1m50.667s LizardFS on EBS real 97m25.045s user 0m0.007s sys 73m32.042s real 39m59.167s user 0m50.900s sys 4m25.582s real 50m25.868s user 0m58.079s sys 19m12.443s real 27m50.506s user 2m26.704s sys 3m9.926s As we can see from the presented results, Amazon FSx for Lustre is several times faster (from 1.3 to 3.2 times for read mode and from 1.3 to 4 times for write mode) than Amazon EFS . As expected, the local systems performed better than FSx for Lustre or EFS generally. But it should be noted, that the FSx for Lustre was comparable to local systems in some cases. Real data experiment The cellranger count pipeline was used to conduct an experiment with real data. The input data: 15Gb transcriptome reference 50Gb fastqs The command that was used to run cellranger count pipeline: /path-to-cellranger/3.0.2/bin/cellranger count --localcores=8 --id={id} --transcriptome=/path-to-transcriptome-reference/refdata-cellranger-mm10-3.0.0 --chemistry=SC5P-R2 --fastqs=/path-to-fastqs/fastqs_test --sample={sample_name} The experimental run result is presented in the following table: Storage Execution time EFS real 207m15.566s user 413m32.168s sys 13m40.581s LUSTRE real 189m23.586s user 434m6.950s sys 13m2.902s BTRFS on EBS real 187m23.048s user 413m32.666s sys 12m30.285s LizardFS on EBS real 189m8.210s user 412m6.558s sys 14m18.429s The best result was shown by the BTRFS on EBS local system. However, it can be said that BTRFS on EBS , LizardFS on EBS , and FSx for Lustre are showed the comparable time. Amazon FSx for Lustre was faster Amazon EFS just to ~ 9%. Costs Cost calculations have been performed in according to Amazon pricing at the time of this document. For the experiments were used the storages that were created in the US East (N.Virginia) region with similar features: Storage Storage size Throughput mode EFS Size in EFS Standard: 1 TiB (100%) Bursting: 50 MB/s/TiB LUSTRE SSD: 1.2 TiB Capacity 50 MB/s/TiB baseline, up to 1.3 GB/s/TiB burst BTRFS on EBS SSD: 1.2 TiB Capacity Max Throughput/Instance - 4,750 MB/s LizardFS on EBS SSD: 1.2 TiB Capacity Max Throughput/Instance - 4,750 MB/s The total charge for the month of usage for various storages is calculated in different ways: EFS 1 TB per month x 1024 GB in a TB = 1024 GB per month (data stored in Standard Storage ) 1 024 GB per month x $0,30 GB-month = $307,20 ( Standard Storage monthly cost) FSx for Lustre $0.14 GB-month / 30 / 24 = $0.000194 GB-hour 1228 GB x $0.000194 GB-hour x 720 hours = $171,5 ( FSx for Lustre monthly cost) BTRFS on EBS and LizardFS on EBS (1228 GB x $0.10 GB-month * 86400 seconds (for 24 hours)) / (86,400 seconds/day * 30 day-month) = $4 (for the volume) As seen from the calculation, the most beneficial is to use BTRFS on EBS and LizardFS on EBS local systems. However, this is not suitable for long term storage. In this case, EFS monthly cost is more expensive than Amazon FSx for Lustre monthly cost in 1.8 times for similar features storage at the time of this document.","title":"Appendix F. \u0421omparison of using different FS storage types"},{"location":"manual/Appendix_F/Appendix_F._\u0421omparison_of_using_different_FS_storages_(FSx_for_Lustre_vs_EFS_in_AWS)/#omparison-of-using-different-fs-storage-types-in-cloud-pipeline-environment","text":"Performance Synthetic data experiment Real data experiment Costs","title":"\u0421omparison of using different FS storage types in Cloud Pipeline environment"},{"location":"manual/Appendix_F/Appendix_F._\u0421omparison_of_using_different_FS_storages_(FSx_for_Lustre_vs_EFS_in_AWS)/#performance-comparison","text":"The performance was measured for different AWS file systems are using in Cloud Pipeline: filesystems are managed by S3 EFS in AWS FSx for Lustre local filesystems BTRFS on EBS LizardFS on EBS A performance comparison of Cloud Pipeline storages was conducted against synthetic and real data. All experiments were carried out on c5.2xlarge ( 8 CPU, 16 RAM) AWS instance.","title":"Performance comparison"},{"location":"manual/Appendix_F/Appendix_F._\u0421omparison_of_using_different_FS_storages_(FSx_for_Lustre_vs_EFS_in_AWS)/#synthetic-data-experiment","text":"In this experiment we have generated two types of data: 100Gb large file 100 000 small files by 500Kb With this data, we have measured the creation and read times using the time command of Unix and Unix-like systems. The command that was used to create a 100Gb large file by 100Mb chunks: dd if=/dev/urandom of=/path-to-storage/large_100gb.txt iflag=fullblock bs=100M count=1024 The command that was used to create a 100 000 small files by 500Kb: for j in {1..100000}; do head -c 500kB /dev/urandom /path-to-storage/small/randfile$j.txt done The command that was used to read a large file: dd if=/path-to-storage/large_100gb.txt of=/dev/null conv=fdatasync The command that was used to read small files: for file in /path-to-storage/small/* ; do dd if=$file of=/dev/null done The synthetic data experiment was initially carried out in one thread and then 4 and 8 threads. The experimental results on synthetic data are presented in the following tables:","title":"Synthetic data experiment"},{"location":"manual/Appendix_F/Appendix_F._\u0421omparison_of_using_different_FS_storages_(FSx_for_Lustre_vs_EFS_in_AWS)/#1-thread","text":"Storage Create the large file Read the large file Create many small files Read many small files EFS real 17m24.812s user 0m0.004s sys 10m34.675s real 17m18.216s user 0m48.904s sys 3m9.517s real 53m3.498s user 0m58.209s sys 5m46.386s real 27m11.831s user 2m14.295s sys 1m35.923s LUSTRE real 10m55.494s user 0m0.004s sys 9m47.326s real 13m28.536s user 0m51.675s sys 12m36.813s real 12m37.380s user 1m54.744s sys 5m13.759s real 20m40.877s user 0m44.095s sys 9m4.877s BTRFS on EBS real 11m9.866s user 0m0.032s sys 11m7.813s real 7m0.032s user 0m53.303s sys 3m47.549s real 6m48.540s user 0m47.254s sys 6m6.036s real 6m26.190s user 2m1.610s sys 1m25.726s LizardFS on EBS real 13m4.352s user 0m0.008s sys 10m57.101s real 7m54.142s user 0m53.089s sys 3m51.089s real 16m39.980s user 2m11.618s sys 6m35.383s real 10m3.791s user 2m18.924s sys 1m28.035s","title":"1 thread"},{"location":"manual/Appendix_F/Appendix_F._\u0421omparison_of_using_different_FS_storages_(FSx_for_Lustre_vs_EFS_in_AWS)/#4-threads","text":"Storage Create the large file Read the large file Create many small files Read many small files EFS real 69m25.583s user 0m0.015s sys 11m30.451s real 64m20.614s user 0m48.233s sys 3m15.074s real 59m18.137s user 0m37.185s sys 8m29.882s real 33m19.459s user 2m23.134s sys 2m18.345s LUSTRE real 38m32.383s user 0m0.014s sys 36m40.821s real 20m45.156s user 0m59.189s sys 19m26.054s real 25m38.531s user 0m21.820s sys 16m59.318s real 24m58.620s user 2m11.449s sys 11m57.240s BTRFS on EBS real 38m50.438s user 0m0.028s sys 38m45.451s real 27m55.173s user 0m52.903s sys 4m26.061s real 20m54.831s user 0m20.394s sys 20m34.926s real 12m50.153s user 2m5.149s sys 1m18.555s LizardFS on EBS real 48m47.367s user 0m0.020s sys 40m17.341s real 32m21.257s user 0m57.588s sys 5m32.215s real 28m12.707s user 1m30.504s sys 12m40.881s real 15m31.591s user 2m21.772s sys 2m31.211s","title":"4 threads"},{"location":"manual/Appendix_F/Appendix_F._\u0421omparison_of_using_different_FS_storages_(FSx_for_Lustre_vs_EFS_in_AWS)/#8-threads","text":"Storage Create the large file Read the large file Create many small files Read many small files EFS real 127m49.718s user 0m0.010s sys 14m44.358s real 122m46.188s user 1m12.786s sys 21m14.236s real 72m43.596s user 0m26.727s sys 15m0.582s real 62m46.118s user 2m31.595s sys 2m31.577s LUSTRE real 93m56.846s user 0m0.018s sys 90m30.5s real 94m1.258s user 0m48.908s sys 3m18.557s real 50m42.845s user 0m23.462s sys 35m12.511s real 30m53.199s user 0m54.020s sys 14m42.712s BTRFS on EBS real 87m48.066s user 0m0.016s sys 86m15.610s real 39m59.167s user 0m50.900s sys 4m25.582s real 44m36.847s user 0m24.491s sys 43m11.719s real 17m12.744s user 2m14.324s sys 1m50.667s LizardFS on EBS real 97m25.045s user 0m0.007s sys 73m32.042s real 39m59.167s user 0m50.900s sys 4m25.582s real 50m25.868s user 0m58.079s sys 19m12.443s real 27m50.506s user 2m26.704s sys 3m9.926s As we can see from the presented results, Amazon FSx for Lustre is several times faster (from 1.3 to 3.2 times for read mode and from 1.3 to 4 times for write mode) than Amazon EFS . As expected, the local systems performed better than FSx for Lustre or EFS generally. But it should be noted, that the FSx for Lustre was comparable to local systems in some cases.","title":"8 threads"},{"location":"manual/Appendix_F/Appendix_F._\u0421omparison_of_using_different_FS_storages_(FSx_for_Lustre_vs_EFS_in_AWS)/#real-data-experiment","text":"The cellranger count pipeline was used to conduct an experiment with real data. The input data: 15Gb transcriptome reference 50Gb fastqs The command that was used to run cellranger count pipeline: /path-to-cellranger/3.0.2/bin/cellranger count --localcores=8 --id={id} --transcriptome=/path-to-transcriptome-reference/refdata-cellranger-mm10-3.0.0 --chemistry=SC5P-R2 --fastqs=/path-to-fastqs/fastqs_test --sample={sample_name} The experimental run result is presented in the following table: Storage Execution time EFS real 207m15.566s user 413m32.168s sys 13m40.581s LUSTRE real 189m23.586s user 434m6.950s sys 13m2.902s BTRFS on EBS real 187m23.048s user 413m32.666s sys 12m30.285s LizardFS on EBS real 189m8.210s user 412m6.558s sys 14m18.429s The best result was shown by the BTRFS on EBS local system. However, it can be said that BTRFS on EBS , LizardFS on EBS , and FSx for Lustre are showed the comparable time. Amazon FSx for Lustre was faster Amazon EFS just to ~ 9%.","title":"Real data experiment"},{"location":"manual/Appendix_F/Appendix_F._\u0421omparison_of_using_different_FS_storages_(FSx_for_Lustre_vs_EFS_in_AWS)/#costs","text":"Cost calculations have been performed in according to Amazon pricing at the time of this document. For the experiments were used the storages that were created in the US East (N.Virginia) region with similar features: Storage Storage size Throughput mode EFS Size in EFS Standard: 1 TiB (100%) Bursting: 50 MB/s/TiB LUSTRE SSD: 1.2 TiB Capacity 50 MB/s/TiB baseline, up to 1.3 GB/s/TiB burst BTRFS on EBS SSD: 1.2 TiB Capacity Max Throughput/Instance - 4,750 MB/s LizardFS on EBS SSD: 1.2 TiB Capacity Max Throughput/Instance - 4,750 MB/s The total charge for the month of usage for various storages is calculated in different ways: EFS 1 TB per month x 1024 GB in a TB = 1024 GB per month (data stored in Standard Storage ) 1 024 GB per month x $0,30 GB-month = $307,20 ( Standard Storage monthly cost) FSx for Lustre $0.14 GB-month / 30 / 24 = $0.000194 GB-hour 1228 GB x $0.000194 GB-hour x 720 hours = $171,5 ( FSx for Lustre monthly cost) BTRFS on EBS and LizardFS on EBS (1228 GB x $0.10 GB-month * 86400 seconds (for 24 hours)) / (86,400 seconds/day * 30 day-month) = $4 (for the volume) As seen from the calculation, the most beneficial is to use BTRFS on EBS and LizardFS on EBS local systems. However, this is not suitable for long term storage. In this case, EFS monthly cost is more expensive than Amazon FSx for Lustre monthly cost in 1.8 times for similar features storage at the time of this document.","title":"Costs"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/","text":"Cloud Pipeline v.0.13 - Release notes Data sharing with external collaborators Batch processing in EU Running instances sharing with other user(s) or group(s) of users Automated instances pause/stop according to the resource usage Tools versions details Data download from external http/ftp resources to the cloud data storage Automatically rerun a batch job if a spot instance is terminated Displaying estimated price of a job run Displaying details of the user profile Breadcrumbs for the Library view Data sharing with external collaborators NGS users often get the raw datasets from the external partners for processing. Typically external collaborator sends such datasets using hard drives. To enable such type of collaboration - S3 buckets within a Cloud Platform can now be \"Shared\". When a bucket is created - owner can set \"Enable sharing\" option. Bucket will be created and can be managed and consumed as any other bucket: But such types of buckets also display a \"Share\" button, which can be used to generate URL, that can be shared with the external collaborator Click \"Share\" Get the URL and send it to the external partner Once external users loads this URL: Authentication is performed using SAML Access is granted according to the user's permissions S3 bucket browser is displayed This collaboration space can be used to exchange large data files (up to 5Tb per one file) Compared to the \" Data download from external http/ftp resources to the cloud data storage \" feature (see below) - this use case considers that external colleague cannot provide a URL for direct download . For more information about data sharing with external collaborators see 8.8. Data sharing . Batch processing in EU Previously all computing nodes and storages were located in the US region. For EU NGS use cases, which operate on huge data volumes, data movement to US took too much time. To workaround this issue - CP Platform was improved to support batch processing and compute nodes management within other Cloud regions. Bucket creation form now allows to set - where to create a data storage: All the buckets, that are shown in the \"Library tree view\", \"Library details form\" and \"Home dashboard\" are now tagged with region flag to visually distinguish storage locations: Library tree Library details view Home dashboard When a pipeline configuration is created within a project or a new run is launched - user can specify to use a specific region for a compute node placement: Project method configuration Launch form configuration Examples of using Cloud regions see in sections 6. Manage Pipeline , 7. Manage Detached configuration , 8. Manage Data Storage . Running instances sharing with other user(s) or group(s) of users For certain use cases it is beneficial to be able to share applications with other users/groups. Thus, providing a \"persistent\" service that can be shared and run in the Cloud platform. Current version introduces a feature that allows to: Specify a \"Friendly URL\" for persistent services. This produces endpoint URL in a more friendly/descriptive format: {cloud-pipeline_url}/ friendly_url instead of {cloud-pipeline_url}/ pipeline-XXXX-XXXX . This can be configured at a service launch time in the \"Advanced\" section of the Launch form (name shall be unique) URL will be generated using the specified name For more information about \"Friendly URL\" for persistent services see here . User can now share a run with others: \"Share with: ...\" parameter, within a run log form, can be used for this Specific users or whole groups can be set for sharing Once this is set - other users will be able to access run's endpoints (not SSH) \"Services\" widget within a Home dashboard page is now capable of listing such \"shared\" services. It is intended to display a \"catalog\" of services, that can be accessed by a current user, without running own jobs. For more information about runs sharing see 11.3. Sharing with other users or groups of users . Automated instances pause/stop according to the resource usage Version v0.12 introduced a PAUSE/RESUME option for the users, which allowed to persist whole state of the environment and resume it. This feature required AWS On-Demand instances to be used, which are more expensive compared to Spots. Current version provides a way to control spendings by automatically pausing on-demand instances if they are not used. Administrators can now control this behaviour using a set of parameters: system.idle.cpu.threshold - specify %% of the average CPU, below which action shall be taken system.resource.monitoring.period - specify period (in seconds) between the users' instances scanning to collect the monitoring metrics system.max.idle.timeout.minutes - specify a duration in minutes. If CPU utilization is below system.idle.cpu.threshold for this duration - notification will be sent to the user. system.idle.action.timeout.minutes - specify a duration in minutes. If CPU utilization is below system.idle.cpu.threshold for this duration - an action, specified in system.idle.action will be performed system.idle.action - which action to perform on the instance, that showed low CPU utilization: NOTIFY - only send notification PAUSE - pause an instance if possible (e.g. instance is On-Demand, Spot instances are skipped) PAUSE_OR_STOP - pause an instance if it is On-Demand, stop an instance if it is Spot STOP - Stop an instance, disregarding price-type Tools versions details Now more information on the docker image version is available: Image size Modified date Unique identifier (Digest) Corresponding aliases (e.g. if some digest has two aliases) For more information see here . Data download from external http/ftp resources to the cloud data storage Users often get the raw datasets from the external partners for processing. Previously, users had to get the data to the local cluster storage and them upload data to the cloud using clommand-line interface. Now users can provide CSV/TSV files with the external links and submit a data transfer job, so that the files will be moved to the cloud storage in the background. Upload CSV/TSV file and view list of samples with the external links: Select the transfer options: Which S3 bucket to use as a destination Which CSV/TSV columns shall be used to get external URLs (if several columns contain URLs - both can be used) ( optionally ) Select whether to rename resulting path to some other value (can be specified as another column cell, e.g. sample name) ( optionally ) Create new folders within destination if several columns are selected for \"Path fields\" option. E.g. if two columns contain URLs and both are selected - then folders will be created for the corresponding column name and used for appropriate files storage. Whether to update external URL within a table to the new location of the files. If set - http/ftp URLs will be changed to s3 path. Such data structure can be then used for a processing by a pipeline (See below - URLs are changed to the S3-clickable hyperlinks): Once transfer job is finished - files will be located in the selected S3 storage: For more information about data downloading from external http/ftp resources to the CP see 5.5. Download data from external resources to the cloud data storage . Automatically rerun a batch job if a spot instance is terminated In certain cases - AWS may terminate a node, that is used to run a job or an interactive tool: Spot prices changed AWS experienced a hardware issue These cases shall not be treated as a Cloud Platform bug. To make it more explicit, the following features are implemented: If a job fails due to server-related issue - a more friendly message is displayed, describing a reason for the hardware failure: If a batch job fails due to server-related issue and AWS reports one of the following EC2 status codes - batch job is restarted from scratch: Server.SpotInstanceShutdown - AWS stopped a spot instance due to price changes Server.SpotInstanceTermination - AWS terminated a spot instance due to price changes Server.InternalError - AWS hardware issue Administrator can configure whether to apply this behavior and how much retries shall be performed: For more information about automatically reruns batch jobs in cases when spot instances are terminated see here and in section 12.10. Manage system-level settings . Displaying estimated price of a job run Now a list of active runs (both \"ACTIVE RUNS\" menu and \"Dashboard\") shows \"Estimated price\", which is calculated based on the run duration and selected instance type. This field is updated interactively (i.e. each 5 - 10 seconds) ACTIVE RUNS menu Dashboard widget For more information and examples of using see in sections 11. Manage Runs , 18. Home page . Displaying details of the user profile Previously only user id was shown within all GUI forms, that displayed object/run OWNER. Now one can get information on the user name/email when hovering user id, which is shown in the GUI. It is shown in the tooltip with all information available from the IdP. Breadcrumbs for the Library view Previously user was able to collapse a \"Library\" tree view using button. This allows to work with the plain objects lists, which is more comfortable for certain users, compared to the hierarchy. But navigation to the upper level of the hierarchy was not convenient in a collapsed mode. Now breadcrumbs are shown in the header of the plain objects list, which allow to view current path and navigate to the upper level by clicking a path item. Expanded mode (default) Collapsed mode","title":"v.0.13"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/#cloud-pipeline-v013-release-notes","text":"Data sharing with external collaborators Batch processing in EU Running instances sharing with other user(s) or group(s) of users Automated instances pause/stop according to the resource usage Tools versions details Data download from external http/ftp resources to the cloud data storage Automatically rerun a batch job if a spot instance is terminated Displaying estimated price of a job run Displaying details of the user profile Breadcrumbs for the Library view","title":"Cloud Pipeline v.0.13 - Release notes"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/#data-sharing-with-external-collaborators","text":"NGS users often get the raw datasets from the external partners for processing. Typically external collaborator sends such datasets using hard drives. To enable such type of collaboration - S3 buckets within a Cloud Platform can now be \"Shared\". When a bucket is created - owner can set \"Enable sharing\" option. Bucket will be created and can be managed and consumed as any other bucket: But such types of buckets also display a \"Share\" button, which can be used to generate URL, that can be shared with the external collaborator Click \"Share\" Get the URL and send it to the external partner Once external users loads this URL: Authentication is performed using SAML Access is granted according to the user's permissions S3 bucket browser is displayed This collaboration space can be used to exchange large data files (up to 5Tb per one file) Compared to the \" Data download from external http/ftp resources to the cloud data storage \" feature (see below) - this use case considers that external colleague cannot provide a URL for direct download . For more information about data sharing with external collaborators see 8.8. Data sharing .","title":"Data sharing with external collaborators"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/#batch-processing-in-eu","text":"Previously all computing nodes and storages were located in the US region. For EU NGS use cases, which operate on huge data volumes, data movement to US took too much time. To workaround this issue - CP Platform was improved to support batch processing and compute nodes management within other Cloud regions. Bucket creation form now allows to set - where to create a data storage: All the buckets, that are shown in the \"Library tree view\", \"Library details form\" and \"Home dashboard\" are now tagged with region flag to visually distinguish storage locations: Library tree Library details view Home dashboard When a pipeline configuration is created within a project or a new run is launched - user can specify to use a specific region for a compute node placement: Project method configuration Launch form configuration Examples of using Cloud regions see in sections 6. Manage Pipeline , 7. Manage Detached configuration , 8. Manage Data Storage .","title":"Batch processing in EU"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/#running-instances-sharing-with-other-users-or-groups-of-users","text":"For certain use cases it is beneficial to be able to share applications with other users/groups. Thus, providing a \"persistent\" service that can be shared and run in the Cloud platform. Current version introduces a feature that allows to: Specify a \"Friendly URL\" for persistent services. This produces endpoint URL in a more friendly/descriptive format: {cloud-pipeline_url}/ friendly_url instead of {cloud-pipeline_url}/ pipeline-XXXX-XXXX . This can be configured at a service launch time in the \"Advanced\" section of the Launch form (name shall be unique) URL will be generated using the specified name For more information about \"Friendly URL\" for persistent services see here . User can now share a run with others: \"Share with: ...\" parameter, within a run log form, can be used for this Specific users or whole groups can be set for sharing Once this is set - other users will be able to access run's endpoints (not SSH) \"Services\" widget within a Home dashboard page is now capable of listing such \"shared\" services. It is intended to display a \"catalog\" of services, that can be accessed by a current user, without running own jobs. For more information about runs sharing see 11.3. Sharing with other users or groups of users .","title":"Running instances sharing with other user(s) or group(s) of users"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/#automated-instances-pausestop-according-to-the-resource-usage","text":"Version v0.12 introduced a PAUSE/RESUME option for the users, which allowed to persist whole state of the environment and resume it. This feature required AWS On-Demand instances to be used, which are more expensive compared to Spots. Current version provides a way to control spendings by automatically pausing on-demand instances if they are not used. Administrators can now control this behaviour using a set of parameters: system.idle.cpu.threshold - specify %% of the average CPU, below which action shall be taken system.resource.monitoring.period - specify period (in seconds) between the users' instances scanning to collect the monitoring metrics system.max.idle.timeout.minutes - specify a duration in minutes. If CPU utilization is below system.idle.cpu.threshold for this duration - notification will be sent to the user. system.idle.action.timeout.minutes - specify a duration in minutes. If CPU utilization is below system.idle.cpu.threshold for this duration - an action, specified in system.idle.action will be performed system.idle.action - which action to perform on the instance, that showed low CPU utilization: NOTIFY - only send notification PAUSE - pause an instance if possible (e.g. instance is On-Demand, Spot instances are skipped) PAUSE_OR_STOP - pause an instance if it is On-Demand, stop an instance if it is Spot STOP - Stop an instance, disregarding price-type","title":"Automated instances pause/stop according to the resource usage"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/#tools-versions-details","text":"Now more information on the docker image version is available: Image size Modified date Unique identifier (Digest) Corresponding aliases (e.g. if some digest has two aliases) For more information see here .","title":"Tools versions details"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/#data-download-from-external-httpftp-resources-to-the-cloud-data-storage","text":"Users often get the raw datasets from the external partners for processing. Previously, users had to get the data to the local cluster storage and them upload data to the cloud using clommand-line interface. Now users can provide CSV/TSV files with the external links and submit a data transfer job, so that the files will be moved to the cloud storage in the background. Upload CSV/TSV file and view list of samples with the external links: Select the transfer options: Which S3 bucket to use as a destination Which CSV/TSV columns shall be used to get external URLs (if several columns contain URLs - both can be used) ( optionally ) Select whether to rename resulting path to some other value (can be specified as another column cell, e.g. sample name) ( optionally ) Create new folders within destination if several columns are selected for \"Path fields\" option. E.g. if two columns contain URLs and both are selected - then folders will be created for the corresponding column name and used for appropriate files storage. Whether to update external URL within a table to the new location of the files. If set - http/ftp URLs will be changed to s3 path. Such data structure can be then used for a processing by a pipeline (See below - URLs are changed to the S3-clickable hyperlinks): Once transfer job is finished - files will be located in the selected S3 storage: For more information about data downloading from external http/ftp resources to the CP see 5.5. Download data from external resources to the cloud data storage .","title":"Data download from external http/ftp resources to the cloud data storage"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/#automatically-rerun-a-batch-job-if-a-spot-instance-is-terminated","text":"In certain cases - AWS may terminate a node, that is used to run a job or an interactive tool: Spot prices changed AWS experienced a hardware issue These cases shall not be treated as a Cloud Platform bug. To make it more explicit, the following features are implemented: If a job fails due to server-related issue - a more friendly message is displayed, describing a reason for the hardware failure: If a batch job fails due to server-related issue and AWS reports one of the following EC2 status codes - batch job is restarted from scratch: Server.SpotInstanceShutdown - AWS stopped a spot instance due to price changes Server.SpotInstanceTermination - AWS terminated a spot instance due to price changes Server.InternalError - AWS hardware issue Administrator can configure whether to apply this behavior and how much retries shall be performed: For more information about automatically reruns batch jobs in cases when spot instances are terminated see here and in section 12.10. Manage system-level settings .","title":"Automatically rerun a batch job if a spot instance is terminated"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/#displaying-estimated-price-of-a-job-run","text":"Now a list of active runs (both \"ACTIVE RUNS\" menu and \"Dashboard\") shows \"Estimated price\", which is calculated based on the run duration and selected instance type. This field is updated interactively (i.e. each 5 - 10 seconds) ACTIVE RUNS menu Dashboard widget For more information and examples of using see in sections 11. Manage Runs , 18. Home page .","title":"Displaying estimated price of a job run"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/#displaying-details-of-the-user-profile","text":"Previously only user id was shown within all GUI forms, that displayed object/run OWNER. Now one can get information on the user name/email when hovering user id, which is shown in the GUI. It is shown in the tooltip with all information available from the IdP.","title":"Displaying details of the user profile"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/#breadcrumbs-for-the-library-view","text":"Previously user was able to collapse a \"Library\" tree view using button. This allows to work with the plain objects lists, which is more comfortable for certain users, compared to the hierarchy. But navigation to the upper level of the hierarchy was not convenient in a collapsed mode. Now breadcrumbs are shown in the header of the plain objects list, which allow to view current path and navigate to the upper level by clicking a path item. Expanded mode (default) Collapsed mode","title":"Breadcrumbs for the Library view"},{"location":"release_notes/v.0.14/v.0.14_-_Release_notes/","text":"Cloud Pipeline v.0.14 - Release notes Docker image installed packages list Docker image version settings Global Search Default values (restrictions) on instance types for the users (groups) and tools Auto-scaled cluster A mandatory prefix for the new creating S3-buckets \"White list\" for docker images \"Grace\" period for unscanned or vulnerable docker images Docker image installed packages list Often users would like to know the full list software packages installed into a specific Docker images. E.g. to decide which one to run. Now this information is available from the docker version menu. User can click a specific version of the tool \" Packages \" tab will shown in the list of tabs in menu of tool's version. List of packages is generated from the docker version together with vulnerabilities scanning (introduced in v0.12). This occurs nightly (all dockers are scanned) or if admin explicitly requests scanning by clicking SCAN button for a specific version. Currently the following types of software packages can be scanned: System package manager's database (i.e. yum , apt ) R packages Python packages Software packages are combined into groups named \" Ecosystems \". User can select one of the ecosystems in the dropdown list and view its contents: Information about each package contains the package name and its short description ( if available ). User can filter packages by name (search will be done across all ecosystems ) For more details see here . Docker image version settings Previously docker image settings (instance type, disk, default command) were set using the following approaches: Global defaults Docker image settings Specific run launch parameters This introduces a number of limitations, e.g. if a docker image contains two version: one with CPU-only and another with GPU support, user was not able to define which instance type to use for what version. Now settings can be applied to the specific docker version. These settings are defined in the \" Settings \" tab of the tool's version menu: If these (version-level) settings are specified - they will be applied to each run of the docker image: All other settings levels are still remaining in place. If version-specific settings are not defined: docker-level settings will be applied. For more details see here . Global Search While Cloud Pipeline grows in terms of data and pipelines being add/implemented - it is crucial to be able to search for specific datasets or tools. Previously user was able to search only for the jobs runs. In v0.14 of the Cloud Pipeline Platform - a capability to search over all existing objects types is implemented. The following object types are being indexed and can be searched: \" FOLDERS \" - in the folders (\"Library hierarchy\") and metadata entities \" PIPELINES \" - in the pipelines metadata, pipelines files (documents) and configurations \" RUNS \" - in the runs information \" TOOLS \" - in the docker registries, groups and tools \" STORAGES \" - in S3/NFS storages metadata, S3/NFS files (names) \" ISSUES \" - in the issues (discussions) User can open a search form by pressing \"Ctrl+F\" being at any page of the Cloud Pipeline Web GUI ( excluding case when run's logs page is open ) or by clicking on in left menu bar ( global searching form will not be opened if any pop-up window is shown ): To start searching a \"google-like\" query string shall be entered (search can be triggered by pressing \"Enter\" button or automatically if no new input is provided for 2 seconds): Special expressions in query string are available as well (the rules for their assignment are described in pop-up window that appears when hovering over the icon): By default search will occur across all the available object types. If user would to limit search scope - appropriate section can be selected above the query input: To get a brief information on the object found, user can hover an item with a mouse and a \"Preview\" pane will be shown to the right or click an entry to navigate to it's location within the Cloud Pipeline. Tool preview: Pipeline preview: In the \"Preview\" window user can see: name of the found object path to the object in library description ( optionally ) block with indication and highlighting of the object's concrete part, where inputted word was found preview of the found object ( if it's available ) Default values (restrictions) on instance types for the users (groups) and tools Users may make a mistake while selecting instance types and storages when launching a run. This may be: Too large instance type for the job Spot instance for GPU job Not valid data storage path etc. Now admin can restrict certain options for the specific users/groups or tools to minimize a number of invalid configurations runs. Restrictions could be set by different ways on several forms: within \"User management\" tab (for more information see 12.4. Edit/delete a user and 12.6. Edit a group/role ) admin can specify for a user or a group of users (role) allowed price types and allowed instance types for the pipelines, configurations and tool runs: within \"Instance management\" panel in tool settings (for more information see 10.5. Launch a Tool ) admin can specify allowed price types and allowed instance types for Tool runs: within \"Cluster\" tab in \"Preferences\" section of the system-level settings (see here ) admin also can specify allowed price types (with setting cluster.allowed.price.types ) and allowed instance types for the pipelines, configurations and tool runs (with settings cluster.allowed.instance.types and cluster.allowed.instance.types.docker ) as global defaults: Next hierarchy is set for applying of inputted allowed instance types (sorted by priority): User level (specified for a user on \"User management\" tab) User group level (specified for a group (role) on \"User management\" tab. If a user is a member of several groups - list of allowed instances will be summarized across all the groups) Tool level (specified for a tool on \"Instance management\" panel ) (global) cluster.allowed.instance.types.docker (specified on \"Cluster\" tab in \"Preferences\" section of the system-level settings) (global) cluster.allowed.instance.types (specified on \"Cluster\" tab in \"Preferences\" section of the system-level settings) After specifying allowed instance types, all GUI forms that allow to select the list of instance types (configurations/launch forms) - will display only valid instance type, according to hierarchy above. For price type specifying - if it is set for the user/group/tool - GUI will allow to select only that price type. Auto-scaled cluster Previously, when you needed several nodes running at one task, you could configure and launch cluster: master machine and one or several worker machines. Their count was predefined before launching the task and could not changing during the run. In current version, the auto-scaled cluster is implemented. This one differs from \"usual\" cluster in that it can attach or drop additional nodes during the run depending on the queue load. Using such type of cluster will minimize total cost. If during the run there are jobs in waiting state longer than a specific time ( this time threshold is set by admin in \"Preferences\" section of the system-level settings ) - auto-scaled cluster will attach new computation nodes (\"scale-up\"). If during the run the queue is empty or all jobs are running longer than a specific time ( this time threshold is set by admin tab in \"Preferences\" section of the system-level settings ) - auto-scaled cluster will drop existing auto-scaled nodes (\"scale-down\"). For this cluster user may specify total count of child nodes - max count of auto-scaled nodes, and the count of \"persistent\" child nodes - count of nodes that will never be \"scaled-down\". To set auto-scaled cluster click \"Configure cluster\" in \"Exec environment\" panel before launch a run: In pop-up window select \"Auto-scaled cluster\" and specify total count of \"auto-scaled\" nodes: If you want to set a count of \"persistent\" child nodes, click \"Setup default child nodes count\" and specify the value: For more details see here and here . A mandatory prefix for the new creating S3-buckets Now admin can set storage.object.prefix on \"Data storage\" tab in \"Preferences\" section of the system-level settings: If it is set, all new storages will be created with this prefix (e.g. \" ds \"): For more information see 8.1. Create and edit storage . \"White list\" for docker images Previously user is not able to run a new image if it is not scanned yet or has a lot of vulnerabilities. In current version a \"white list\" option for docker images is implemented. It's meaning that admin may allow users to run certain docker versions even if they are vulnerable/unscanned. For do that admin has to check a docker version in tool-versions list by click on special flag \"Add to white list\" near the \"SCAN\" button: In this case user would be able to run such version of tool and none errors will be displayed during launch time and viewing. Only admin can \"add\"/\"remove\" tool version to the \"white list\". For more details see 10.6. Tool security check . \"Grace\" period for unscanned or vulnerable docker images Previously user is not able to run a new image if it is not scanned yet or has a lot of vulnerabilities. In current version a \"grace\" period option for such docker images is implemented. During this period user will be able to run a tool, but an appropriate message will be displayed when viewing a tool or running it. The duration of \"grace\" period (in hours ) is set on \"Docker security\" tab in \"Preferences\" section of the system-level settings: If this time value is not elapsed from the date/time since the docker version became vulnerable or since the push time (if this version was not scanned yet) - user would be able to run such version of tool, but warning message will be displaying during version launch. Since \"grace\" period is elapsed for this tool's version - behavior will became as for \"usual\" vulnerable/unscanned docker image. For more details see 10.6. Tool security check and 12.10. Manage the system-level settings .","title":"v.0.14"},{"location":"release_notes/v.0.14/v.0.14_-_Release_notes/#cloud-pipeline-v014-release-notes","text":"Docker image installed packages list Docker image version settings Global Search Default values (restrictions) on instance types for the users (groups) and tools Auto-scaled cluster A mandatory prefix for the new creating S3-buckets \"White list\" for docker images \"Grace\" period for unscanned or vulnerable docker images","title":"Cloud Pipeline v.0.14 - Release notes"},{"location":"release_notes/v.0.14/v.0.14_-_Release_notes/#docker-image-installed-packages-list","text":"Often users would like to know the full list software packages installed into a specific Docker images. E.g. to decide which one to run. Now this information is available from the docker version menu. User can click a specific version of the tool \" Packages \" tab will shown in the list of tabs in menu of tool's version. List of packages is generated from the docker version together with vulnerabilities scanning (introduced in v0.12). This occurs nightly (all dockers are scanned) or if admin explicitly requests scanning by clicking SCAN button for a specific version. Currently the following types of software packages can be scanned: System package manager's database (i.e. yum , apt ) R packages Python packages Software packages are combined into groups named \" Ecosystems \". User can select one of the ecosystems in the dropdown list and view its contents: Information about each package contains the package name and its short description ( if available ). User can filter packages by name (search will be done across all ecosystems ) For more details see here .","title":"Docker image installed packages list"},{"location":"release_notes/v.0.14/v.0.14_-_Release_notes/#docker-image-version-settings","text":"Previously docker image settings (instance type, disk, default command) were set using the following approaches: Global defaults Docker image settings Specific run launch parameters This introduces a number of limitations, e.g. if a docker image contains two version: one with CPU-only and another with GPU support, user was not able to define which instance type to use for what version. Now settings can be applied to the specific docker version. These settings are defined in the \" Settings \" tab of the tool's version menu: If these (version-level) settings are specified - they will be applied to each run of the docker image: All other settings levels are still remaining in place. If version-specific settings are not defined: docker-level settings will be applied. For more details see here .","title":"Docker image version settings"},{"location":"release_notes/v.0.14/v.0.14_-_Release_notes/#global-search","text":"While Cloud Pipeline grows in terms of data and pipelines being add/implemented - it is crucial to be able to search for specific datasets or tools. Previously user was able to search only for the jobs runs. In v0.14 of the Cloud Pipeline Platform - a capability to search over all existing objects types is implemented. The following object types are being indexed and can be searched: \" FOLDERS \" - in the folders (\"Library hierarchy\") and metadata entities \" PIPELINES \" - in the pipelines metadata, pipelines files (documents) and configurations \" RUNS \" - in the runs information \" TOOLS \" - in the docker registries, groups and tools \" STORAGES \" - in S3/NFS storages metadata, S3/NFS files (names) \" ISSUES \" - in the issues (discussions) User can open a search form by pressing \"Ctrl+F\" being at any page of the Cloud Pipeline Web GUI ( excluding case when run's logs page is open ) or by clicking on in left menu bar ( global searching form will not be opened if any pop-up window is shown ): To start searching a \"google-like\" query string shall be entered (search can be triggered by pressing \"Enter\" button or automatically if no new input is provided for 2 seconds): Special expressions in query string are available as well (the rules for their assignment are described in pop-up window that appears when hovering over the icon): By default search will occur across all the available object types. If user would to limit search scope - appropriate section can be selected above the query input: To get a brief information on the object found, user can hover an item with a mouse and a \"Preview\" pane will be shown to the right or click an entry to navigate to it's location within the Cloud Pipeline. Tool preview: Pipeline preview: In the \"Preview\" window user can see: name of the found object path to the object in library description ( optionally ) block with indication and highlighting of the object's concrete part, where inputted word was found preview of the found object ( if it's available )","title":"Global Search"},{"location":"release_notes/v.0.14/v.0.14_-_Release_notes/#default-values-restrictions-on-instance-types-for-the-users-groups-and-tools","text":"Users may make a mistake while selecting instance types and storages when launching a run. This may be: Too large instance type for the job Spot instance for GPU job Not valid data storage path etc. Now admin can restrict certain options for the specific users/groups or tools to minimize a number of invalid configurations runs. Restrictions could be set by different ways on several forms: within \"User management\" tab (for more information see 12.4. Edit/delete a user and 12.6. Edit a group/role ) admin can specify for a user or a group of users (role) allowed price types and allowed instance types for the pipelines, configurations and tool runs: within \"Instance management\" panel in tool settings (for more information see 10.5. Launch a Tool ) admin can specify allowed price types and allowed instance types for Tool runs: within \"Cluster\" tab in \"Preferences\" section of the system-level settings (see here ) admin also can specify allowed price types (with setting cluster.allowed.price.types ) and allowed instance types for the pipelines, configurations and tool runs (with settings cluster.allowed.instance.types and cluster.allowed.instance.types.docker ) as global defaults: Next hierarchy is set for applying of inputted allowed instance types (sorted by priority): User level (specified for a user on \"User management\" tab) User group level (specified for a group (role) on \"User management\" tab. If a user is a member of several groups - list of allowed instances will be summarized across all the groups) Tool level (specified for a tool on \"Instance management\" panel ) (global) cluster.allowed.instance.types.docker (specified on \"Cluster\" tab in \"Preferences\" section of the system-level settings) (global) cluster.allowed.instance.types (specified on \"Cluster\" tab in \"Preferences\" section of the system-level settings) After specifying allowed instance types, all GUI forms that allow to select the list of instance types (configurations/launch forms) - will display only valid instance type, according to hierarchy above. For price type specifying - if it is set for the user/group/tool - GUI will allow to select only that price type.","title":"Default values (restrictions) on instance types for the users (groups) and tools"},{"location":"release_notes/v.0.14/v.0.14_-_Release_notes/#auto-scaled-cluster","text":"Previously, when you needed several nodes running at one task, you could configure and launch cluster: master machine and one or several worker machines. Their count was predefined before launching the task and could not changing during the run. In current version, the auto-scaled cluster is implemented. This one differs from \"usual\" cluster in that it can attach or drop additional nodes during the run depending on the queue load. Using such type of cluster will minimize total cost. If during the run there are jobs in waiting state longer than a specific time ( this time threshold is set by admin in \"Preferences\" section of the system-level settings ) - auto-scaled cluster will attach new computation nodes (\"scale-up\"). If during the run the queue is empty or all jobs are running longer than a specific time ( this time threshold is set by admin tab in \"Preferences\" section of the system-level settings ) - auto-scaled cluster will drop existing auto-scaled nodes (\"scale-down\"). For this cluster user may specify total count of child nodes - max count of auto-scaled nodes, and the count of \"persistent\" child nodes - count of nodes that will never be \"scaled-down\". To set auto-scaled cluster click \"Configure cluster\" in \"Exec environment\" panel before launch a run: In pop-up window select \"Auto-scaled cluster\" and specify total count of \"auto-scaled\" nodes: If you want to set a count of \"persistent\" child nodes, click \"Setup default child nodes count\" and specify the value: For more details see here and here .","title":"Auto-scaled cluster"},{"location":"release_notes/v.0.14/v.0.14_-_Release_notes/#a-mandatory-prefix-for-the-new-creating-s3-buckets","text":"Now admin can set storage.object.prefix on \"Data storage\" tab in \"Preferences\" section of the system-level settings: If it is set, all new storages will be created with this prefix (e.g. \" ds \"): For more information see 8.1. Create and edit storage .","title":"A mandatory prefix for the new creating S3-buckets"},{"location":"release_notes/v.0.14/v.0.14_-_Release_notes/#white-list-for-docker-images","text":"Previously user is not able to run a new image if it is not scanned yet or has a lot of vulnerabilities. In current version a \"white list\" option for docker images is implemented. It's meaning that admin may allow users to run certain docker versions even if they are vulnerable/unscanned. For do that admin has to check a docker version in tool-versions list by click on special flag \"Add to white list\" near the \"SCAN\" button: In this case user would be able to run such version of tool and none errors will be displayed during launch time and viewing. Only admin can \"add\"/\"remove\" tool version to the \"white list\". For more details see 10.6. Tool security check .","title":"\"White list\" for docker images"},{"location":"release_notes/v.0.14/v.0.14_-_Release_notes/#grace-period-for-unscanned-or-vulnerable-docker-images","text":"Previously user is not able to run a new image if it is not scanned yet or has a lot of vulnerabilities. In current version a \"grace\" period option for such docker images is implemented. During this period user will be able to run a tool, but an appropriate message will be displayed when viewing a tool or running it. The duration of \"grace\" period (in hours ) is set on \"Docker security\" tab in \"Preferences\" section of the system-level settings: If this time value is not elapsed from the date/time since the docker version became vulnerable or since the push time (if this version was not scanned yet) - user would be able to run such version of tool, but warning message will be displaying during version launch. Since \"grace\" period is elapsed for this tool's version - behavior will became as for \"usual\" vulnerable/unscanned docker image. For more details see 10.6. Tool security check and 12.10. Manage the system-level settings .","title":"\"Grace\" period for unscanned or vulnerable docker images"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/","text":"Cloud Pipeline v.0.15 - Release notes Microsoft Azure Support SAML claims based access Notifications on the RAM/Disk pressure Limit mounted storages Personal SSH keys configuration Allow to set the Grid Engine capability for the \"fixed\" cluster Enable Apache Spark for the Cloud Pipeline's clusters Consider Cloud Providers' resource limitations when scheduling a job Allow to terminate paused runs Pre/Post-commit hooks implementation Restricting manual installation of the nvidia tools Setup swap files for the Cloud VMs Run's system paths shall be available for the general user account Renewed WDL visualization \"QUEUED\" state of the run Help tooltips for the run state icons VM monitor service Web GUI caching Installation via pipectl Add more logging to troubleshoot unexpected pods failures Displaying information on the nested runs Environment Modules support Sharing SSH access to running instances with other user(s)/group(s) Allow to limit the number of concurrent SSH sessions Verification of docker/storage permissions when launching a run Ability to override the queue/PE configuration in the GE configuration Estimation run's disk size according to the input/common parameters Disabling of the Global Search form if a corresponding service is not installed Disabling of the FS mounts creation if no FS mount points are registered Displaying resource limit errors during run resuming Object storage creation in despite of that the CORS/Policies could not be applied Track the confirmation of the \"Blocking\" notifications pipe CLI warnings on the JWT expiration pipe configuration for using NTLM Authentication Proxy Files uploading via pipe in case of restrictions Run a single command or an interactive session over the SSH protocol via pipe Perform objects restore in a batch mode via pipe Mounting data storages to Linux and Mac workstations Allow to run pipe commands on behalf of the other user Ability to restrict the visibility of the jobs Ability to perform scheduled runs from detached configurations Using custom domain names as a \"friendly URL\" for the interactive services Displaying of the additional support icon/info Pass proxy settings to the DIND containers Interactive endpoints can be (optionally) available to the anonymous users Notable Bug fixes Incorrect behavior of the global search filter \"COMMITING...\" status hangs Instances of Metadata entity aren't correctly sorted Tool group cannot be deleted until all child tools are removed Missing region while estimating a run price Cannot specify region when an existing object storage is added ACL control for PIPELINE_USER and ROLE entities for metadata API Getting logs from Kubernetes may cause OutOfMemory error AWS: Incorrect nodeup handling of spot request status Not handling clusters in autopause daemon Incorrect pipe CLI version displaying JWT token shall be updated for the jobs being resumed Trying to rename file in the data storage, while the \"Attributes\" panel is opened, throws an error pipe : incorrect behavior of the -nc option for the run command Cluster run cannot be launched with a Pretty URL Cloning of large repositories might fail System events HTML overflow AWS: Pipeline run InitializeNode task fails git-sync shall not fail the whole object synchronization if a single entry errors endDate isn't set when node of a paused run was terminated AWS: Nodeup retry process may stuck when first attempt to create a spot instance failed Resume job timeout throws strange error message GE autoscaler doesn't remove dead additional workers from cluster Broken layouts Microsoft Azure Support One of the major v0.15 features is a support for the Microsoft Azure Cloud . All the features, that were previously used for AWS , are now available in all the same manner, from all the same GUI/CLI, for Azure . Another cool thing, is that now it is possible to have a single installation of the Cloud Pipeline , which will manage both Cloud Providers ( AWS and Azure ). This provides a flexibility to launch jobs in the locations, closer to the data, with cheaper prices or better compute hardware for a specific task. SAML claims based access Previously, there were two options for users to be generally authenticated in Cloud Pipeline Platform, i.e. to pass SAML validation: Auto-register - any valid SAML authentication automatically registers user (if he isn't registered yet) and grant ROLE_USER access Explicit-register - after a valid SAML authentication, it is checked whether this user is already registered in the Cloud Pipeline catalog, and if no - request denies, authentication fails To automate things a bit more, in v0.15 additional way to grant first-time access to the users was implemented: Explicit-group register. If such registration type is set - once a valid SAML authentication is received, it is checked, whether SAML response contains any of the domain groups, that are already granted any access to the Cloud Pipeline objects (registered in Cloud Pipeline catalog). If so - proceeds as with Auto-register - user with all his domain groups and granted ROLE_USER access is being registered. If user's SAML domain groups aren't intersected with pre-registered groups the authentication fails. This Platform's behavior is set via application property saml.user.auto.create that could accept one of the corresponding values: AUTO , EXPLICIT , EXPLICIT_GROUP . Notifications on the RAM/Disk pressure When a compute-intensive job is run - compute node may start starving for the resources. CPU high load is typically a normal situation - it could result just to the SSH/metrics slowdown. But running low on memory and disk could crash the node, in such cases autoscaler will eventually terminate the cloud instance. In v0.15 version, the Cloud Pipeline platform could notify user on the fact of Memory/Disk high load. When memory or disk consuming will be higher than a threshold value for a specified period of time (in average) - a notification will be sent (and resent after a delay, if the problem is still in place). Such notifications could be configured at HIGH_CONSUMED_RESOURCES section of the Email notifications : The following items at System section of the Preferences define behavior of such notifications: system.memory.consume.threshold - memory threshold (in %) above which the notification would be sent system.disk.consume.threshold - disk threshold (in %) above which the notification would be sent system.monitoring.time.range - time delay (in sec) after which the notification would be sent again, if the problem is still in place. See more information about Email notifications and System Preferences . Limit mounted storages Previously, all available storages were mounted to the container during the run initialization. User could have access to them via /cloud-data or ~/cloud-data folder using the interactive sessions (SSH/Web endpoints/Desktop) or pipeline runs. For certain reasons (e.g. takes too much time to mount all or a run is going to be shared with others), user may want to limit the number of data storages being mounted to a specific job run. Now, user can configure the list of the storages that will be mounted. This can be accomplished using the Limit mount field of the Launch form: By default, All available storages are mounted (i.e. the ones, that user has ro or rw permissions) To change the default behavior - click the drop-down list next to \"Limit mounts\" label: Select storages that shall be mounted: Review that only a limited number of data storages is mounted: Mounted storage is available for the interactive/batch jobs using the path /cloud-data/{storage_name} : See an example here . Personal SSH keys configuration Previously, there were two options to communicate with the embedded gitlab repositories, that host pipelines code: From the local (non-cloud) machine: use the https protocol and the repository URI, provided by the GUI (i.e. GIT REPOSITORY button in the pipeline view) From the cloud compute node: git command line interface, that is preconfigured to authenticate using https protocol and the auto-generated credentials Both options consider that https is used. It worked fine for 99% of the use cases. But for some of the applications - ssh protocol is required as this is the only way to achieve a passwordless authentication against the gitlab instance To address this issue, SSH keys management was introduced: Users' ssh keys are generated by the Git Sync service SSH key-pair is created and assigned to the Cloud Pipeline user and a public key is registered in the GitLab Those SSH keys are now also configured for all the runs, launched by the user. So it is possible to perform a passwordless authentication, when working with the gitlab or other services, that will be implemented in the near future HTTPS/SSH selector is added to the GIT REPOSITORY popup of the Cloud Pipeline Web GUI: Default selection is HTTP, which displays the same URI as previously (repository), but user is able to switch it to the SSH and get the reformatted address: For more details see here . Allow to set the Grid Engine capability for the \"fixed\" cluster Version 0.14 introduced an ability to launch autoscaled clusters. Besides the autoscaling itself - such cluster were configured to use GridEngine server by default, which is pretty handy. On the other hand - fixed cluster (i.e. those which contain a predefined number of compute nodes), required user to set the CP_CAP_SGE explicitly. Which is no a big deal, but may be tedious. In v0.15 Grid Engine can be configured within the Cluster (fixed size cluster) tab. This is accomplished by using the Enable GridEngine checkbox. By default, this checkbox is unticked. If the user sets this ON - CP_CAP_SGE parameter is added automatically. Also a number of help icons is added to the Cluster configuration dialog to clarify the controls purpose: Popup header (E.g. next to the tabs line) - displays information on different cluster modes (Cluster) Enable GridEngine checkbox - displays information on the GridEngine usage (Cluster) Enable Apache Spark checkbox - displays information on the Apache Spark usage (see below ) (Auto-scaled cluster) Auto-scaled up - displays information on the autoscaling logic (Auto-scaled cluster) Default child nodes - displays information on the initial node pool size See more information about cluster launch here . Enable Apache Spark for the Cloud Pipeline's clusters Another one feature for the Cloud Pipeline's clusters was implemented in v0.15 . Now, Apache Spark with the access to File/Object Storages from the Spark Applications can be configured within the Cluster tab. It is available only for the fixed size clusters. To enable this feature - tick the Enable Apache Spark checkbox and set the child nodes count at cluster settings. By default, this checkbox is unticked. Also users can manually enable Spark functionality by the CP_CAP_SPARK system parameter: This feature, for example, allows you to run Apache Spark cluster with RStudio where you may code in R using sparklyr to run the workload over the cluster: Open the RStudio tool Select the node type, set the Apache Spark cluster as shown above, and launch the tool: Open main Dashboard and wait until the OPEN hyperlink for the launched tool will appear. Hover over it: Two endpoints will appear: RStudio (as the main endpoint it is in bold) - it exposes RStudio's Web IDE SparkUI - it exposes Web GUI of the Spark. It allows to monitor Spark master/workers/application via the web-browser. Details are available in the Spark UI manual Click the RStudio endpoint: Here you can start create scripts in R using the pre-installed sparklyr package to distribute the workload over the cluster. Click the SparkUI endpoint: Here you can view the details of the jobs being executed in Spark, how the memory is used and get other useful information. For more information about using Apache Spark via the Cloud Pipeline see here . Consider Cloud Providers' resource limitations when scheduling a job Cloud Pipeline supports queueing of the jobs, that cannot be scheduled to the existing or new compute nodes immediately. Queueing occurs if: cluster.max.size limit is reached (configurable via Preferences ) Cloud Provider limitations are reached (e.g. AWS EC2 Limits ) If ( 1 ) happens - job will sit in the QUEUED state, until a spare will be available or stopped manually. This is the correct behavior. But if ( 2 ) occurs - an error will be raised by the Cloud Provider, Cloud Pipeline treats this as an issue with the node creation. Autoscaler will then resubmit the node creation task for cluster.nodeup.retry.count times (default: 5) and then fail the job run. This behavior confuses users, as ( 2 ) shall behave almost in the same manner as ( 1 ) - job shall be kept in the queue until there will be free space for a new node. In v0.15 ( 2 ) scenario works as described: If a certain limit is reached (e.g. number of m4.large instances exceeds the configured limit) - run will not fail, but will await for a spare node or limit increase A warning will be highlighted in the job initialization log: Allow to terminate paused runs Some of the jobs, that were paused (either manually, or by the automated service), may be not needed anymore. But when a run is paused - the user cannot terminate/stop it before resuming. I.e. one have to run RESUME operation, wait for it's completion and then STOP the run. While this is the expected behavior (at least designed in this manner) - it requires some extra steps to be performed, which may look like meaningless (why one shall resume a run, that is going to be stopped?). Another issue with such a behavior is that in certain \"bad\" conditions - paused runs are not able to resume and just fail, e.g.: An underlying instance is terminated outside of the Cloud Pipeline Docker image was removed from the registry And other cases that are not yet uncovered This introduces a number of stale runs, that just sit there in the PAUSED state and nobody can remove them. To address those concerns - current version of Cloud Pipeline allows to terminate PAUSED run, without a prior RESUME . This operation can be performed by the OWNER of the run and the ADMIN users. Termination of the PAUSED run drops the underlying cloud instance and marks the run as STOPPED . From the GUI perspective - TERMINATE button is shown (instead of STOP ), when a run is in the PAUSED state: on the \"Active runs\" page on the \"Run logs\" page on the \"Dashboard\" page Clicking it - performs the run termination, as described above. See more information here . Pre/Post-commit hooks implementation In certain use-cases, extra steps shall be executed before/after running the commit command in the container. E.g. imagine the following scenario: User launches RStudio User installs packages and commits it as a new image User launches the new image The following error will be displayed in the R Console: 16 Jan 2019 21:17:15 [rsession-GRODE01] ERROR session hadabend; LOGGED FROM: rstudio::core::Error {anonymous}::rInit(const rstudio::r::session::RInitInfo ) /home/ubuntu/rstudio/src/cpp/session/SessionMain.cpp:563 There is nothing bad about this message and states that previous RSession was terminated in a non-graceful manner. RStudio will work correctly, but it may confuse the users. While this is only one example - there are other applications, that require extra cleanup to be performed before the termination. To workaround such issues (RStudio and others) an approach of pre/post-commit hooks is implemented. That allows to perform some graceful cleanup/restore before/after performing the commit itself. Those hooks are valid only for the specific images and therefore shall be contained within those images. Cloud Pipeline itself performs the calls to the hooks if they exist. Two preferences are introduced: commit.pre.command.path : specified a path to a script within a docker image, that will be executed in a currently running container, BEFORE docker commit occurs (default: /root/pre_commit.sh ). This option is useful if any operations shall be performed with the running processes (e.g. send a signal), because in the subsequent post operation - only filesystem operations will be available. Note that any changes done at this stage will affect the running container. commit.post.command.path : specified a path to a script within a docker image, that will be executed in a committed image, AFTER docker commit occurs (default: /root/post_commit.sh ). This hook can be used to perform any filesystem cleanup or other operations, that shall not affect the currently running processes. If a corresponding pre/post script is not found in the docker image - it will not be executed. For more details see here . Restricting manual installation of the nvidia tools It was uncovered that some of the GPU-enabled runs are not able to initialize due to an issue describe at NVIDIA/nvidia-docker#825 . To limit a possibility of producing such docker images (which will not be able to start using GPU instance types) - a set of restrictions/notifications was implemented: A notification is now displayed (in the Web GUI), that warns a user about the risks of installing any of the nvidia packages manually. And that all cuda-based dockers shall be built using nvidia/cuda base images instead: Restrict users (to the reasonable extent) from installing those packages while running SSH/terminal session in the container. If user will try to install a restricted package - a warning will be shown (with an option to bypass it - for the advanced users): Setup swap files for the Cloud VMs This feature is addresses the same issues as the previous Notifications about high resources pressure by making the compute-intensive jobs runs more reliable. In certain cases jobs may fail with unexpected errors if the compute node runs Out Of Memory . v0.15 provides an ability for admin users to configure a default swap volume to the compute node being created. This allows to avoid runs failures due to memory limits. The size of the swap volume can be configured via cluster.networks.config item of the Preferences . It is accomplished by adding the similar json object to the platform's global or a region/cloud specific configuration: Options that can be used to configure swap : swap_ratio - defines a swap file size. It is equal the node RAM multiplied by that ratio. If ratio is 0, a swap file will not be created (default: 0) swap_location - defines a location of the swap file. If that option is not set - default location will be used (default: AWS will use SSD/gp2 EBS, Azure will be Temporary Storage ) See an example here . Run's system paths shall be available for the general user account Previously, all the system-level directories (e.g. pipeline code location - $SCRIPTS_DIR , input/common data folders - $INPUT_DATA , etc.) were owned by the root user with read-only access to the general users. This was working fine for the pipeline runs, as they are executed on behalf of root . But for the interactive sessions (SSH/Web endpoints/Desktop) - such location were not writable. From now on - all the system-level location will be granted rwx access for the OWNER of the job (the user, who launched that run). Renewed WDL visualization v0.15 offers an updated Web GUI viewer/editor for the WDL scripts. These improvements allow to focus on the WDL diagram and make it more readable and clear. Which very useful for large WDL scripts. Auxiliary controls (\"Save\", \"Revert changes\", \"Layout\", \"Fit to screen\", \"Show links\", \"Zoom out\", \"Zoom in\", \"Fullscreen\") are moved to the left side of the WDL GRAPH into single menu: WDL search capabilities are added. This feature allows to search for any task/variable/input/output within the script and focus/zoom to the found element. Search box is on the auxiliary controls menu and supports entry navigation (for cases when more than one item was found in the WDL): Workflow/Task editor is moved from the modal popup to the right floating menu PROPERTIES : See more details here . \"QUEUED\" state of the run Previously, user was not able to distinguish runs that are waiting in the queue and the runs that are being initialized (both were reporting the same state using the same icons). Now, a more clear run's state is provided - \"QUEUED\" state is introduced: During this phase of the lifecycle - a job is waiting in the queue for the available compute node. Typically this shall last for a couple of second and proceed to the initialization phase. But if this state lasts for a long time - it may mean that a cluster capacity is reached (limited by the administrator). This feature allows users to make a decision - whether to wait for run in a queue or stop it and resubmit. See more details here . Help tooltips for the run state icons With the runs' QUEUED state introduction - we now have a good number of possible job phases. To make the phases meaning more clear - tooltips are provided when hovering a run state icon within all relevant forms, i.e.: Dashboard , Runs ( Active Runs / History /etc.), Run Log . Tooltips contain a state name in bold (e.g. Queued ) and a short description of the state and info on the next stage: See more details - Active runs states , Completed runs states and Home page . VM monitor service For various reasons cloud VM instances may \"hang\" and become invisible to Cloud Pipeline services. E.g. VM was created but some error occurred during joining k8s cluster or a communication to the Cloud Providers API is interrupted. In this case Autoscaler service will not be able to find such instance and it won't be shut down. This problem may lead to unmonitored useless resource consumption and billing. To address this issue - a separate VM-Monitor service was implemented: Tracks all VM instances in the registered Cloud Regions Determines whether instances is in \"hang\" state Notifies a configurable set of users about a possible problem Notification recipients (administrators) may check the actual state of VM in Cloud Provider console and shut down VM manually. Additionally, VM-Monitor : Checks states of Cloud Pipeline's services (Kubernetes deployments). If any service changes it's state (i.e. goes down or up) - administrators will get the corresponding notification. List of such Kubernetes deployments to check includes all Cloud Pipeline's services by default, but also could be configed manually Checks all the PKI assets, available for the Platform, for the expiration - traverses over the list of directories that should contains certificate files, searches that certificates and verifies their expiration date. If some certificate expires less than in a certain amount of days - administrators also will get the corresponding notification. List of certificate directories to scan, certificate's mask and amount of days before expiration after which the notification will be sent are configurable Web GUI caching Previously, Cloud Pipeline Web GUI was not using HTTP caching to optimize the page load time. Each time application was loaded - ~2Mb of the app bundle was downloaded. This caused \"non-optimal\" experience for the end-users. Now the application bundle is split into chunks, which are identified by the content-hash in names: If nothing is changed - no data will be downloaded If some part of the app is changed - only certain chunks will be downloaded, not the whole bundle Administrator may control cache period using the static.resources.cache.sec.period parameter in the application.properties of the Core API service. Installation via pipectl Previous versions of the Cloud Pipeline did not offer any automated approach for deploying its components/services. All the deployment tasks were handed manually or by custom scripts. To simplify the deployment procedure and improve stability of the deployment - pipectl utility was introduced. pipectl offers an automated approach to deploy and configure the Cloud Pipeline platform, as well as publish some demo pipelines and docker images for NGS/MD/MnS tasks. Brief description and example installation commands are available in the pipectl's home directory . More sophisticated documentation on the installation procedure and resulting deployment architecture will be created further. Add more logging to troubleshoot unexpected pods failures When a Cloud Pipeline is being for a long time (e.g. years), it is common to observe rare \"strange\" problems with the jobs execution. I.e. the following behavior was observed couple of times over the last year: Scenario 1 Run is launched and initialized fine During processing execution - run fails Console logs print nothing, compute node is fine and is attached to the cluster Scenario 2 Run is launched, compute node is up Run fails during initialization Console logs print the similar error message: failed to open log file /var/log/pods/**.log : open /var/log/pods/**.log: no such file or directory Both scenarios are flaky and almost impossible to reproduce. To provide more insights into the situation - an extended node-level logging was implemented: kubelet logs (from all compute nodes) are now written to the files (via DaemonSet ) Log files are streamed to the storage, identified by storage.system.storage.name preference Administrators can find the corresponding node logs (e.g. by the hostname or ip that are attached to the run information) in that storage under logs/node/{hostname} See an example here . Displaying information on the nested runs within a parent log form Previously, if user launched a run, that has a number of children (e.g. a cluster run or any other case with the parent-id specified), he could view the children list only from \"Runs\" page. In v.0.15 a convenient opportunity to view the list of children directly in the parent's run logs form is implemented: For each child-run in the list the following information is displayed: State icons with help tooltips when hovering over them Pipeline name and version/docker image and version Run time duration Similar as a parent-run state, states for nested runs are automatically updated without page refreshing. So, you can watch for them in real time. If you click any of the children-runs, you will navigate to its log page. That feature is implemented for the comleted runs too: More information about nested runs displaying see here and here . Environment Modules support for the Cloud Pipeline runs The Environment Modules package provides for the dynamic modification of a user's environment via modulefiles . In current version, an ability to configure the Modules support for the compute jobs is introduced, if this is required by any use case. For using facilities of the Environment Modules package, a new system parameter was added to the Cloud Pipeline: CP_CAP_MODULES (boolean) - enables installation and using the Modules for the current run (for all supported Linux distributions) If CP_CAP_MODULES system parameter is set - the Modules will be installed and made available. While installing, Modules will be configured to the source modulefiles path from the CP_CAP_MODULES_FILES_DIR launch environment variable (value of this variable could be set only by admins via system-level settings). If that variable is not set - default modulefiles location will be used. See an example here . Sharing SSH access to running instances with other user(s)/group(s) As was introduced in Release Notes v.0.13 , for certain use cases it is beneficial to be able to share applications with other users/groups. v0.15 introduces a feature that allows to share the SSH-session of any active run (regardless of whether the job type is interactive or not): The user can share an interactive run with others: \"Share with: ...\" parameter, within a run log form, can be used for this Specific users or whole groups can be set for sharing Once this is set - other users will be able to access run's endpoints Also you can share SSH access to the running instance via setting \" Enable SSH connection \" checkbox Also, the user can share a non-interactive run: \"Share with: ...\" parameter, within a run log form, can be used for this Specific users or whole groups can be set for sharing Once this is set - specified users/groups will be able to access the running instance via the SSH SERVICES widget within a Home dashboard page lists such \"shared\" services. It displays a \"catalog\" of services, that can be accessed by a current user, without running own jobs. To open shared instance application user should click the service name. To get SSH-access to the shared instance (regardless of whether the job type is interactive or not), the user should hover over the service \"card\" and click the SSH hyperlink For more information about runs sharing see 11.3. Sharing with other users or groups of users . Allow to limit the number of concurrent SSH sessions Previously, some users could try to start a real big number of Web SSH sessions. If 1000+ SSH sessions are established via EDGE service, the performance will degrade. It is not common, but it could be critical as it affects all the users of the platform deploment. To avoid such cases, in v0.15 the pipectl parameter CP_EDGE_MAX_SSH_CONNECTIONS (with default value 25 ) for the EDGE server is introduced, that allows to control a number of simultaneous SSH connections to a single job. Now, if this max number will be reached, the next attemp to open another one Web SSH session to the same job will return a notification to the user and a new session will not be opened until the any one previous is closed: Verification of docker/storage permissions when launching a run Users are allowed to launch pipeline, detached configuration or tool if they have a corresponding permission for that executable. But in some cases this verification is not enough, e.g. when user has no read permission for input parameter - in this case, run execution could cause an error. In v0.15 additional verification implemented that checks if: execution is allowed for specified docker image; read operations are allowed for input and common path parameters; write operations are allowed for output path parameters. If there are such permission issues, run won't be launched and special warning notifications will be shown to a user, e.g.: For more details see sections 6.2. Launch a pipeline , 7.2. Launch Detached Configuration and 10.5. Launch a Tool . Ability to override the queue / PE configuration in the GE configuration Previously, if the Grid Engine was enabled, the following was configured: a single queue with all the hosts was creating, named \" main.q \" a single PE (Parallel Environment) was creating, named \" local \" In v0.15 , the overriding of the names of the queue / PE is implemented to be compatible with any existing scripts, that rely on a specific GE configuration (e.g. hardcoded). You can do it using two new System Parameters at the Launch or the Configuration forms: CP_CAP_SGE_QUEUE_NAME (string) - allows to override the GE's queue name (default: \" main.q \") CP_CAP_SGE_PE_NAME (string) - allows to override the GE's PE name (default: \" local \") More information how to use System Parameters when a job is launched see here . Estimation run's disk size according to the input/common parameters Previously, if a job was run with the disk size, which was not enough to handle the job's inputs - it failed (e.g. 10Gb disk was set for a run, which processed data using STAR aligner, where the genome index file is 20Gb). In v0.15 , an attempt to handle some of such cases is implemented. Now, the Cloud Pipeline try to estimate the required disk size using the input/common parameters and warn the user if the requested disk is not enough. When a job is launching, the system try to get the size of all input/common parameters. The time of the size getting for all files is limited, as this may take too much for lots of small files. Limit for this time is set by the storage.listing.time.limit system preference (in milliseconds). Default: 3 sec (3000 milliseconds). If computation doesn't end in this timeout, accumulated size will return as is. If the resulting size of all input/common parameters is greater than requested disk size (considering cluster configuration) - the user will be warned: User can set suggested disk size or launch a job at user's own risk with the requested size. If calculated suggested disk size exceeds 16Tb (hard limit) a different warning message will be shown: The requested disk size for this run is N Gb, but the data that is going to be processed exceeds 16 Tb (which is a hard limit). Please use the cluster run configuration to scale the disks horizontally or reduce the input data volume. Do you want to use the maximum disk size 16 Tb anyway? Disabling of the Global Search form if a corresponding service is not installed Version 0.14 introduced the Global Search feature over all Cloud Pipeline objects. In current version, a small enhancement for the Global Search is implemented. Now, if the search.elastic.host system preference is not set by admin - other users will not be able to try search performing: the \"Search\" button will be hidden from the left menu keyboard search shortcut will be disabled Disabling of the FS mounts creation if no FS mount points are registered In the Cloud Pipeline , along with the regular data storages user can also create FS mounts - data storages based on the network file system: For the correct FS mount creation, at least one mount point shall be registered in the Cloud Pipeline Preferences. Now, if no FS mount points are registered for any Cloud Region in the System Preferences - user can not create a new FS mount, the corresponding button becomes invisible: Displaying resource limit errors during run resuming User may hit a situation of resource limits while trying to resume previously paused run. E.g. instance type was available when run was initially launched, but at the moment of resume operation provider has no sufficient capacity for this type. Previously, in this case run could be failed with an error of insufficient resources. In v0.15 the following approach is implemented for such cases: resuming run doesn't fail if resource limits are hit. That run returns to the Paused state log message that contains a reason for resume failure and returning back to the Paused state is being added to the ResumeRun task user is notified about such event. The corresponding warning messages are displayed: at the Run logs page at the ACTIVE RUNS page (hint message while hovering the RESUME button) at the ACTIVE RUNS panel of the Dashboard (hint message while hovering the RESUME button) Object storage creation in despite of that the CORS/Policies could not be applied Previously, if the Cloud service account/role had permissions to create object storages, but lacked permissions to apply CORS or other policies - object storage was created, but the Cloud Pipeline API threw an exception and storage was not being registered. This led to the creation of a \"zombie\" storage, which was not available via GUI, but existed in the Cloud. Currently, the Cloud Pipeline API doesn't fail such requests and storage is being registered normally. But the corresponding warning will be displayed to the user like this: The storage {storage_name} was created, but certain policies were not applied. This can be caused by insufficient permissions. Track the confirmation of the \"Blocking\" notifications System events allow to create popup notifications for users. One of the notification types - the \"Blocking\" notification. Such event emerges in the middle of the window and requires confirmation from the user to disappear for proceeding with the GUI operations. In certain cases (e.g. for some important messages), it is handy to be able to check which users confirmed the notification. For that, in the current version the ability to view, which \"blocking\" notifications confirmed by specific user, was implemented for admins. Information about confirmed notifications can be viewed at the \" Attributes \" section of the specific user's profile page: Confirmed notifications are displayed as user attribute with the KEY confirmed_notifications (that name could be changed via the system-level preference system.events.confirmation.metadata.key ) and the VALUE link that shows summary count of confirmed notifications for the user. Click the VALUE link with the notification count to open the detailed table with confirmed notifications: For more details see \"blocking\" notifications track . pipe CLI warnings on the JWT expiration By default, when pipe CLI is being configured JWT token is given for one month, if user didn't select another expiration date. In v.0.15 extra pipe CLI warnings are introduced to provide users an information on the JWT token expiration: When pipe configure command is executed - the warning about the expiration date of the provided token is printed, if it is less than 7 days left: When --version option is specified - pipe prints dates of issue and expiration for the currently used token: When any other command is running - the warning about the expiration date of the provided JWT token is printed, if it is less than 7 days left: See more information about pipe CLI installation here . pipe configuration for using NTLM Authentication Proxy For some special customer needs, pipe configuration for using NTLM Authentication Proxy, when running in Linux, could be required. For that, several new options were added to pipe configure command: -nt or --proxy-ntlm - flag that enable NTLM proxy support -nu or --proxy-ntlm-user - username for NTLM proxy authorization -np or --proxy-ntlm-pass - password for NTLM proxy authorization -nd or --proxy-ntlm-domain - domain for NTLM proxy authorization If --proxy-ntlm is set, pipe will try to get the proxy value from the environment variables or --proxy option ( --proxy option has a higher priority). If --proxy-ntlm-user and --proxy-ntlm-pass options are not set - user will be prompted for username/password in an interactive manner. Valid configuration examples: User will be prompted for NTLM Proxy Username, Password and Domain: pipe configure --proxy-ntlm ... Username for the proxy NTLM authentication: user1 Domain of the user1 user: '' Password of the user1 user: Use http://myproxy:3128 as the \"original\" proxy address. User will not be prompted for NTLM credentials: pipe configure --proxy-ntlm --proxy-ntlm-user $MY_NAME --proxy-ntlm-pass $MY_PASS --proxy http://myproxy:3128 See more information about pipe CLI installation and configure here . Execution of files uploading via pipe without failures in case of lacks read permissions Previously, pipe storage cp / mv commands could fail if a \"local\" source file/dir lacked read permissions. For example, when user tried to upload to the \"remote\" storage several files and when the pipe process had reached one of files that was not readable for the pipe process, then the whole command was being failed, remaining files did not upload. In current version, the pipe process checks read permission for the \"local\" source (directories and files) and skip those that are not readable: Run a single command or an interactive session over the SSH protocol via pipe For the certain purposes, it could be conveniently to start an interactive session over the SSH protocol for the job run via the pipe CLI. For such cases, in v0.15 the pipe ssh command was implemented. It allows you, if you are the ADMIN or the run OWNER , to perform a single command or launch an interactive session for the specified job run. Launching of an interactive session: This session is similar to the terminal access that user can get via the GUI. Performing the same single command without launching an interactive session: Perform objects restore in a batch mode via pipe Users can restore files that were removed from the data storages with enabled versioning. For these purposes, the Cloud Pipeline's CLI has the restore command which is capable of restoring a single object at a time. In v0.15 the ability to recursively restore the whole folder, deleted from the storage, was implemented. Now, if the source path is a directory, the pipe storage restore command gets the top-level deleted files from the source directory and restore them to the latest version. Also, to the restore command some options were added: -r or --recursive - flag allows to restore the whole directory hierarchy -i or --include [TEXT] - flag allows to restore only files which names match the [TEXT] pattern and skip all others -e or --exclude [TEXT] - flag allows to skip restoring of files which names match the [TEXT] pattern and restore all others Note : this feature is yet supported for AWS only. For more details about file restoring via the pipe see here . Mounting data storages to Linux and Mac workstations Previously, when users had to copy/move datasets to/from Cloud data storages via CLI, they could use only special pipe storage commands. That was not always comfortable or could lead to some functionality restrictions. In v0.15 the ability to mount Cloud data storages (both - File Storages and Object Storages) to Linux and Mac workstations (requires FUSE installed) was added. For the mounted storages, regular listing/read/write commands are supported, users can manage files/folders as with any general hard drive. Note : this feature is yet supported for AWS only. To mount a data storage into the local mountpoint the pipe storage mount command was implemented. It has two main options that are mutually exclusive: -f or --file specifies that all available file systems should be mounted into a mountpoint -b or --bucket [STORAGE_NAME] specifies a storage name to mount into a mountpoint Users can: leverage mount options, supported by underlying FUSE implementation, via -o option enable multithreading for simultaneously interaction of several processes with the mount point, via -t option trace all information about mount operations into a log-file, via -l option To unmount a mountpoint the pipe storage umount command was implemented. For more details about mounting data storages via the pipe see here . Allow to run pipe commands on behalf of the other user In the current version, the ability to run pipe commands on behalf of the other user was implemented. It could be convenient when administrators need to perform some operations on behalf of the other user (e.g. check permissions/act as a service account/etc.). This feature is implemented via the common option that was added to all pipe commands: --user|-u USER_ID (where USER_ID is the name of the user account). Note : the option isn't available for the following pipe commands: configure , --version , --help . If this option is specified - operation (command execution) will be performed using the corresponding user account, e.g.: In the example above, active runs were outputted from the admin account (firstly) and then on behalf of the user without ROLE_ADMIN role. Additionally, a new command pipe token USER_ID was implemented. It prints the JWT token for a specified user. This command also can be used with non-required option -d ( --duration ), that specified the number of days the token will be valid. If it's not set - the default value will be used, same as in the GUI. Example of using: Then, the generated JWT token could be used manually with the pipe configure command - to configure pipe CLI on behalf of the desired user. Note : both - the command ( pipe token ) and the option ( --user ) - are available only for admins. For more details see here . Ability to restrict the visibility of the jobs Previously, Cloud Pipeline inherited the pipeline jobs' permissions from the parent pipeline object. So, if two users had permissions on the same pipeline, then when the first user had launched that pipeline - the second user also could view (not manage) launched run in the Active Runs tab. Now admin can restrict the visibility of the jobs for non-owner users. The setting of such visibility can get one of the following values: Inherit - the behavior is the same as described above (for the previous approach), when the runs visibility is controlled by the pipeline permissions. It is set as a default for the Cloud Pipeline environment Only owner - when only the person who launch a run can see it Jobs visibility could be set by different ways on several forms: within User management tab in the system-level settings admin can specify runs visibility for a user/group/role: within Launch section of Preferences tab in the system-level settings admin can specify runs visibility for a whole platform as global defaults - by the setting launch.run.visibility : Next hierarchy is set for applying of specified jobs visibility: User level - highest priority (specified for a user) Group level (specified for a group/role) Platform level launch.run.visibility (specified as global defaults via system-level settings) Note : admins can see all runs despite of settings Ability to perform scheduled runs from detached configurations Previously, Cloud Pipeline allowed starting compute jobs only manually (API/GUI/CLI). But in certain use cases, it is beneficial to launch runs on a scheduled basis. In v0.15 the ability to configure a schedule for detached configuration was implemented: User is able to set a schedule for launch a run from the detached configuration: Schedule is defined as a list of rules - user is able to specify any number of them: For each rule in the list user is able to set the recurrence: If any schedule rule is configured for the detached configuration - a corresponding job (plain container or a pipeline) will be started accordingly in the scheduled day and time. See more details here . The ability to use custom domain names as a \"friendly URL\" for the interactive services In v0.13 the ability to set a \"friendly URL\" for the interactive services endpoint was implemented. It allows to configure the view of the interactive service endpoint: Default view: https:// host /pipeline- run-id - port \"Friendly\" view: https:// host / friendly-url In the current version this feature is expanded: users allow to specify a custom host. So the endpoint url now can look like: https:// custom-host or https:// custom-host / friendly-url . Note : custom host should exist, be valid and configured. The custom host is being specified into the same field as a \"friendly URL\" previously, e.g.: Final URL for the service endpoint will be generated using the specified host and friendly URL: For more details see here . Displaying of the additional support icon/info In certain cases, users shall have a quick access to the help/support information (e.g. links to docs/faq/support request/etc.) In the current version, the ability to display additional \"support\" icon with the corresponding info in the bottom of the main menu was implemented: The displaying of this icon and the info content can be configured by admins via the system-level preference ui.support.template : this preference is empty by default - in this case the support icon is invisible if this preference contains any text ( Markdown -formatted): the support icon is visible specified text is displayed in the support icon tooltip (support info) For more details see UI system settings . Pass proxy settings to the DIND containers Previously, DIND containers configuration included only registry credentials and a couple of driver settings. In certain environments, it is not possible to access external networks (e.g. for the packages installation) without the proxy settings. So the users had to pass this manually every time when using the docker run command. In the current version, a new system preference launch.dind.container.vars is introduced. It allows to specify all the additions variables, which will be passed to the DIND containers (if they are set for the host environment). By default, the following variables are set for the launch.dind.container.vars preference (and so will be passed to DIND container): http_proxy , https_proxy , no_proxy , API , API_TOKEN . Variables are being specified as a comma-separated list. Example of using: At the same time, a new system parameter (per run) was added - CP_CAP_DIND_CONTAINER_NO_VARS , which disables described behavior. You can set it before any run if you don't want to pass any additional variations to the DIND container. Interactive endpoints can be (optionally) available to the anonymous users Cloud Pipeline allows sharing the interactive and SSH endpoints with the other users/groups. Previously, this necessary required the end-user to be registered in the Cloud Pipeline users database. For certain use-cases, it is required to allow such type of access for any user, who has successfully passed the IdP authentication but is not registered in the Cloud Pipeline and also such users shall not be automatically registered at all and remain Anonymous . In the current version, such ability is implemented. It's enabled by the following application properties: saml.user.auto.create=EXPLICIT_GROUP saml.user.allow.anonymous=true After that, to share any interactive run with the Anonymous - it's simple enough to share endpoints with the following user group - ROLE_ANONYMOUS_USER : At the Run logs page: The user should select the ROLE_ANONYMOUS_USER role to share: Sharing with the Anonymous will be displayed at the Run logs page: That's all. Now, the endpoint-link of the run could be sent to the Anonymous user. If that Anonymous user passes SAML authentication, he will get access to the endpoint. Attempts to open any other Platform pages will fail. For more details see here . Notable Bug fixes Incorrect behavior of the global search filter #221 When user was searching for an entry, that may belong to different classes (e.g. issues and folders ) - user was not able to filter the results by the class. \"COMMITTING...\" status hangs #152 In certain cases, while committing pipeline with the stop flag enabled - the run's status hangs in Committing... state. Run state does not change even after the commit operation succeeds and a job is stopped. Instances of Metadata entity aren't correctly sorted #150 Metadata entities (i.e. project-related metadata) sorting was faulty: Sort direction indicator (Web GUI) was displaying an inverted direction Entities were not sorted correctly Tool group cannot be deleted until all child tools are removed #144 If there is a tool group in the registry, which is not empty (i.e. contains 1+ tools) - an attempt to delete it throws SQL error. It works fine if the child tools are dropped beforehand. Now, it is possible to delete such a group if a force flag is set in the confirmation dialog. Missing region while estimating a run price #93 On the launch page, while calculating a price of the run, Cloud Provider's region was ignored. This way a calculation used a price of the specified instance type in any of the available regions. In practice, requested price may vary from region to region. Cannot specify region when an existing object storage is added #45 Web GUI interface was not providing an option to select a region when adding an existing object storage. And it was impossible to add a bucket from the non-default region. ACL control for PIPELINE_USER and ROLE entities for metadata API #265 All authorized users were permitted to browse the metadata of users and roles entities. But those entries may contain a sensitive data, that shall not be shared across users. Now, a general user may list only personal user-level metadata. Administrators may list both users and roles metadata across all entries. Getting logs from Kubernetes may cause OutOfMemory error #468 For some workloads, container logs may become very large: up to several gigabytes. When we tried to fetch such logs it is likely to cause OutOfMemory error, since Kubernetes library tries to load it into a single String object. In current version, a new system preference was introduced: system.logs.line.limit . That preference sets allowable log size in lines. If actual pod logs exceeds the specified limit only log tail lines will be loaded, the rest will be truncated. AWS: Incorrect nodeup handling of spot request status #556 Previously, in a situation when an AWS spot instance created after some timeout - spot status wasn't updated correctly in the handling of spot request status . It might cause errors while getting spot instance info. Not handling clusters in autopause daemon #557 Previously, if cluster run was launched with enabled \"Auto pause\" option, parent-run or its child-runs could be paused (when autopause conditions were satisfied, of course). It was incorrect behavior because in that case, user couldn't resume such paused runs and go on his work (only \"Terminate\" buttons were available). In current version, autopause daemon doesn't handle any clusters (\"Static\" or \"Autoscaled\"). Also now, if the cluster is configured - Auto pause checkbox doesn't display in the Launch Form for the On-Demand node types. Incorrect pipe CLI version displaying #561 Previously, pipe CLI version displayed incorrectly for the pipe CLI installations performed via hints from the Cloud Pipeline System Settings menu. JWT token shall be updated for the jobs being resumed #579 In cases when users launched on-demand jobs, paused them and then, after a long time period (2+ months), tried to resume such jobs - expired JWT tokens were set for them that led to different problems when any of the initialization routines tried to communicate with the API. Now, the JWT token and other variables as well are being updated when a job is being resumed. Trying to rename file in the data storage, while the \"Attributes\" panel is opened, throws an error #520 Renaming file in the datastorage with opened \"Attributes\" panel caused an unexpected error. pipe : incorrect behavior of the -nc option for the run command #609 Previously, trying to launch a pipeline via the pipe run command with the single -nc option threw an error. Cluster run cannot be launched with a Pretty URL #620 Previously, if user tried to launch any interactive tool with Pretty URL and configured cluster - an error appeared URL {Pretty URL} is already used for run {Run ID} . Now, pretty URL could be set only for the parent runs, for the child runs regular URLs are set. Cloning of large repositories might fail #626 When large repository ( 1Gb) was cloned (e.g. when a pipeline was being run) - git clone could fail with the OOM error happened at the GitLab server if it is not powerful enough. OOM was produced by the git pack-objects process, which tries to pack all the data in-memory. Now, git pack-objects memory usage is limited to avoid errors in cases described above. System events HTML overflow #630 If admin set a quite long text (without separators) into the message body of the system event notifications - the resulting notification text \"overflowed\" the browser window. Now, text wrapping is considered for such cases. Also, support of Markdown was added for the system notification messages: AWS: Pipeline run InitializeNode task fails #635 Previously, if AWS spot instance could not be created after the specific number of attempts during the run initialization - such run was failed with the error, e.g.: Exceeded retry count (100) for spot instance. Spot instance request status code: capacity-not-available . Now, in these cases, if spot instance isn't created after specific attempts number - the price type is switched to on-demand and run initialization continues. git-sync shall not fail the whole object synchronization if a single entry errors #648 , #691 When the git-sync script processed a repository and failed to sync permissions of a specific user (e.g. git exception was thrown) - the subsequent users were not being processed for that repository. Now, the repository sync routine does not fail if a single user cannot be synced. Also, the issues with the synchronization of users with duplicate email addresses and users with empty email were resolved. endDate isn't set when node of a paused run was terminated #743 Previously, when user terminated the node of a paused run - endDate for that run wasn't being set. This was leading to wrong record of running time for such run. AWS: Nodeup retry process may stuck when first attempt to create a spot instance failed #744 Previously, if first nodeup attempt failed due to unavailablity to connect on 8888 port (in expected amounts of attempts) after getting instance running state, the second nodeup attempt might stuck because it waited for the same instance (associated with existed SpotRequest for the first attempt) to be up. But it couldn't happen - this instance was already in terminating state after the first attempt. Now, checks that instance associated with SpotRequest (created for the first attempt) is in appropriate status, if not - a new SpotRequest is being created and the nodeup process is being started from scratch. Resume job timeout throws strange error message #832 Previously, the non-informative error message was shown if the paused run could't be resumed in a reasonable amount of time - the count of attempts to resume was displaying incorrectly. GE autoscaler doesn't remove dead additional workers from cluster #946 Previously, the Grid Engine Autoscaler didn't properly handle dead workers downscaling. For example, if some spot worker instance was preempted during the run then the autoscaler could not remove such worker from GE. Moreover, such cluster was blocked from accepting new jobs. Broken layouts #553 , #619 , #643 , #644 , #915 Previously, pipeline versions page had broken layout if there were pipeline versions with long description. Global search page was not rendered correctly when the search results table had too many records. When a list of items in the docker groups selection dialog was long - it was almost impossible to use a search feature, as the list hid immediately. Some of the other page layouts also were broken.","title":"v.0.15"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#cloud-pipeline-v015-release-notes","text":"Microsoft Azure Support SAML claims based access Notifications on the RAM/Disk pressure Limit mounted storages Personal SSH keys configuration Allow to set the Grid Engine capability for the \"fixed\" cluster Enable Apache Spark for the Cloud Pipeline's clusters Consider Cloud Providers' resource limitations when scheduling a job Allow to terminate paused runs Pre/Post-commit hooks implementation Restricting manual installation of the nvidia tools Setup swap files for the Cloud VMs Run's system paths shall be available for the general user account Renewed WDL visualization \"QUEUED\" state of the run Help tooltips for the run state icons VM monitor service Web GUI caching Installation via pipectl Add more logging to troubleshoot unexpected pods failures Displaying information on the nested runs Environment Modules support Sharing SSH access to running instances with other user(s)/group(s) Allow to limit the number of concurrent SSH sessions Verification of docker/storage permissions when launching a run Ability to override the queue/PE configuration in the GE configuration Estimation run's disk size according to the input/common parameters Disabling of the Global Search form if a corresponding service is not installed Disabling of the FS mounts creation if no FS mount points are registered Displaying resource limit errors during run resuming Object storage creation in despite of that the CORS/Policies could not be applied Track the confirmation of the \"Blocking\" notifications pipe CLI warnings on the JWT expiration pipe configuration for using NTLM Authentication Proxy Files uploading via pipe in case of restrictions Run a single command or an interactive session over the SSH protocol via pipe Perform objects restore in a batch mode via pipe Mounting data storages to Linux and Mac workstations Allow to run pipe commands on behalf of the other user Ability to restrict the visibility of the jobs Ability to perform scheduled runs from detached configurations Using custom domain names as a \"friendly URL\" for the interactive services Displaying of the additional support icon/info Pass proxy settings to the DIND containers Interactive endpoints can be (optionally) available to the anonymous users Notable Bug fixes Incorrect behavior of the global search filter \"COMMITING...\" status hangs Instances of Metadata entity aren't correctly sorted Tool group cannot be deleted until all child tools are removed Missing region while estimating a run price Cannot specify region when an existing object storage is added ACL control for PIPELINE_USER and ROLE entities for metadata API Getting logs from Kubernetes may cause OutOfMemory error AWS: Incorrect nodeup handling of spot request status Not handling clusters in autopause daemon Incorrect pipe CLI version displaying JWT token shall be updated for the jobs being resumed Trying to rename file in the data storage, while the \"Attributes\" panel is opened, throws an error pipe : incorrect behavior of the -nc option for the run command Cluster run cannot be launched with a Pretty URL Cloning of large repositories might fail System events HTML overflow AWS: Pipeline run InitializeNode task fails git-sync shall not fail the whole object synchronization if a single entry errors endDate isn't set when node of a paused run was terminated AWS: Nodeup retry process may stuck when first attempt to create a spot instance failed Resume job timeout throws strange error message GE autoscaler doesn't remove dead additional workers from cluster Broken layouts","title":"Cloud Pipeline v.0.15 - Release notes"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#microsoft-azure-support","text":"One of the major v0.15 features is a support for the Microsoft Azure Cloud . All the features, that were previously used for AWS , are now available in all the same manner, from all the same GUI/CLI, for Azure . Another cool thing, is that now it is possible to have a single installation of the Cloud Pipeline , which will manage both Cloud Providers ( AWS and Azure ). This provides a flexibility to launch jobs in the locations, closer to the data, with cheaper prices or better compute hardware for a specific task.","title":"Microsoft Azure Support"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#saml-claims-based-access","text":"Previously, there were two options for users to be generally authenticated in Cloud Pipeline Platform, i.e. to pass SAML validation: Auto-register - any valid SAML authentication automatically registers user (if he isn't registered yet) and grant ROLE_USER access Explicit-register - after a valid SAML authentication, it is checked whether this user is already registered in the Cloud Pipeline catalog, and if no - request denies, authentication fails To automate things a bit more, in v0.15 additional way to grant first-time access to the users was implemented: Explicit-group register. If such registration type is set - once a valid SAML authentication is received, it is checked, whether SAML response contains any of the domain groups, that are already granted any access to the Cloud Pipeline objects (registered in Cloud Pipeline catalog). If so - proceeds as with Auto-register - user with all his domain groups and granted ROLE_USER access is being registered. If user's SAML domain groups aren't intersected with pre-registered groups the authentication fails. This Platform's behavior is set via application property saml.user.auto.create that could accept one of the corresponding values: AUTO , EXPLICIT , EXPLICIT_GROUP .","title":"SAML claims based access"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#notifications-on-the-ramdisk-pressure","text":"When a compute-intensive job is run - compute node may start starving for the resources. CPU high load is typically a normal situation - it could result just to the SSH/metrics slowdown. But running low on memory and disk could crash the node, in such cases autoscaler will eventually terminate the cloud instance. In v0.15 version, the Cloud Pipeline platform could notify user on the fact of Memory/Disk high load. When memory or disk consuming will be higher than a threshold value for a specified period of time (in average) - a notification will be sent (and resent after a delay, if the problem is still in place). Such notifications could be configured at HIGH_CONSUMED_RESOURCES section of the Email notifications : The following items at System section of the Preferences define behavior of such notifications: system.memory.consume.threshold - memory threshold (in %) above which the notification would be sent system.disk.consume.threshold - disk threshold (in %) above which the notification would be sent system.monitoring.time.range - time delay (in sec) after which the notification would be sent again, if the problem is still in place. See more information about Email notifications and System Preferences .","title":"Notifications on the RAM/Disk pressure"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#limit-mounted-storages","text":"Previously, all available storages were mounted to the container during the run initialization. User could have access to them via /cloud-data or ~/cloud-data folder using the interactive sessions (SSH/Web endpoints/Desktop) or pipeline runs. For certain reasons (e.g. takes too much time to mount all or a run is going to be shared with others), user may want to limit the number of data storages being mounted to a specific job run. Now, user can configure the list of the storages that will be mounted. This can be accomplished using the Limit mount field of the Launch form: By default, All available storages are mounted (i.e. the ones, that user has ro or rw permissions) To change the default behavior - click the drop-down list next to \"Limit mounts\" label: Select storages that shall be mounted: Review that only a limited number of data storages is mounted: Mounted storage is available for the interactive/batch jobs using the path /cloud-data/{storage_name} : See an example here .","title":"Limit mounted storages"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#personal-ssh-keys-configuration","text":"Previously, there were two options to communicate with the embedded gitlab repositories, that host pipelines code: From the local (non-cloud) machine: use the https protocol and the repository URI, provided by the GUI (i.e. GIT REPOSITORY button in the pipeline view) From the cloud compute node: git command line interface, that is preconfigured to authenticate using https protocol and the auto-generated credentials Both options consider that https is used. It worked fine for 99% of the use cases. But for some of the applications - ssh protocol is required as this is the only way to achieve a passwordless authentication against the gitlab instance To address this issue, SSH keys management was introduced: Users' ssh keys are generated by the Git Sync service SSH key-pair is created and assigned to the Cloud Pipeline user and a public key is registered in the GitLab Those SSH keys are now also configured for all the runs, launched by the user. So it is possible to perform a passwordless authentication, when working with the gitlab or other services, that will be implemented in the near future HTTPS/SSH selector is added to the GIT REPOSITORY popup of the Cloud Pipeline Web GUI: Default selection is HTTP, which displays the same URI as previously (repository), but user is able to switch it to the SSH and get the reformatted address: For more details see here .","title":"Personal SSH keys configuration"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#allow-to-set-the-grid-engine-capability-for-the-fixed-cluster","text":"Version 0.14 introduced an ability to launch autoscaled clusters. Besides the autoscaling itself - such cluster were configured to use GridEngine server by default, which is pretty handy. On the other hand - fixed cluster (i.e. those which contain a predefined number of compute nodes), required user to set the CP_CAP_SGE explicitly. Which is no a big deal, but may be tedious. In v0.15 Grid Engine can be configured within the Cluster (fixed size cluster) tab. This is accomplished by using the Enable GridEngine checkbox. By default, this checkbox is unticked. If the user sets this ON - CP_CAP_SGE parameter is added automatically. Also a number of help icons is added to the Cluster configuration dialog to clarify the controls purpose: Popup header (E.g. next to the tabs line) - displays information on different cluster modes (Cluster) Enable GridEngine checkbox - displays information on the GridEngine usage (Cluster) Enable Apache Spark checkbox - displays information on the Apache Spark usage (see below ) (Auto-scaled cluster) Auto-scaled up - displays information on the autoscaling logic (Auto-scaled cluster) Default child nodes - displays information on the initial node pool size See more information about cluster launch here .","title":"Allow to set the Grid Engine capability for the \"fixed\" cluster"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#enable-apache-spark-for-the-cloud-pipelines-clusters","text":"Another one feature for the Cloud Pipeline's clusters was implemented in v0.15 . Now, Apache Spark with the access to File/Object Storages from the Spark Applications can be configured within the Cluster tab. It is available only for the fixed size clusters. To enable this feature - tick the Enable Apache Spark checkbox and set the child nodes count at cluster settings. By default, this checkbox is unticked. Also users can manually enable Spark functionality by the CP_CAP_SPARK system parameter: This feature, for example, allows you to run Apache Spark cluster with RStudio where you may code in R using sparklyr to run the workload over the cluster: Open the RStudio tool Select the node type, set the Apache Spark cluster as shown above, and launch the tool: Open main Dashboard and wait until the OPEN hyperlink for the launched tool will appear. Hover over it: Two endpoints will appear: RStudio (as the main endpoint it is in bold) - it exposes RStudio's Web IDE SparkUI - it exposes Web GUI of the Spark. It allows to monitor Spark master/workers/application via the web-browser. Details are available in the Spark UI manual Click the RStudio endpoint: Here you can start create scripts in R using the pre-installed sparklyr package to distribute the workload over the cluster. Click the SparkUI endpoint: Here you can view the details of the jobs being executed in Spark, how the memory is used and get other useful information. For more information about using Apache Spark via the Cloud Pipeline see here .","title":"Enable Apache Spark for the Cloud Pipeline's clusters"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#consider-cloud-providers-resource-limitations-when-scheduling-a-job","text":"Cloud Pipeline supports queueing of the jobs, that cannot be scheduled to the existing or new compute nodes immediately. Queueing occurs if: cluster.max.size limit is reached (configurable via Preferences ) Cloud Provider limitations are reached (e.g. AWS EC2 Limits ) If ( 1 ) happens - job will sit in the QUEUED state, until a spare will be available or stopped manually. This is the correct behavior. But if ( 2 ) occurs - an error will be raised by the Cloud Provider, Cloud Pipeline treats this as an issue with the node creation. Autoscaler will then resubmit the node creation task for cluster.nodeup.retry.count times (default: 5) and then fail the job run. This behavior confuses users, as ( 2 ) shall behave almost in the same manner as ( 1 ) - job shall be kept in the queue until there will be free space for a new node. In v0.15 ( 2 ) scenario works as described: If a certain limit is reached (e.g. number of m4.large instances exceeds the configured limit) - run will not fail, but will await for a spare node or limit increase A warning will be highlighted in the job initialization log:","title":"Consider Cloud Providers' resource limitations when scheduling a job"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#allow-to-terminate-paused-runs","text":"Some of the jobs, that were paused (either manually, or by the automated service), may be not needed anymore. But when a run is paused - the user cannot terminate/stop it before resuming. I.e. one have to run RESUME operation, wait for it's completion and then STOP the run. While this is the expected behavior (at least designed in this manner) - it requires some extra steps to be performed, which may look like meaningless (why one shall resume a run, that is going to be stopped?). Another issue with such a behavior is that in certain \"bad\" conditions - paused runs are not able to resume and just fail, e.g.: An underlying instance is terminated outside of the Cloud Pipeline Docker image was removed from the registry And other cases that are not yet uncovered This introduces a number of stale runs, that just sit there in the PAUSED state and nobody can remove them. To address those concerns - current version of Cloud Pipeline allows to terminate PAUSED run, without a prior RESUME . This operation can be performed by the OWNER of the run and the ADMIN users. Termination of the PAUSED run drops the underlying cloud instance and marks the run as STOPPED . From the GUI perspective - TERMINATE button is shown (instead of STOP ), when a run is in the PAUSED state: on the \"Active runs\" page on the \"Run logs\" page on the \"Dashboard\" page Clicking it - performs the run termination, as described above. See more information here .","title":"Allow to terminate paused runs"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#prepost-commit-hooks-implementation","text":"In certain use-cases, extra steps shall be executed before/after running the commit command in the container. E.g. imagine the following scenario: User launches RStudio User installs packages and commits it as a new image User launches the new image The following error will be displayed in the R Console: 16 Jan 2019 21:17:15 [rsession-GRODE01] ERROR session hadabend; LOGGED FROM: rstudio::core::Error {anonymous}::rInit(const rstudio::r::session::RInitInfo ) /home/ubuntu/rstudio/src/cpp/session/SessionMain.cpp:563 There is nothing bad about this message and states that previous RSession was terminated in a non-graceful manner. RStudio will work correctly, but it may confuse the users. While this is only one example - there are other applications, that require extra cleanup to be performed before the termination. To workaround such issues (RStudio and others) an approach of pre/post-commit hooks is implemented. That allows to perform some graceful cleanup/restore before/after performing the commit itself. Those hooks are valid only for the specific images and therefore shall be contained within those images. Cloud Pipeline itself performs the calls to the hooks if they exist. Two preferences are introduced: commit.pre.command.path : specified a path to a script within a docker image, that will be executed in a currently running container, BEFORE docker commit occurs (default: /root/pre_commit.sh ). This option is useful if any operations shall be performed with the running processes (e.g. send a signal), because in the subsequent post operation - only filesystem operations will be available. Note that any changes done at this stage will affect the running container. commit.post.command.path : specified a path to a script within a docker image, that will be executed in a committed image, AFTER docker commit occurs (default: /root/post_commit.sh ). This hook can be used to perform any filesystem cleanup or other operations, that shall not affect the currently running processes. If a corresponding pre/post script is not found in the docker image - it will not be executed. For more details see here .","title":"Pre/Post-commit hooks implementation"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#restricting-manual-installation-of-the-nvidia-tools","text":"It was uncovered that some of the GPU-enabled runs are not able to initialize due to an issue describe at NVIDIA/nvidia-docker#825 . To limit a possibility of producing such docker images (which will not be able to start using GPU instance types) - a set of restrictions/notifications was implemented: A notification is now displayed (in the Web GUI), that warns a user about the risks of installing any of the nvidia packages manually. And that all cuda-based dockers shall be built using nvidia/cuda base images instead: Restrict users (to the reasonable extent) from installing those packages while running SSH/terminal session in the container. If user will try to install a restricted package - a warning will be shown (with an option to bypass it - for the advanced users):","title":"Restricting manual installation of the nvidia tools"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#setup-swap-files-for-the-cloud-vms","text":"This feature is addresses the same issues as the previous Notifications about high resources pressure by making the compute-intensive jobs runs more reliable. In certain cases jobs may fail with unexpected errors if the compute node runs Out Of Memory . v0.15 provides an ability for admin users to configure a default swap volume to the compute node being created. This allows to avoid runs failures due to memory limits. The size of the swap volume can be configured via cluster.networks.config item of the Preferences . It is accomplished by adding the similar json object to the platform's global or a region/cloud specific configuration: Options that can be used to configure swap : swap_ratio - defines a swap file size. It is equal the node RAM multiplied by that ratio. If ratio is 0, a swap file will not be created (default: 0) swap_location - defines a location of the swap file. If that option is not set - default location will be used (default: AWS will use SSD/gp2 EBS, Azure will be Temporary Storage ) See an example here .","title":"Setup swap files for the Cloud VMs"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#runs-system-paths-shall-be-available-for-the-general-user-account","text":"Previously, all the system-level directories (e.g. pipeline code location - $SCRIPTS_DIR , input/common data folders - $INPUT_DATA , etc.) were owned by the root user with read-only access to the general users. This was working fine for the pipeline runs, as they are executed on behalf of root . But for the interactive sessions (SSH/Web endpoints/Desktop) - such location were not writable. From now on - all the system-level location will be granted rwx access for the OWNER of the job (the user, who launched that run).","title":"Run's system paths shall be available for the general user account"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#renewed-wdl-visualization","text":"v0.15 offers an updated Web GUI viewer/editor for the WDL scripts. These improvements allow to focus on the WDL diagram and make it more readable and clear. Which very useful for large WDL scripts. Auxiliary controls (\"Save\", \"Revert changes\", \"Layout\", \"Fit to screen\", \"Show links\", \"Zoom out\", \"Zoom in\", \"Fullscreen\") are moved to the left side of the WDL GRAPH into single menu: WDL search capabilities are added. This feature allows to search for any task/variable/input/output within the script and focus/zoom to the found element. Search box is on the auxiliary controls menu and supports entry navigation (for cases when more than one item was found in the WDL): Workflow/Task editor is moved from the modal popup to the right floating menu PROPERTIES : See more details here .","title":"Renewed WDL visualization"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#queued-state-of-the-run","text":"Previously, user was not able to distinguish runs that are waiting in the queue and the runs that are being initialized (both were reporting the same state using the same icons). Now, a more clear run's state is provided - \"QUEUED\" state is introduced: During this phase of the lifecycle - a job is waiting in the queue for the available compute node. Typically this shall last for a couple of second and proceed to the initialization phase. But if this state lasts for a long time - it may mean that a cluster capacity is reached (limited by the administrator). This feature allows users to make a decision - whether to wait for run in a queue or stop it and resubmit. See more details here .","title":"\"QUEUED\" state of the run"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#help-tooltips-for-the-run-state-icons","text":"With the runs' QUEUED state introduction - we now have a good number of possible job phases. To make the phases meaning more clear - tooltips are provided when hovering a run state icon within all relevant forms, i.e.: Dashboard , Runs ( Active Runs / History /etc.), Run Log . Tooltips contain a state name in bold (e.g. Queued ) and a short description of the state and info on the next stage: See more details - Active runs states , Completed runs states and Home page .","title":"Help tooltips for the run state icons"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#vm-monitor-service","text":"For various reasons cloud VM instances may \"hang\" and become invisible to Cloud Pipeline services. E.g. VM was created but some error occurred during joining k8s cluster or a communication to the Cloud Providers API is interrupted. In this case Autoscaler service will not be able to find such instance and it won't be shut down. This problem may lead to unmonitored useless resource consumption and billing. To address this issue - a separate VM-Monitor service was implemented: Tracks all VM instances in the registered Cloud Regions Determines whether instances is in \"hang\" state Notifies a configurable set of users about a possible problem Notification recipients (administrators) may check the actual state of VM in Cloud Provider console and shut down VM manually. Additionally, VM-Monitor : Checks states of Cloud Pipeline's services (Kubernetes deployments). If any service changes it's state (i.e. goes down or up) - administrators will get the corresponding notification. List of such Kubernetes deployments to check includes all Cloud Pipeline's services by default, but also could be configed manually Checks all the PKI assets, available for the Platform, for the expiration - traverses over the list of directories that should contains certificate files, searches that certificates and verifies their expiration date. If some certificate expires less than in a certain amount of days - administrators also will get the corresponding notification. List of certificate directories to scan, certificate's mask and amount of days before expiration after which the notification will be sent are configurable","title":"VM monitor service"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#web-gui-caching","text":"Previously, Cloud Pipeline Web GUI was not using HTTP caching to optimize the page load time. Each time application was loaded - ~2Mb of the app bundle was downloaded. This caused \"non-optimal\" experience for the end-users. Now the application bundle is split into chunks, which are identified by the content-hash in names: If nothing is changed - no data will be downloaded If some part of the app is changed - only certain chunks will be downloaded, not the whole bundle Administrator may control cache period using the static.resources.cache.sec.period parameter in the application.properties of the Core API service.","title":"Web GUI caching"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#installation-via-pipectl","text":"Previous versions of the Cloud Pipeline did not offer any automated approach for deploying its components/services. All the deployment tasks were handed manually or by custom scripts. To simplify the deployment procedure and improve stability of the deployment - pipectl utility was introduced. pipectl offers an automated approach to deploy and configure the Cloud Pipeline platform, as well as publish some demo pipelines and docker images for NGS/MD/MnS tasks. Brief description and example installation commands are available in the pipectl's home directory . More sophisticated documentation on the installation procedure and resulting deployment architecture will be created further.","title":"Installation via pipectl"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#add-more-logging-to-troubleshoot-unexpected-pods-failures","text":"When a Cloud Pipeline is being for a long time (e.g. years), it is common to observe rare \"strange\" problems with the jobs execution. I.e. the following behavior was observed couple of times over the last year:","title":"Add more logging to troubleshoot unexpected pods failures"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#scenario-1","text":"Run is launched and initialized fine During processing execution - run fails Console logs print nothing, compute node is fine and is attached to the cluster","title":"Scenario 1"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#scenario-2","text":"Run is launched, compute node is up Run fails during initialization Console logs print the similar error message: failed to open log file /var/log/pods/**.log : open /var/log/pods/**.log: no such file or directory Both scenarios are flaky and almost impossible to reproduce. To provide more insights into the situation - an extended node-level logging was implemented: kubelet logs (from all compute nodes) are now written to the files (via DaemonSet ) Log files are streamed to the storage, identified by storage.system.storage.name preference Administrators can find the corresponding node logs (e.g. by the hostname or ip that are attached to the run information) in that storage under logs/node/{hostname} See an example here .","title":"Scenario 2"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#displaying-information-on-the-nested-runs-within-a-parent-log-form","text":"Previously, if user launched a run, that has a number of children (e.g. a cluster run or any other case with the parent-id specified), he could view the children list only from \"Runs\" page. In v.0.15 a convenient opportunity to view the list of children directly in the parent's run logs form is implemented: For each child-run in the list the following information is displayed: State icons with help tooltips when hovering over them Pipeline name and version/docker image and version Run time duration Similar as a parent-run state, states for nested runs are automatically updated without page refreshing. So, you can watch for them in real time. If you click any of the children-runs, you will navigate to its log page. That feature is implemented for the comleted runs too: More information about nested runs displaying see here and here .","title":"Displaying information on the nested runs within a parent log form"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#environment-modules-support-for-the-cloud-pipeline-runs","text":"The Environment Modules package provides for the dynamic modification of a user's environment via modulefiles . In current version, an ability to configure the Modules support for the compute jobs is introduced, if this is required by any use case. For using facilities of the Environment Modules package, a new system parameter was added to the Cloud Pipeline: CP_CAP_MODULES (boolean) - enables installation and using the Modules for the current run (for all supported Linux distributions) If CP_CAP_MODULES system parameter is set - the Modules will be installed and made available. While installing, Modules will be configured to the source modulefiles path from the CP_CAP_MODULES_FILES_DIR launch environment variable (value of this variable could be set only by admins via system-level settings). If that variable is not set - default modulefiles location will be used. See an example here .","title":"Environment Modules support for the Cloud Pipeline runs"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#sharing-ssh-access-to-running-instances-with-other-usersgroups","text":"As was introduced in Release Notes v.0.13 , for certain use cases it is beneficial to be able to share applications with other users/groups. v0.15 introduces a feature that allows to share the SSH-session of any active run (regardless of whether the job type is interactive or not): The user can share an interactive run with others: \"Share with: ...\" parameter, within a run log form, can be used for this Specific users or whole groups can be set for sharing Once this is set - other users will be able to access run's endpoints Also you can share SSH access to the running instance via setting \" Enable SSH connection \" checkbox Also, the user can share a non-interactive run: \"Share with: ...\" parameter, within a run log form, can be used for this Specific users or whole groups can be set for sharing Once this is set - specified users/groups will be able to access the running instance via the SSH SERVICES widget within a Home dashboard page lists such \"shared\" services. It displays a \"catalog\" of services, that can be accessed by a current user, without running own jobs. To open shared instance application user should click the service name. To get SSH-access to the shared instance (regardless of whether the job type is interactive or not), the user should hover over the service \"card\" and click the SSH hyperlink For more information about runs sharing see 11.3. Sharing with other users or groups of users .","title":"Sharing SSH access to running instances with other user(s)/group(s)"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#allow-to-limit-the-number-of-concurrent-ssh-sessions","text":"Previously, some users could try to start a real big number of Web SSH sessions. If 1000+ SSH sessions are established via EDGE service, the performance will degrade. It is not common, but it could be critical as it affects all the users of the platform deploment. To avoid such cases, in v0.15 the pipectl parameter CP_EDGE_MAX_SSH_CONNECTIONS (with default value 25 ) for the EDGE server is introduced, that allows to control a number of simultaneous SSH connections to a single job. Now, if this max number will be reached, the next attemp to open another one Web SSH session to the same job will return a notification to the user and a new session will not be opened until the any one previous is closed:","title":"Allow to limit the number of concurrent SSH sessions"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#verification-of-dockerstorage-permissions-when-launching-a-run","text":"Users are allowed to launch pipeline, detached configuration or tool if they have a corresponding permission for that executable. But in some cases this verification is not enough, e.g. when user has no read permission for input parameter - in this case, run execution could cause an error. In v0.15 additional verification implemented that checks if: execution is allowed for specified docker image; read operations are allowed for input and common path parameters; write operations are allowed for output path parameters. If there are such permission issues, run won't be launched and special warning notifications will be shown to a user, e.g.: For more details see sections 6.2. Launch a pipeline , 7.2. Launch Detached Configuration and 10.5. Launch a Tool .","title":"Verification of docker/storage permissions when launching a run"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#ability-to-override-the-queuepe-configuration-in-the-ge-configuration","text":"Previously, if the Grid Engine was enabled, the following was configured: a single queue with all the hosts was creating, named \" main.q \" a single PE (Parallel Environment) was creating, named \" local \" In v0.15 , the overriding of the names of the queue / PE is implemented to be compatible with any existing scripts, that rely on a specific GE configuration (e.g. hardcoded). You can do it using two new System Parameters at the Launch or the Configuration forms: CP_CAP_SGE_QUEUE_NAME (string) - allows to override the GE's queue name (default: \" main.q \") CP_CAP_SGE_PE_NAME (string) - allows to override the GE's PE name (default: \" local \") More information how to use System Parameters when a job is launched see here .","title":"Ability to override the queue/PE configuration in the GE configuration"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#estimation-runs-disk-size-according-to-the-inputcommon-parameters","text":"Previously, if a job was run with the disk size, which was not enough to handle the job's inputs - it failed (e.g. 10Gb disk was set for a run, which processed data using STAR aligner, where the genome index file is 20Gb). In v0.15 , an attempt to handle some of such cases is implemented. Now, the Cloud Pipeline try to estimate the required disk size using the input/common parameters and warn the user if the requested disk is not enough. When a job is launching, the system try to get the size of all input/common parameters. The time of the size getting for all files is limited, as this may take too much for lots of small files. Limit for this time is set by the storage.listing.time.limit system preference (in milliseconds). Default: 3 sec (3000 milliseconds). If computation doesn't end in this timeout, accumulated size will return as is. If the resulting size of all input/common parameters is greater than requested disk size (considering cluster configuration) - the user will be warned: User can set suggested disk size or launch a job at user's own risk with the requested size. If calculated suggested disk size exceeds 16Tb (hard limit) a different warning message will be shown: The requested disk size for this run is N Gb, but the data that is going to be processed exceeds 16 Tb (which is a hard limit). Please use the cluster run configuration to scale the disks horizontally or reduce the input data volume. Do you want to use the maximum disk size 16 Tb anyway?","title":"Estimation run's disk size according to the input/common parameters"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#disabling-of-the-global-search-form-if-a-corresponding-service-is-not-installed","text":"Version 0.14 introduced the Global Search feature over all Cloud Pipeline objects. In current version, a small enhancement for the Global Search is implemented. Now, if the search.elastic.host system preference is not set by admin - other users will not be able to try search performing: the \"Search\" button will be hidden from the left menu keyboard search shortcut will be disabled","title":"Disabling of the Global Search form if a corresponding service is not installed"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#disabling-of-the-fs-mounts-creation-if-no-fs-mount-points-are-registered","text":"In the Cloud Pipeline , along with the regular data storages user can also create FS mounts - data storages based on the network file system: For the correct FS mount creation, at least one mount point shall be registered in the Cloud Pipeline Preferences. Now, if no FS mount points are registered for any Cloud Region in the System Preferences - user can not create a new FS mount, the corresponding button becomes invisible:","title":"Disabling of the FS mounts creation if no FS mount points are registered"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#displaying-resource-limit-errors-during-run-resuming","text":"User may hit a situation of resource limits while trying to resume previously paused run. E.g. instance type was available when run was initially launched, but at the moment of resume operation provider has no sufficient capacity for this type. Previously, in this case run could be failed with an error of insufficient resources. In v0.15 the following approach is implemented for such cases: resuming run doesn't fail if resource limits are hit. That run returns to the Paused state log message that contains a reason for resume failure and returning back to the Paused state is being added to the ResumeRun task user is notified about such event. The corresponding warning messages are displayed: at the Run logs page at the ACTIVE RUNS page (hint message while hovering the RESUME button) at the ACTIVE RUNS panel of the Dashboard (hint message while hovering the RESUME button)","title":"Displaying resource limit errors during run resuming"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#object-storage-creation-in-despite-of-that-the-corspolicies-could-not-be-applied","text":"Previously, if the Cloud service account/role had permissions to create object storages, but lacked permissions to apply CORS or other policies - object storage was created, but the Cloud Pipeline API threw an exception and storage was not being registered. This led to the creation of a \"zombie\" storage, which was not available via GUI, but existed in the Cloud. Currently, the Cloud Pipeline API doesn't fail such requests and storage is being registered normally. But the corresponding warning will be displayed to the user like this: The storage {storage_name} was created, but certain policies were not applied. This can be caused by insufficient permissions.","title":"Object storage creation in despite of that the CORS/Policies could not be applied"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#track-the-confirmation-of-the-blocking-notifications","text":"System events allow to create popup notifications for users. One of the notification types - the \"Blocking\" notification. Such event emerges in the middle of the window and requires confirmation from the user to disappear for proceeding with the GUI operations. In certain cases (e.g. for some important messages), it is handy to be able to check which users confirmed the notification. For that, in the current version the ability to view, which \"blocking\" notifications confirmed by specific user, was implemented for admins. Information about confirmed notifications can be viewed at the \" Attributes \" section of the specific user's profile page: Confirmed notifications are displayed as user attribute with the KEY confirmed_notifications (that name could be changed via the system-level preference system.events.confirmation.metadata.key ) and the VALUE link that shows summary count of confirmed notifications for the user. Click the VALUE link with the notification count to open the detailed table with confirmed notifications: For more details see \"blocking\" notifications track .","title":"Track the confirmation of the \"Blocking\" notifications"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#pipe-cli-warnings-on-the-jwt-expiration","text":"By default, when pipe CLI is being configured JWT token is given for one month, if user didn't select another expiration date. In v.0.15 extra pipe CLI warnings are introduced to provide users an information on the JWT token expiration: When pipe configure command is executed - the warning about the expiration date of the provided token is printed, if it is less than 7 days left: When --version option is specified - pipe prints dates of issue and expiration for the currently used token: When any other command is running - the warning about the expiration date of the provided JWT token is printed, if it is less than 7 days left: See more information about pipe CLI installation here .","title":"pipe CLI warnings on the JWT expiration"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#pipe-configuration-for-using-ntlm-authentication-proxy","text":"For some special customer needs, pipe configuration for using NTLM Authentication Proxy, when running in Linux, could be required. For that, several new options were added to pipe configure command: -nt or --proxy-ntlm - flag that enable NTLM proxy support -nu or --proxy-ntlm-user - username for NTLM proxy authorization -np or --proxy-ntlm-pass - password for NTLM proxy authorization -nd or --proxy-ntlm-domain - domain for NTLM proxy authorization If --proxy-ntlm is set, pipe will try to get the proxy value from the environment variables or --proxy option ( --proxy option has a higher priority). If --proxy-ntlm-user and --proxy-ntlm-pass options are not set - user will be prompted for username/password in an interactive manner. Valid configuration examples: User will be prompted for NTLM Proxy Username, Password and Domain: pipe configure --proxy-ntlm ... Username for the proxy NTLM authentication: user1 Domain of the user1 user: '' Password of the user1 user: Use http://myproxy:3128 as the \"original\" proxy address. User will not be prompted for NTLM credentials: pipe configure --proxy-ntlm --proxy-ntlm-user $MY_NAME --proxy-ntlm-pass $MY_PASS --proxy http://myproxy:3128 See more information about pipe CLI installation and configure here .","title":"pipe configuration for using NTLM Authentication Proxy"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#execution-of-files-uploading-via-pipe-without-failures-in-case-of-lacks-read-permissions","text":"Previously, pipe storage cp / mv commands could fail if a \"local\" source file/dir lacked read permissions. For example, when user tried to upload to the \"remote\" storage several files and when the pipe process had reached one of files that was not readable for the pipe process, then the whole command was being failed, remaining files did not upload. In current version, the pipe process checks read permission for the \"local\" source (directories and files) and skip those that are not readable:","title":"Execution of files uploading via pipe without failures in case of lacks read permissions"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#run-a-single-command-or-an-interactive-session-over-the-ssh-protocol-via-pipe","text":"For the certain purposes, it could be conveniently to start an interactive session over the SSH protocol for the job run via the pipe CLI. For such cases, in v0.15 the pipe ssh command was implemented. It allows you, if you are the ADMIN or the run OWNER , to perform a single command or launch an interactive session for the specified job run. Launching of an interactive session: This session is similar to the terminal access that user can get via the GUI. Performing the same single command without launching an interactive session:","title":"Run a single command or an interactive session over the SSH protocol via pipe"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#perform-objects-restore-in-a-batch-mode-via-pipe","text":"Users can restore files that were removed from the data storages with enabled versioning. For these purposes, the Cloud Pipeline's CLI has the restore command which is capable of restoring a single object at a time. In v0.15 the ability to recursively restore the whole folder, deleted from the storage, was implemented. Now, if the source path is a directory, the pipe storage restore command gets the top-level deleted files from the source directory and restore them to the latest version. Also, to the restore command some options were added: -r or --recursive - flag allows to restore the whole directory hierarchy -i or --include [TEXT] - flag allows to restore only files which names match the [TEXT] pattern and skip all others -e or --exclude [TEXT] - flag allows to skip restoring of files which names match the [TEXT] pattern and restore all others Note : this feature is yet supported for AWS only. For more details about file restoring via the pipe see here .","title":"Perform objects restore in a batch mode via pipe"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#mounting-data-storages-to-linux-and-mac-workstations","text":"Previously, when users had to copy/move datasets to/from Cloud data storages via CLI, they could use only special pipe storage commands. That was not always comfortable or could lead to some functionality restrictions. In v0.15 the ability to mount Cloud data storages (both - File Storages and Object Storages) to Linux and Mac workstations (requires FUSE installed) was added. For the mounted storages, regular listing/read/write commands are supported, users can manage files/folders as with any general hard drive. Note : this feature is yet supported for AWS only. To mount a data storage into the local mountpoint the pipe storage mount command was implemented. It has two main options that are mutually exclusive: -f or --file specifies that all available file systems should be mounted into a mountpoint -b or --bucket [STORAGE_NAME] specifies a storage name to mount into a mountpoint Users can: leverage mount options, supported by underlying FUSE implementation, via -o option enable multithreading for simultaneously interaction of several processes with the mount point, via -t option trace all information about mount operations into a log-file, via -l option To unmount a mountpoint the pipe storage umount command was implemented. For more details about mounting data storages via the pipe see here .","title":"Mounting data storages to Linux and Mac workstations"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#allow-to-run-pipe-commands-on-behalf-of-the-other-user","text":"In the current version, the ability to run pipe commands on behalf of the other user was implemented. It could be convenient when administrators need to perform some operations on behalf of the other user (e.g. check permissions/act as a service account/etc.). This feature is implemented via the common option that was added to all pipe commands: --user|-u USER_ID (where USER_ID is the name of the user account). Note : the option isn't available for the following pipe commands: configure , --version , --help . If this option is specified - operation (command execution) will be performed using the corresponding user account, e.g.: In the example above, active runs were outputted from the admin account (firstly) and then on behalf of the user without ROLE_ADMIN role. Additionally, a new command pipe token USER_ID was implemented. It prints the JWT token for a specified user. This command also can be used with non-required option -d ( --duration ), that specified the number of days the token will be valid. If it's not set - the default value will be used, same as in the GUI. Example of using: Then, the generated JWT token could be used manually with the pipe configure command - to configure pipe CLI on behalf of the desired user. Note : both - the command ( pipe token ) and the option ( --user ) - are available only for admins. For more details see here .","title":"Allow to run pipe commands on behalf of the other user"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#ability-to-restrict-the-visibility-of-the-jobs","text":"Previously, Cloud Pipeline inherited the pipeline jobs' permissions from the parent pipeline object. So, if two users had permissions on the same pipeline, then when the first user had launched that pipeline - the second user also could view (not manage) launched run in the Active Runs tab. Now admin can restrict the visibility of the jobs for non-owner users. The setting of such visibility can get one of the following values: Inherit - the behavior is the same as described above (for the previous approach), when the runs visibility is controlled by the pipeline permissions. It is set as a default for the Cloud Pipeline environment Only owner - when only the person who launch a run can see it Jobs visibility could be set by different ways on several forms: within User management tab in the system-level settings admin can specify runs visibility for a user/group/role: within Launch section of Preferences tab in the system-level settings admin can specify runs visibility for a whole platform as global defaults - by the setting launch.run.visibility : Next hierarchy is set for applying of specified jobs visibility: User level - highest priority (specified for a user) Group level (specified for a group/role) Platform level launch.run.visibility (specified as global defaults via system-level settings) Note : admins can see all runs despite of settings","title":"Ability to restrict the visibility of the jobs"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#ability-to-perform-scheduled-runs-from-detached-configurations","text":"Previously, Cloud Pipeline allowed starting compute jobs only manually (API/GUI/CLI). But in certain use cases, it is beneficial to launch runs on a scheduled basis. In v0.15 the ability to configure a schedule for detached configuration was implemented: User is able to set a schedule for launch a run from the detached configuration: Schedule is defined as a list of rules - user is able to specify any number of them: For each rule in the list user is able to set the recurrence: If any schedule rule is configured for the detached configuration - a corresponding job (plain container or a pipeline) will be started accordingly in the scheduled day and time. See more details here .","title":"Ability to perform scheduled runs from detached configurations"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#the-ability-to-use-custom-domain-names-as-a-friendly-url-for-the-interactive-services","text":"In v0.13 the ability to set a \"friendly URL\" for the interactive services endpoint was implemented. It allows to configure the view of the interactive service endpoint: Default view: https:// host /pipeline- run-id - port \"Friendly\" view: https:// host / friendly-url In the current version this feature is expanded: users allow to specify a custom host. So the endpoint url now can look like: https:// custom-host or https:// custom-host / friendly-url . Note : custom host should exist, be valid and configured. The custom host is being specified into the same field as a \"friendly URL\" previously, e.g.: Final URL for the service endpoint will be generated using the specified host and friendly URL: For more details see here .","title":"The ability to use custom domain names as a \"friendly URL\" for the interactive services"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#displaying-of-the-additional-support-iconinfo","text":"In certain cases, users shall have a quick access to the help/support information (e.g. links to docs/faq/support request/etc.) In the current version, the ability to display additional \"support\" icon with the corresponding info in the bottom of the main menu was implemented: The displaying of this icon and the info content can be configured by admins via the system-level preference ui.support.template : this preference is empty by default - in this case the support icon is invisible if this preference contains any text ( Markdown -formatted): the support icon is visible specified text is displayed in the support icon tooltip (support info) For more details see UI system settings .","title":"Displaying of the additional support icon/info"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#pass-proxy-settings-to-the-dind-containers","text":"Previously, DIND containers configuration included only registry credentials and a couple of driver settings. In certain environments, it is not possible to access external networks (e.g. for the packages installation) without the proxy settings. So the users had to pass this manually every time when using the docker run command. In the current version, a new system preference launch.dind.container.vars is introduced. It allows to specify all the additions variables, which will be passed to the DIND containers (if they are set for the host environment). By default, the following variables are set for the launch.dind.container.vars preference (and so will be passed to DIND container): http_proxy , https_proxy , no_proxy , API , API_TOKEN . Variables are being specified as a comma-separated list. Example of using: At the same time, a new system parameter (per run) was added - CP_CAP_DIND_CONTAINER_NO_VARS , which disables described behavior. You can set it before any run if you don't want to pass any additional variations to the DIND container.","title":"Pass proxy settings to the DIND containers"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#interactive-endpoints-can-be-optionally-available-to-the-anonymous-users","text":"Cloud Pipeline allows sharing the interactive and SSH endpoints with the other users/groups. Previously, this necessary required the end-user to be registered in the Cloud Pipeline users database. For certain use-cases, it is required to allow such type of access for any user, who has successfully passed the IdP authentication but is not registered in the Cloud Pipeline and also such users shall not be automatically registered at all and remain Anonymous . In the current version, such ability is implemented. It's enabled by the following application properties: saml.user.auto.create=EXPLICIT_GROUP saml.user.allow.anonymous=true After that, to share any interactive run with the Anonymous - it's simple enough to share endpoints with the following user group - ROLE_ANONYMOUS_USER : At the Run logs page: The user should select the ROLE_ANONYMOUS_USER role to share: Sharing with the Anonymous will be displayed at the Run logs page: That's all. Now, the endpoint-link of the run could be sent to the Anonymous user. If that Anonymous user passes SAML authentication, he will get access to the endpoint. Attempts to open any other Platform pages will fail. For more details see here .","title":"Interactive endpoints can be (optionally) available to the anonymous users"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#notable-bug-fixes","text":"","title":"Notable Bug fixes"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#incorrect-behavior-of-the-global-search-filter","text":"#221 When user was searching for an entry, that may belong to different classes (e.g. issues and folders ) - user was not able to filter the results by the class.","title":"Incorrect behavior of the global search filter"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#committing-status-hangs","text":"#152 In certain cases, while committing pipeline with the stop flag enabled - the run's status hangs in Committing... state. Run state does not change even after the commit operation succeeds and a job is stopped.","title":"\"COMMITTING...\" status hangs"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#instances-of-metadata-entity-arent-correctly-sorted","text":"#150 Metadata entities (i.e. project-related metadata) sorting was faulty: Sort direction indicator (Web GUI) was displaying an inverted direction Entities were not sorted correctly","title":"Instances of Metadata entity aren't correctly sorted"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#tool-group-cannot-be-deleted-until-all-child-tools-are-removed","text":"#144 If there is a tool group in the registry, which is not empty (i.e. contains 1+ tools) - an attempt to delete it throws SQL error. It works fine if the child tools are dropped beforehand. Now, it is possible to delete such a group if a force flag is set in the confirmation dialog.","title":"Tool group cannot be deleted until all child tools are removed"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#missing-region-while-estimating-a-run-price","text":"#93 On the launch page, while calculating a price of the run, Cloud Provider's region was ignored. This way a calculation used a price of the specified instance type in any of the available regions. In practice, requested price may vary from region to region.","title":"Missing region while estimating a run price"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#cannot-specify-region-when-an-existing-object-storage-is-added","text":"#45 Web GUI interface was not providing an option to select a region when adding an existing object storage. And it was impossible to add a bucket from the non-default region.","title":"Cannot specify region when an existing object storage is added"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#acl-control-for-pipeline_user-and-role-entities-for-metadata-api","text":"#265 All authorized users were permitted to browse the metadata of users and roles entities. But those entries may contain a sensitive data, that shall not be shared across users. Now, a general user may list only personal user-level metadata. Administrators may list both users and roles metadata across all entries.","title":"ACL control for PIPELINE_USER and ROLE entities for metadata API"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#getting-logs-from-kubernetes-may-cause-outofmemory-error","text":"#468 For some workloads, container logs may become very large: up to several gigabytes. When we tried to fetch such logs it is likely to cause OutOfMemory error, since Kubernetes library tries to load it into a single String object. In current version, a new system preference was introduced: system.logs.line.limit . That preference sets allowable log size in lines. If actual pod logs exceeds the specified limit only log tail lines will be loaded, the rest will be truncated.","title":"Getting logs from Kubernetes may cause OutOfMemory error"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#aws-incorrect-nodeup-handling-of-spot-request-status","text":"#556 Previously, in a situation when an AWS spot instance created after some timeout - spot status wasn't updated correctly in the handling of spot request status . It might cause errors while getting spot instance info.","title":"AWS: Incorrect nodeup handling of spot request status"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#not-handling-clusters-in-autopause-daemon","text":"#557 Previously, if cluster run was launched with enabled \"Auto pause\" option, parent-run or its child-runs could be paused (when autopause conditions were satisfied, of course). It was incorrect behavior because in that case, user couldn't resume such paused runs and go on his work (only \"Terminate\" buttons were available). In current version, autopause daemon doesn't handle any clusters (\"Static\" or \"Autoscaled\"). Also now, if the cluster is configured - Auto pause checkbox doesn't display in the Launch Form for the On-Demand node types.","title":"Not handling clusters in autopause daemon"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#incorrect-pipe-cli-version-displaying","text":"#561 Previously, pipe CLI version displayed incorrectly for the pipe CLI installations performed via hints from the Cloud Pipeline System Settings menu.","title":"Incorrect pipe CLI version displaying"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#jwt-token-shall-be-updated-for-the-jobs-being-resumed","text":"#579 In cases when users launched on-demand jobs, paused them and then, after a long time period (2+ months), tried to resume such jobs - expired JWT tokens were set for them that led to different problems when any of the initialization routines tried to communicate with the API. Now, the JWT token and other variables as well are being updated when a job is being resumed.","title":"JWT token shall be updated for the jobs being resumed"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#trying-to-rename-file-in-the-data-storage-while-the-attributes-panel-is-opened-throws-an-error","text":"#520 Renaming file in the datastorage with opened \"Attributes\" panel caused an unexpected error.","title":"Trying to rename file in the data storage, while the \"Attributes\" panel is opened, throws an error"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#pipe-incorrect-behavior-of-the-nc-option-for-the-run-command","text":"#609 Previously, trying to launch a pipeline via the pipe run command with the single -nc option threw an error.","title":"pipe: incorrect behavior of the -nc option for the run command"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#cluster-run-cannot-be-launched-with-a-pretty-url","text":"#620 Previously, if user tried to launch any interactive tool with Pretty URL and configured cluster - an error appeared URL {Pretty URL} is already used for run {Run ID} . Now, pretty URL could be set only for the parent runs, for the child runs regular URLs are set.","title":"Cluster run cannot be launched with a Pretty URL"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#cloning-of-large-repositories-might-fail","text":"#626 When large repository ( 1Gb) was cloned (e.g. when a pipeline was being run) - git clone could fail with the OOM error happened at the GitLab server if it is not powerful enough. OOM was produced by the git pack-objects process, which tries to pack all the data in-memory. Now, git pack-objects memory usage is limited to avoid errors in cases described above.","title":"Cloning of large repositories might fail"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#system-events-html-overflow","text":"#630 If admin set a quite long text (without separators) into the message body of the system event notifications - the resulting notification text \"overflowed\" the browser window. Now, text wrapping is considered for such cases. Also, support of Markdown was added for the system notification messages:","title":"System events HTML overflow"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#aws-pipeline-run-initializenode-task-fails","text":"#635 Previously, if AWS spot instance could not be created after the specific number of attempts during the run initialization - such run was failed with the error, e.g.: Exceeded retry count (100) for spot instance. Spot instance request status code: capacity-not-available . Now, in these cases, if spot instance isn't created after specific attempts number - the price type is switched to on-demand and run initialization continues.","title":"AWS: Pipeline run InitializeNode task fails"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#git-sync-shall-not-fail-the-whole-object-synchronization-if-a-single-entry-errors","text":"#648 , #691 When the git-sync script processed a repository and failed to sync permissions of a specific user (e.g. git exception was thrown) - the subsequent users were not being processed for that repository. Now, the repository sync routine does not fail if a single user cannot be synced. Also, the issues with the synchronization of users with duplicate email addresses and users with empty email were resolved.","title":"git-sync shall not fail the whole object synchronization if a single entry errors"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#enddate-isnt-set-when-node-of-a-paused-run-was-terminated","text":"#743 Previously, when user terminated the node of a paused run - endDate for that run wasn't being set. This was leading to wrong record of running time for such run.","title":"endDate isn't set when node of a paused run was terminated"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#aws-nodeup-retry-process-may-stuck-when-first-attempt-to-create-a-spot-instance-failed","text":"#744 Previously, if first nodeup attempt failed due to unavailablity to connect on 8888 port (in expected amounts of attempts) after getting instance running state, the second nodeup attempt might stuck because it waited for the same instance (associated with existed SpotRequest for the first attempt) to be up. But it couldn't happen - this instance was already in terminating state after the first attempt. Now, checks that instance associated with SpotRequest (created for the first attempt) is in appropriate status, if not - a new SpotRequest is being created and the nodeup process is being started from scratch.","title":"AWS: Nodeup retry process may stuck when first attempt to create a spot instance failed"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#resume-job-timeout-throws-strange-error-message","text":"#832 Previously, the non-informative error message was shown if the paused run could't be resumed in a reasonable amount of time - the count of attempts to resume was displaying incorrectly.","title":"Resume job timeout throws strange error message"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#ge-autoscaler-doesnt-remove-dead-additional-workers-from-cluster","text":"#946 Previously, the Grid Engine Autoscaler didn't properly handle dead workers downscaling. For example, if some spot worker instance was preempted during the run then the autoscaler could not remove such worker from GE. Moreover, such cluster was blocked from accepting new jobs.","title":"GE autoscaler doesn't remove dead additional workers from cluster"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#broken-layouts","text":"#553 , #619 , #643 , #644 , #915 Previously, pipeline versions page had broken layout if there were pipeline versions with long description. Global search page was not rendered correctly when the search results table had too many records. When a list of items in the docker groups selection dialog was long - it was almost impossible to use a search feature, as the list hid immediately. Some of the other page layouts also were broken.","title":"Broken layouts"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/","text":"Cloud Pipeline v.0.16 - Release notes Google Cloud Platform Support System logs Displaying Cloud Provider's icon Configurable timeout of GE Autoscale waiting Storage mounts data transfer restrictor Extended recursive symlinks handling Displaying of the latest commit date/time Renaming of the GitLab repository in case of Pipeline renaming Pushing pipeline changes to the GitLab on behalf of the user Allowing to expose compute node FS to upload and download files Resource usage form improvement View the historical resources utilization Ability to schedule automatic pause/restart of the running jobs Update pipe CLI version Blocking/unblocking users and groups Displaying additional node metrics Export user list Displaying SSH link for the active runs in the Dashboard view Enable Slurm for the Cloud Pipeline's clusters The ability to generate the pipe run command from the GUI pipe CLI: view tools definitions List the users/groups/objects permissions globally via pipe CLI Storage usage statistics retrieval via pipe GE Autoscaler respects CPU requirements of the job in the queue Restrictions of \"other\" users permissions for the mounted storage Search the tool by its version/package name The ability to restrict which run statuses trigger the email notification The ability to force the specific Cloud Provider for an image Restrict mounting of data storages for a given Cloud Provider Ability to \"symlink\" the tools between the tools groups Notable Bug fixes Parameter values changes cannot be saved for a tool Packages are duplicated in the tool's version details Autoscaling cluster can be autopaused pipe : not-handled error while trying to execute commands with invalid config Setting of the tool icon size NPE while building cloud-specific environment variables for run Worker nodes fail due to mismatch of the regions with the parent run Worker nodes shall not be restarted automatically Uploaded storage file content is downloaded back to client GUI improperly works with detached configurations in a non-default region Detached configuration doesn't respect region setting Incorrect behavior of the \"Transfer to the cloud\" form in case when a subfolder has own metadata Incorrect displaying of the \"Start idle\" checkbox Limit check of the maximum cluster size is incorrect Fixed cluster with SGE and DIND capabilities fails to start Azure: Server shall check Azure Blob existence when a new storage is created Azure: pipe CLI cannot transfer empty files between storages Azure: runs with enabled GE autoscaling doesn't stop Incorrect behavior while download files from external resources into several folders Detach configuration doesn't setup SGE for a single master run Broken layouts Google Cloud Platform Support One of the major v0.16 features is a support for the Google Cloud Platform . All the features, that were previously used for AWS and Azure , are now available in all the same manner, from all the same GUI/CLI, for GCP . This provides an even greater level of a flexibility to launch different jobs in the locations, closer to the data, with cheaper prices or better compute hardware in depend on a specific task. System logs In the current version, the \"Security Logging\" was implemented. Now, the system records audit trail events: users' authentication attempts users' profiles modifications platform objects' permissions management access to interactive applications from pipeline runs other platform functionality features Logs are collected/managed at the Elasticsearch node and backed up to the object storage (that could be configured during the platform deployment). The administrator can view/filter these logs via the GUI - in the System-level settings, e.g.: Each record in the logs list contains: Field Description Date The date and time of the log event Log status The status of the log message ( INFO , ERROR , etc.) Log message Description of the log event User User name who performed the event Service Service name that registered the event ( api-srv , edge ) Type Log message type (currently, only security type is available) For more details see here . Displaying Cloud Provider's icon for the storage/compute resources As presented in v0.15 , Cloud Pipeline can manage multi Cloud Providers in a single installation. In the current version, useful icon-hints with the information about using Cloud Provider are introduced. If a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - corresponding icons/text information are displaying next to the cloud resource. Such cloud resources are: Object/File Storages (icons in the Library , at the \"DATA\" panel of the Dashboard etc.) Regions (icons in the Cloud Regions configuration, at the Launch form etc.) Running jobs : text hints (at the RUNS page) icons (at the Run logs page, at the \"RUNS\" panels of the Dashboard ) Note : this feature is not available for deployments with a single Cloud Provider. Examples of displaying Cloud Region icons/info see in sections 6. Manage Pipeline , 7. Manage Detached configuration , 8. Manage Data Storage and 18. Home page . Configurable timeout of GE Autoscale waiting for a worker node up Previously, GE Autoscaler waited for a worker node up for a fixed timeout. This could lead to incorrect behavior for specific CLoud Providers , because the timeout can be very different. Current version extracts GE Autoscaler polling timeout to a new system preference ge.autoscaling.scale.up.polling.timeout . That preference defines how many seconds GE Autoscaler should wait for pod initialization and run initialization . Default value is 600 seconds ( 10 minutes). Storage mounts data transfer restrictor Users may perform cp or mv operations of the large files (50+ Gb) to and from the fuse-mounted storages. It is uncovered that such operations are not handled properly within the fuse implementations. Commands may hang for a long timeout or produce zero-sized result. The suggested more graceful approach for the copying of the large files is to use pipe cp / pipe mv commands, which are behaving correctly for the huge volumes. To avoid users of performing usual cp or mv commands for operations with the large files - now, Cloud Pipeline warns them about possible errors and suggest to use corresponding pipe commands. Specified approach is implemented in the following manner: if a cp / mv command is called with the source/dest pointing to the storage (e.g. /cloud-data/ storage_path /... ) - the overall size of the data being transferred is checked - if that size is greater than allowed, a warning message will be shown, e.g.: this warning doesn't abort the user's command execution, it is continued appearance of this warning is configured by the following launch environment variables (values of these variables could be set only by admins via system-level settings): CP_ALLOWED_MOUNT_TRANSFER_SIZE - sets number of gigabytes that is allowed to be transferred without warning. By default 50 Gb . CP_ALLOWED_MOUNT_TRANSFER_SIZE_TIMEOUT - sets number of seconds that the transfer size retrieving operation can take. By default 5 seconds . CP_ALLOWED_MOUNT_TRANSFER_FILES - sets number of files that is allowed to be transferred without warning. Supported only for Azure Cloud Provider. By default 100 files . Note : this feature is not available for NFS / SMB mounts, only for object storages. Extended recursive symlinks handling There could be specific cases when some services execute tasks using the on-prem storages, where \"recursive\" symlinks are presented. This causes the Cloud Pipeline Data transfer service to follow symlinks infinitely. In v0.16 , a new feature is introduced for Data transfer service to detect such issues and skip the upload for files/folders, that cause infinite loop over symlinks. A new option -sl ( --symlinks ) was added to the pipe storage cp / mv operations to handle symlinks (for local source) with the following possible values: follow - follow symlinks ( default ) skip - do not follow symlinks filter - follow symlinks but check for cyclic links and skip them Example for the folder with recursive and non-recursive symbolic links: Also options were added to the Data transfer service to set symlink policy for transfer operations. For more details about pipe storage cp / mv operations see here . Displaying of the latest commit date/time Users can modify existing tools and then commit them to save performed changes. It can be done by COMMIT button on the run's details page: Previously, if user committed some tool, only commit status was shown on the run's details page. In the current version, displaying of the date/time for the tool latest commit is added: For more details about tool commit see here . Renaming of the GitLab repository in case of Pipeline renaming Pipeline in the Cloud Pipeline environment is a workflow script with versioned source code, documentation, and configuration. Under the hood, it is a git repository. Previously, if the Pipeline object was renamed - the underlying GitLab repository was keeping the previous name. In the current version, if user renames a Pipeline the corresponding GitLab repository will be also automatically renamed: Need to consider in such case that the clone/pull/push URL changes too. Make sure to change the remote address, if you use the Pipeline somewhere else. For more details see here . Pushing pipeline changes to the GitLab on behalf of the user If the user saves the changed Pipeline object - it actually means that a new commit is created and pushed to the corresponding GitLab repo. Previously, all the commits pushed to the GitLab via the Cloud Pipeline GUI were made on behalf of the service account . This could break traceability of the changes. In current version, the author of the commit is displayed in the Web GUI (for all Pipeline versions - the draft and released), the commits are performed on behalf of the real user: Allowing to expose compute node FS to upload and download files For the interactive runs users are processing data in ad-hoc manner, which requires upload data from the local storage to the cloud and download results from the cloud to the local storage. Cloud Pipeline supports a number of options for the user to perform that data transfers for the interactive runs: via the Object Storage (using Web GUI or CLI) via the File Storage (using Web GUI, CLI, WebDav-mounted-drive) Previously, view, download, upload and delete operations required an intermediate location (bucket or fs) to be used. It might confuse user when a small dataset shall be loaded to the specific location within a run's filesystem. In the current version, direct exposure of the run's filesystem is supported. The BROWSE hyperlink is displayed on the Run logs page after a job had been initialized: User can click the link and a Storage browser Web GUI will be loaded: User is able to: view files and directories download and delete files and directories upload files and directories search files and directories For more details see here . Resource usage form improvement In v0.16 , the number of filters were added to the Monitor cluster nodes feature: Common range for all charts User can synchronize the time period for all plots. To do so user should mark the \"Common range for all charts\" filter. If this filter is unmarked, user can zoom any plot without any change for others. Live update The plots data will be updated every 5 seconds in a real-time manner. The fields with dates will be updated as well. Set range User can select the predefined time range for all plots from the list: Whole range Last week Last day Last hour Date filter User can specify the Start and the End dates for plots. The system will substitute the node creating date as the Start date and current date for the End date, if user doesn't select anything. All filters are working for all plots simultaneously: data for all plots will be dynamically updated as soon as the user changes filter value. For more details see here . Allow to view the historical resources utilization Another convenient feature that was implemented in v0.16 linked to the Cluster nodes monitor is viewing of the detailed history of the resource utilization for any jobs. Previously, it was available only for active jobs. Now, users can view the utilization data even for completed (succeed/stopped/failed) jobs for debugging/optimization purposes. The utilization data for all runs is stored for a preconfigured period of time that is set by the system preference system.resource.monitoring.stats.retention.period (defaults to 5 days). I.e. if the job has been stopped and the specified time period isn't over - the user can access to the resources utilization data of that job: Open the Run logs page for the completed job: Click the node name hyperlink The Monitor page of the node resources utilization will be opened: Also now, users have the ability to export the utilization information into a .csv file. This is required, if the user wants to keep locally the information for a longer period of time than defined by system.resource.monitoring.stats.retention.period : The user can select the interval for the utilization statistics output and export the corresponding file. For more details see here . Ability to schedule automatic pause/restart of the running jobs For certain use cases (e.g. when Cloud Pipeline is used as a development/research environment) users can launch jobs and keep them running all the time, including weekends and holidays. To reduce costs, in the current version, the ability to set a Run schedule was implemented. This feature allows to automatically pause/resume runs, based on the configuration specified. This feature is applied only to the \"Pausable\" runs (i.e. \"On-demand\" and non-cluster): The user (who has permissions to pause/resume a run) is able to set a schedule for a run being launched: Schedule is defined as a list of rules - user is able to specify any number of them: For each rule in the list user is able to set the action ( PAUSE / RESUME ) and the recurrence: If any schedule rule is configured for the launched active run - that run will be paused/restarted accordingly in the scheduled day and time. Also, users (who have permissions to pause/resume a run) can create/view/modify/delete schedule rules anytime run is active via the Run logs page: See more details here (item 5). Update pipe CLI version Previously, if the user installed pipe CLI to his local workstation, used it some time - and the Cloud Pipeline API version could be updated during this period - so the user had to manually perform complete re-installing of pipe CLI every time to have an actual version. Currently, the pipe update command to update the Cloud Pipeline CLI version was implemented. This command compare the CLI and API versions. If the CLI version is less than the API one, it will update the CLI - the latest Cloud Pipeline CLI version will be installed. Otherwise no actions will be performed: For more details see here . Blocking/unblocking users and groups In current version, the ability for administrators to set the \"blocked\" flag for the specific user or a whole group is implemented. This flag prevents user(s) to access Cloud Pipeline platform by the GUI. Administrators can block or unblock users/groups via the User management tab of the System Settings dashboard. For example, for a user: For the blocked user the special label will be displayed aside to the user's name: The blocked user won't be able to access Cloud Pipeline via the GUI. The error message will be displayed when trying to login: Note : any content generated by the blocked users is kept in the Cloud Pipeline, all the log trails on the users activities are kept as well. Only access to the platform is being restricted. To unblock click the UNBLOCK button that appeared instead of the BLOCK button: All these actions could be performed also to a group. Users from a blocked group will not have access to the platform. For more details about users blocking/unblocking see here . For more details about groups blocking/unblocking see here . Displaying additional node metrics at the Runs page In v0.16 , the displaying of high-level metrics information for the Active Runs is implemented: - \"Idle\" - this auxiliary label is shown when node's CPU consumption is lower than a certain level, defined by the admin. This label should attract the users attention cause such run may produce extra costs. - \"Pressure\" - this auxiliary label is shown when node's Memory/Disk consumption is higher than a certain level, defined by the admin. This label should attract the users attention cause such run may accidentally fail. These labels are displayed: at the Runs page at the Run logs page at the main dashboard (the ACTIVE RUNS panel) Note : if a user clicks this label from the Runs or the Run logs page the Cluster node Monitor will be opened to view the current node consumption. Admins can configure the emergence of these labels by the set of previous implemented system-level parameters. For the IDLE label: system.max.idle.timeout.minutes - specifies the duration in minutes how often the system should check node's activity system.idle.cpu.threshold - specifies the percentage of the CPU utilization, below which label will be displayed For the PRESSURE label: system.disk.consume.threshold - specifies the percentage of the node disk threshold above which the label will be displayed system.memory.consume.threshold - specifies the percentage of the node memory threshold above which the label will be displayed For more details see here . Export user list In the current version, the ability to export list of the platform users is implemented. This feature is available only for admins via User management tab of the system-level settings: Admin can download list with all users and their properties (user name, email, roles, attributes, state, etc.) by default or custom configure which properties should be exported: The downloaded file will have the .csv format. For more details see here . Displaying SSH link for the active runs in the Dashboard view Users can run SSH session over launched container. Previously, SSH link is only available from the Run logs page. In v0.16 , a new helpful capability was implemented that allows to open the SSH connection right from the Active Runs panel of the main Dashboard: That SSH link is available for all the non-paused active jobs (interactive and non-interactive) after all run's checks and initializations have passed (same as at the Run logs page). Enable Slurm workload manager for the Cloud Pipeline's clusters A new feature for the Cloud Pipeline's clusters was implemented in v0.16 . Now, Slurm can be configured within the Cluster tab. It is available only for the fixed size clusters. To enable this feature - tick the Enable Slurm checkbox and set the child nodes count at cluster settings. By default, this checkbox is unticked. Also users can manually enable Slurm functionality by the CP_CAP_SLURM system parameter: This feature allows you to use the full stack of Slurm cluster's commands to allocate the workload over the cluster, for example: For more information about using Slurm via the Cloud Pipeline see here . The ability to generate the pipe run command from the GUI A user has a couple of options to launch a new job in the Cloud Pipeline: API CLI GUI The easiest way to perform it is the GUI . But for automation purposes - the CLI is much more handy. Previously, users had to construct the commands manually, which made it hard to use. Now, the ability to automatically generate the CLI commands for job runs appeared in the GUI . Now, users can get a generated CLI command (that assembled all the run information): at the Launch form: at the Run logs form: Note : this button is available for completed runs too Once click these buttons - the popup with the corresponding pipe run command will appear: User can copy such command and paste it to the CLI for further launch. Also user can select the API tab in that popup and get the POST request for a job launch: See an example here . pipe CLI: view tools definitions In v0.16 the ability to view details of a tool/tool version or tools group via the CLI was implemented. The general command to perform these operations: pipe view-tools [OPTIONS] Via the options users can specify a Docker registry ( -r option), a tools group ( -g option), a tool ( -t option), a tool version ( -v option) and view the corresponding information. For example, to show a full tools list of a group: To show a tool definition with versions list: Also the specifying of \"path\" to the object (registry/group/tool) is supported. The \"full path\" format is: registry_name : port / group_name / tool_name : verion_name : For more details and usage examples see here . List the users/groups/objects permissions globally via pipe CLI Administrators may need to receive the following information - in a quick and convenient way: Which objects are accessible by a user? Which objects are accessible by a user group? Which user(s)/group(s) have access to the object? The lattest case was implemented early - see the command pipe view-acl . For other cases, new commands were implemented: pipe view-user-objects Username [OPTIONS] and pipe view-group-objects Groupname [OPTIONS] - to get a list of objects accessible by a user and by a user group/role respectively: Each of these commands has the non-required option -t ( --object-type ) OBJECT_TYPE - to restrict the output list of accessible objects only for the specific type (e.g., \"pipeline\" or \"tool\", etc.): For more details see: pipe view-user-objects and pipe view-group-objects . Storage usage statistics retrieval via pipe In some cases, it may be necessary to obtain an information about storage usage or some inner folder(s). In the current version, the command pipe storage du is implemented that provides \"disk usage\" information on the supported data storages/path: number of files in the storage/path summary size of the files in the storage/path In general, the command has the following format: pipe storage du [OPTIONS] [STORAGE] Without specifying any options and storage this command prints the full list of the available storages (both types - object and FS) with the \"usage\" information for each of them. With specifying the storage name this command prints the \"usage\" information only by that storage, e.g.: With -p ( --relative-path ) option the command prints the \"usage\" information for the specified path in the required storage, e.g.: With -d ( --depth ) option the command prints the \"usage\" information in the required storage (and path) for the specified folders nesting depth, e.g.: For more details about that command and full list of its options see here . GE Autoscaler respects CPU requirements of the job in the queue At the moment, GE Autoscaler treats each job in the queue as a single-core job. Previously, autoscale workers could have only fixed instance type (the same as the master) - that could lead to unschedulable jobs in the queue - for examle, if one of the jobs requested 4 slots in the local parallel environment within a 2 -cored machine. Described job waited in a queue forever (as autoscaler could setup only new 2 -cored nodes). In the current version the hybrid behavior for the GE Autoscaler was implemented, that allows processing the data even if the initial node type is not enough. That behavior allows to scale-up the cluster (attach a worker node) with the instance type distinct of the master - worker is being picked up based on the amount of unsatisfied CPU requirements of all pending jobs (according to required slots and parallel environment types). To enable hybrid mode for auto-scaled cluster set the corresponding checkbox in the cluster settings before the run: On the other hand, there are several System parameters to configure hybrid behavior in details: CP_CAP_AUTOSCALE_HYBRID ( boolean ) - enables the hybrid mode ( the same as the \"Enable Hybrid cluster\" checkbox setting ). In that mode the additional worker type can vary within either master instance type family (or CP_CAP_AUTOSCALE_HYBRID_FAMILY if specified). If disabled or not specified - the GE Autoscaler will work in a general regimen (when scaled-up workers have the same instance type as the master node) CP_CAP_AUTOSCALE_HYBRID_FAMILY ( string ) - defines the instance \"family\", from which the GE Autoscaler should pick up the worker node in case of hybrid behavior. If not specified (by default) - the GE Autoscaler will pick up worker instance from the same \"family\" as the master node CP_CAP_AUTOSCALE_HYBRID_MAX_CORE_PER_NODE ( string ) - determines the maximum number of instance cores for the node to be scaled up by the GE Autoscaler in case of hybrid behavior Also now, if no matching instance is present for the job (no matter - in hybrid or general regimen), GE Autoscaler logs error message and rejects such job: for example, when try to request 32 slots for the autoscaled cluster launched in hybrid mode with the parameter CP_CAP_AUTOSCALE_HYBRID_MAX_CORE_PER_NODE set to 20 : and in the logs console at the same time: For more details about GE Autoscaler see here . Restrictions of \"other\" users permissions for the storages mounted via the pipe storage mount command As was introduced in Release Notes v.0.15 , the ability to mount Cloud data storages (both - File Storages and Object Storages) to Linux and Mac workstations (requires FUSE installed) was added. For that, the pipe storage mount command was implemented. Previously, Cloud Pipeline allowed read access to the mounted cloud storages for the other users, by default. This might introduce a security issue when dealing with the sensitive data. In the current version, for the pipe storage mount command the new option is added: -m ( --mode ), that allows to set the permissions on the mountpoint at a mount time. Permissions are being configured by the numerical mask - similarly to chmod Linux command. E.g. to mount the storage with RW access to the OWNER , R access to the GROUP and no access to the OTHERS : If the option -m isn't specified - the default permission mask will be set - 700 (full access to the OWNER ( RWX ), no access to the GROUP and OTHERS ). For more details about mounting data storages via the pipe see here . The ability to find the tool by its version/package name Cloud Pipeline allows searching for the tools in the registry by its name or description. But in some cases, it is more convenient and useful to find which tool contains a specific software package and then use it. In the current version, this ability - to find a tool by its content - is implemented based on the global search capabilities. Now, via the Global Search, you may find a tool by its version name, e.g.: And by the package name (from any available ecosystem), e.g.: The ability to restrict which run statuses trigger the email notification Cloud Pipeline can be configured to send the email to the owner of the job (or specific users) if its status is changed. But in certain cases - it is not desired to get a notification about all changes of the run state. To reduce a number of the emails in these cases, the ability to configure, which statuses are triggering the notifications, is implemented in v0.16 . Now, when the administrator configures the emails sending linked to the run status changes - he can select specific run states that will trigger the notifications. It is done through the PIPELINE_RUN_STATUS section at the Email notifications tab of the System Settings (the \" Statuses to inform \" field): The email notifications will be sent only if the run enters one of the selected states. Note : if no statuses are selected in the \" Statuses to inform \" field - email notifications will be sent as previously - for all status changes. For more information how to configure the email notifications see here . The ability to force the usage of the specific Cloud Provider/Region for a given image Previously, the platform allowed to select a Cloud Provider (and Cloud Region ) for a particular job execution via the Launch Form, but a tool/version itself didn't not have any link with a region. In certain cases, it's necessary to enforce users to run some tools in a specific Cloud Provider / Region . In the current version, such ability was implemented. The Tool / Version Settings forms contain the field for specifying a Cloud Region , e.g.: By default, this parameter has Not configured value. This means, that a tool will be launched in a Default region (configured by the Administrator in the global settings). Or a user can set any allowed Cloud Region / Provider manually. This behavior will be the same as previously. Admin or a tool owner can forcibly set a specific Cloud Region / Provider where the run shall be launched, e.g.: Then, if a specific Cloud Region / Provider is configured - users will have to use it, when launching a tool (regardless of how the launch was started - with default or custom settings): And if a user does not have access to that Cloud Region / Provider - tool won't launch: Note : if a specific Cloud Region / Provider is being specified for the Tool, in general - this action enforce the Region / Provider only for the latest version of that tool. For other versions the settings will remain previous. See for more details about tool execution settings here . See for more details about tool version execution settings here . Restrict mounting of data storages for a given Cloud Provider Previously, Cloud Pipeline attempted to mount all data storages available for the user despite the Cloud Providers / Regions of these storages. E.g. if a job was launched in the GCP , but the user has access to AWS S3 buckets - they were also mounted to the GCP instance. In the current version, the ability to restrict storage mount availability for a run, based on its Cloud Provider / Region , was implemented. Cloud Regions system configuration now has a separate parameter \" Mount storages across other regions \": This parameter has 3 possible values: None - if set, storages from this region will be unavailable for a mount to any jobs. Such storages will not be available even to the same regions (e.g. storage from AWS us-east-1 will be unavailable for a mount to instances launched in AWS eu-central-1 or any GCP region and even in AWS us-east-1 ) Same Cloud - if set, storages from this region will be available only to different Cloud Regions of the same Cloud Provider (e.g. storage from AWS us-east-1 will be available to instances launched in AWS eu-central-1 too, but not in any GCP region) All - if set, storages from this region will be available to all other Cloud Regions / Providers Ability to \"symlink\" the tools between the tools groups The majority of the tools are managed by the administrators and are available via the library tool group. But for some of the users it would be convenient to have separate tool groups, which are going to contain a mix of the custom tools (managed by the users themselves) and the library tools (managed by the admins). For the latter ones the ability to create \" symlinks \" into the other tool groups was implemented. \"Symlinked\" tools are displayed in that users' tool groups as the original tools but can't be edited/updated. When a run is started with \"symlinked\" tool as docker image it is being replaced with original image for Kubernetes pod spec. Example of the \"symlinked\" ubuntu tool: The following behavior is implemented: to create a \"symlink\" to the tool, the user shall have READ access to the source tool and WRITE access to the destination tool group for the \"symlinked\" tool all the same description, icon, settings as in the source image are displayed. It isn't possible to make any changes to the \"symlink\" data (description, icon, settings. attributes, issues, etc.), even for the admins admins and image OWNERs are able to manage the permissions for the \"symlinks\". Permissions on the \"symlinked\" tools are configured separately from the original tool two levels of \"symlinks\" is not possible (\"symlink\" to the \"symlinked\" tool can't be created) it isn't possible to \"push\" into the \"symlinked\" tool For more details see here . Notable Bug fixes Parameter values changes cannot be saved for a tool #871 Previously, if a user changed only the parameter value within a tool's settings form - the SAVE button stayed still unavailable. One had to modify some other option (e.g. disk size) to save the overall changes. Packages are duplicated in the tool's version details #843 Previously, certain tools packages were duplicated in the PACKAGES details page of the tools version. Autoscaling cluster can be autopaused #819 Previously, when users launched auto-scaled clusters without default child-nodes and the PAUSE action was specified as action for the \"idle\" run (via system-levels settings), such cluster runs could be paused. Any cluster runs shall not have the ability to be paused, only stopped. pipe : not-handled error while trying to execute commands with invalid config #750 Previously, if pipe config contained some invalid data (e.g. outdated or invalid access token), then trying to execute any pipe command had been causing an not-handled error. Setting of the tool icon size #493 Previously, setting of any value for the maximum tool's icon size via the sytem-level preference misc.max.tool.icon.size.kb didn't lead to anything - restriction for the size while trying to change an icon was remaining the same - 50 Kb. NPE while building cloud-specific environment variables for run #486 For each run a set of cloud-specific environment variables (including account names, credentials, etc.) is build. This functionality resulted to fails with NPE when some of these variables are null . Now, such null variables are filtered out with warn logs. Worker nodes fail due to mismatch of the regions with the parent run #485 In certain cases, when a new child run was launching in cluster, cloud region was not specified directly and it might be created in a region differing from the parent run, that could lead to fails. Now, worker runs inherit parent's run cloud region. Worker nodes shall not be restarted automatically #483 Cloud Pipeline has a functionality to restart so called batch job runs automatically when run is terminated due to some technical issues, e.g. spot instance termination. Previously, runs that were created as child nodes for some parent run were also restarted. Now, automatically child reruns for the described cases with the batch job runs are rejected. Uploaded storage file content is downloaded back to client #478 Cloud Pipeline clients use specific POST API method to upload local files to the cloud storages. Previously, this method not only uploaded files to the cloud storage but also mistakenly returned uploaded file content back to the client. It led to a significant upload time increase. GUI improperly works with detached configurations in a non-default region #476 Saved instance type of a non-default region in a detached configuration wasn't displayed in case when such configuration was reopened (instance type field was displayed as empty in that cases). Detached configuration doesn't respect region setting #458 Region setting was not applied when pipeline is launched using detached configuration. Now, cloud region ID is merged into the detached configuration settings. Incorrect behavior of the \"Transfer to the cloud\" form in case when a subfolder has own metadata #434 Previously, when you tried to download files from external resources using metadata (see here ) and in that metadata's folder there was any subfolder with its own metadata - on the \"Transfer to the Cloud\" form attributes (columns) of both metadata files were mistakenly displaying. Incorrect displaying of the \"Start idle\" checkbox #418 If for the configuration form with several tabs user was setting the Start idle checkbox on any tab and then switched between sub-configurations tabs - the \"checked\" state of the Start idle checkbox didn't change, even if Cmd template field was appearing with its value (these events are mutually exclusive). Limit check of the maximum cluster size is incorrect #412 Maximum allowed number of runs (size of the cluster) created at once is limited by system preference launch.max.scheduled.number . This check used strictly \"less\" check rather then \"less or equal\" to allow or deny cluster launch. Now, the \"less or equal\" check is used. Fixed cluster with SGE and DIND capabilities fails to start #392 Previously, fixed cluster with both CP_CAP_SGE and CP_CAP_DIND_CONTAINER options enabled with more than one worker failed to start. Some of the workers failed on either SGEWorkerSetup or SetupDind task with different errors. Scripts were executed in the same one shared analysis directory. So, some workers could delete files downloaded by other workers. Azure: Server shall check Azure Blob existence when a new storage is created #768 During the creation of AZ Storage, the validation whether Azure Blob exists or not didn't perform. In that case, if Azure Blob had already existed, the user was getting failed request with Azure exception. Azure: pipe CLI cannot transfer empty files between storages #386 Previously, empty files couldn't be transferred within a single Azure storage or between two Azure storages using pipe CLI, it throwed an error. So for example, a folder that contained empty files couldn't be copied correctly. Azure: runs with enabled GE autoscaling doesn't stop #377 All Azure runs with enabled GE autoscaling were stuck after the launch.sh script has finished its execution. Daemon GE autoscaler process kept container alive. It was caused by the run process stdout and stderr aren't handled the same way for different Cloud Provider. So background processes launched from launch.sh directly could prevent Azure run finalization. Incorrect behavior while download files from external resources into several folders #373 If user was tried to download files from external resources and at the Transfer settings form was set Create folders for each path field checkbox without setting any name field, all files downloaded into one folder without creating folders for each path field (column). Detach configuration doesn't setup SGE for a single master run #342 Grid Engine installation was mistakenly being skipped, if pipeline was launched with enabled system parameter CP_CAP_SGE via a detach configuration. Broken layouts #747 , #834 Previously, pipeline versions page had broken layout if there \"Attributes\" and \"Issues\" panels were simultaneously opened. If there were a lot of node labels at the Cluster nodes page, some of them were \"broken\" and spaced to different lines. Some of the other page layouts also were broken.","title":"v.0.16"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#cloud-pipeline-v016-release-notes","text":"Google Cloud Platform Support System logs Displaying Cloud Provider's icon Configurable timeout of GE Autoscale waiting Storage mounts data transfer restrictor Extended recursive symlinks handling Displaying of the latest commit date/time Renaming of the GitLab repository in case of Pipeline renaming Pushing pipeline changes to the GitLab on behalf of the user Allowing to expose compute node FS to upload and download files Resource usage form improvement View the historical resources utilization Ability to schedule automatic pause/restart of the running jobs Update pipe CLI version Blocking/unblocking users and groups Displaying additional node metrics Export user list Displaying SSH link for the active runs in the Dashboard view Enable Slurm for the Cloud Pipeline's clusters The ability to generate the pipe run command from the GUI pipe CLI: view tools definitions List the users/groups/objects permissions globally via pipe CLI Storage usage statistics retrieval via pipe GE Autoscaler respects CPU requirements of the job in the queue Restrictions of \"other\" users permissions for the mounted storage Search the tool by its version/package name The ability to restrict which run statuses trigger the email notification The ability to force the specific Cloud Provider for an image Restrict mounting of data storages for a given Cloud Provider Ability to \"symlink\" the tools between the tools groups Notable Bug fixes Parameter values changes cannot be saved for a tool Packages are duplicated in the tool's version details Autoscaling cluster can be autopaused pipe : not-handled error while trying to execute commands with invalid config Setting of the tool icon size NPE while building cloud-specific environment variables for run Worker nodes fail due to mismatch of the regions with the parent run Worker nodes shall not be restarted automatically Uploaded storage file content is downloaded back to client GUI improperly works with detached configurations in a non-default region Detached configuration doesn't respect region setting Incorrect behavior of the \"Transfer to the cloud\" form in case when a subfolder has own metadata Incorrect displaying of the \"Start idle\" checkbox Limit check of the maximum cluster size is incorrect Fixed cluster with SGE and DIND capabilities fails to start Azure: Server shall check Azure Blob existence when a new storage is created Azure: pipe CLI cannot transfer empty files between storages Azure: runs with enabled GE autoscaling doesn't stop Incorrect behavior while download files from external resources into several folders Detach configuration doesn't setup SGE for a single master run Broken layouts","title":"Cloud Pipeline v.0.16 - Release notes"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#google-cloud-platform-support","text":"One of the major v0.16 features is a support for the Google Cloud Platform . All the features, that were previously used for AWS and Azure , are now available in all the same manner, from all the same GUI/CLI, for GCP . This provides an even greater level of a flexibility to launch different jobs in the locations, closer to the data, with cheaper prices or better compute hardware in depend on a specific task.","title":"Google Cloud Platform Support"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#system-logs","text":"In the current version, the \"Security Logging\" was implemented. Now, the system records audit trail events: users' authentication attempts users' profiles modifications platform objects' permissions management access to interactive applications from pipeline runs other platform functionality features Logs are collected/managed at the Elasticsearch node and backed up to the object storage (that could be configured during the platform deployment). The administrator can view/filter these logs via the GUI - in the System-level settings, e.g.: Each record in the logs list contains: Field Description Date The date and time of the log event Log status The status of the log message ( INFO , ERROR , etc.) Log message Description of the log event User User name who performed the event Service Service name that registered the event ( api-srv , edge ) Type Log message type (currently, only security type is available) For more details see here .","title":"System logs"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#displaying-cloud-providers-icon-for-the-storagecompute-resources","text":"As presented in v0.15 , Cloud Pipeline can manage multi Cloud Providers in a single installation. In the current version, useful icon-hints with the information about using Cloud Provider are introduced. If a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - corresponding icons/text information are displaying next to the cloud resource. Such cloud resources are: Object/File Storages (icons in the Library , at the \"DATA\" panel of the Dashboard etc.) Regions (icons in the Cloud Regions configuration, at the Launch form etc.) Running jobs : text hints (at the RUNS page) icons (at the Run logs page, at the \"RUNS\" panels of the Dashboard ) Note : this feature is not available for deployments with a single Cloud Provider. Examples of displaying Cloud Region icons/info see in sections 6. Manage Pipeline , 7. Manage Detached configuration , 8. Manage Data Storage and 18. Home page .","title":"Displaying Cloud Provider's icon for the storage/compute resources"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#configurable-timeout-of-ge-autoscale-waiting-for-a-worker-node-up","text":"Previously, GE Autoscaler waited for a worker node up for a fixed timeout. This could lead to incorrect behavior for specific CLoud Providers , because the timeout can be very different. Current version extracts GE Autoscaler polling timeout to a new system preference ge.autoscaling.scale.up.polling.timeout . That preference defines how many seconds GE Autoscaler should wait for pod initialization and run initialization . Default value is 600 seconds ( 10 minutes).","title":"Configurable timeout of GE Autoscale waiting for a worker node up"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#storage-mounts-data-transfer-restrictor","text":"Users may perform cp or mv operations of the large files (50+ Gb) to and from the fuse-mounted storages. It is uncovered that such operations are not handled properly within the fuse implementations. Commands may hang for a long timeout or produce zero-sized result. The suggested more graceful approach for the copying of the large files is to use pipe cp / pipe mv commands, which are behaving correctly for the huge volumes. To avoid users of performing usual cp or mv commands for operations with the large files - now, Cloud Pipeline warns them about possible errors and suggest to use corresponding pipe commands. Specified approach is implemented in the following manner: if a cp / mv command is called with the source/dest pointing to the storage (e.g. /cloud-data/ storage_path /... ) - the overall size of the data being transferred is checked - if that size is greater than allowed, a warning message will be shown, e.g.: this warning doesn't abort the user's command execution, it is continued appearance of this warning is configured by the following launch environment variables (values of these variables could be set only by admins via system-level settings): CP_ALLOWED_MOUNT_TRANSFER_SIZE - sets number of gigabytes that is allowed to be transferred without warning. By default 50 Gb . CP_ALLOWED_MOUNT_TRANSFER_SIZE_TIMEOUT - sets number of seconds that the transfer size retrieving operation can take. By default 5 seconds . CP_ALLOWED_MOUNT_TRANSFER_FILES - sets number of files that is allowed to be transferred without warning. Supported only for Azure Cloud Provider. By default 100 files . Note : this feature is not available for NFS / SMB mounts, only for object storages.","title":"Storage mounts data transfer restrictor"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#extended-recursive-symlinks-handling","text":"There could be specific cases when some services execute tasks using the on-prem storages, where \"recursive\" symlinks are presented. This causes the Cloud Pipeline Data transfer service to follow symlinks infinitely. In v0.16 , a new feature is introduced for Data transfer service to detect such issues and skip the upload for files/folders, that cause infinite loop over symlinks. A new option -sl ( --symlinks ) was added to the pipe storage cp / mv operations to handle symlinks (for local source) with the following possible values: follow - follow symlinks ( default ) skip - do not follow symlinks filter - follow symlinks but check for cyclic links and skip them Example for the folder with recursive and non-recursive symbolic links: Also options were added to the Data transfer service to set symlink policy for transfer operations. For more details about pipe storage cp / mv operations see here .","title":"Extended recursive symlinks handling"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#displaying-of-the-latest-commit-datetime","text":"Users can modify existing tools and then commit them to save performed changes. It can be done by COMMIT button on the run's details page: Previously, if user committed some tool, only commit status was shown on the run's details page. In the current version, displaying of the date/time for the tool latest commit is added: For more details about tool commit see here .","title":"Displaying of the latest commit date/time"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#renaming-of-the-gitlab-repository-in-case-of-pipeline-renaming","text":"Pipeline in the Cloud Pipeline environment is a workflow script with versioned source code, documentation, and configuration. Under the hood, it is a git repository. Previously, if the Pipeline object was renamed - the underlying GitLab repository was keeping the previous name. In the current version, if user renames a Pipeline the corresponding GitLab repository will be also automatically renamed: Need to consider in such case that the clone/pull/push URL changes too. Make sure to change the remote address, if you use the Pipeline somewhere else. For more details see here .","title":"Renaming of the GitLab repository in case of Pipeline renaming"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#pushing-pipeline-changes-to-the-gitlab-on-behalf-of-the-user","text":"If the user saves the changed Pipeline object - it actually means that a new commit is created and pushed to the corresponding GitLab repo. Previously, all the commits pushed to the GitLab via the Cloud Pipeline GUI were made on behalf of the service account . This could break traceability of the changes. In current version, the author of the commit is displayed in the Web GUI (for all Pipeline versions - the draft and released), the commits are performed on behalf of the real user:","title":"Pushing pipeline changes to the GitLab on behalf of the user"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#allowing-to-expose-compute-node-fs-to-upload-and-download-files","text":"For the interactive runs users are processing data in ad-hoc manner, which requires upload data from the local storage to the cloud and download results from the cloud to the local storage. Cloud Pipeline supports a number of options for the user to perform that data transfers for the interactive runs: via the Object Storage (using Web GUI or CLI) via the File Storage (using Web GUI, CLI, WebDav-mounted-drive) Previously, view, download, upload and delete operations required an intermediate location (bucket or fs) to be used. It might confuse user when a small dataset shall be loaded to the specific location within a run's filesystem. In the current version, direct exposure of the run's filesystem is supported. The BROWSE hyperlink is displayed on the Run logs page after a job had been initialized: User can click the link and a Storage browser Web GUI will be loaded: User is able to: view files and directories download and delete files and directories upload files and directories search files and directories For more details see here .","title":"Allowing to expose compute node FS to upload and download files"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#resource-usage-form-improvement","text":"In v0.16 , the number of filters were added to the Monitor cluster nodes feature: Common range for all charts User can synchronize the time period for all plots. To do so user should mark the \"Common range for all charts\" filter. If this filter is unmarked, user can zoom any plot without any change for others. Live update The plots data will be updated every 5 seconds in a real-time manner. The fields with dates will be updated as well. Set range User can select the predefined time range for all plots from the list: Whole range Last week Last day Last hour Date filter User can specify the Start and the End dates for plots. The system will substitute the node creating date as the Start date and current date for the End date, if user doesn't select anything. All filters are working for all plots simultaneously: data for all plots will be dynamically updated as soon as the user changes filter value. For more details see here .","title":"Resource usage form improvement"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#allow-to-view-the-historical-resources-utilization","text":"Another convenient feature that was implemented in v0.16 linked to the Cluster nodes monitor is viewing of the detailed history of the resource utilization for any jobs. Previously, it was available only for active jobs. Now, users can view the utilization data even for completed (succeed/stopped/failed) jobs for debugging/optimization purposes. The utilization data for all runs is stored for a preconfigured period of time that is set by the system preference system.resource.monitoring.stats.retention.period (defaults to 5 days). I.e. if the job has been stopped and the specified time period isn't over - the user can access to the resources utilization data of that job: Open the Run logs page for the completed job: Click the node name hyperlink The Monitor page of the node resources utilization will be opened: Also now, users have the ability to export the utilization information into a .csv file. This is required, if the user wants to keep locally the information for a longer period of time than defined by system.resource.monitoring.stats.retention.period : The user can select the interval for the utilization statistics output and export the corresponding file. For more details see here .","title":"Allow to view the historical resources utilization"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#ability-to-schedule-automatic-pauserestart-of-the-running-jobs","text":"For certain use cases (e.g. when Cloud Pipeline is used as a development/research environment) users can launch jobs and keep them running all the time, including weekends and holidays. To reduce costs, in the current version, the ability to set a Run schedule was implemented. This feature allows to automatically pause/resume runs, based on the configuration specified. This feature is applied only to the \"Pausable\" runs (i.e. \"On-demand\" and non-cluster): The user (who has permissions to pause/resume a run) is able to set a schedule for a run being launched: Schedule is defined as a list of rules - user is able to specify any number of them: For each rule in the list user is able to set the action ( PAUSE / RESUME ) and the recurrence: If any schedule rule is configured for the launched active run - that run will be paused/restarted accordingly in the scheduled day and time. Also, users (who have permissions to pause/resume a run) can create/view/modify/delete schedule rules anytime run is active via the Run logs page: See more details here (item 5).","title":"Ability to schedule automatic pause/restart of the running jobs"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#update-pipe-cli-version","text":"Previously, if the user installed pipe CLI to his local workstation, used it some time - and the Cloud Pipeline API version could be updated during this period - so the user had to manually perform complete re-installing of pipe CLI every time to have an actual version. Currently, the pipe update command to update the Cloud Pipeline CLI version was implemented. This command compare the CLI and API versions. If the CLI version is less than the API one, it will update the CLI - the latest Cloud Pipeline CLI version will be installed. Otherwise no actions will be performed: For more details see here .","title":"Update pipe CLI version"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#blockingunblocking-users-and-groups","text":"In current version, the ability for administrators to set the \"blocked\" flag for the specific user or a whole group is implemented. This flag prevents user(s) to access Cloud Pipeline platform by the GUI. Administrators can block or unblock users/groups via the User management tab of the System Settings dashboard. For example, for a user: For the blocked user the special label will be displayed aside to the user's name: The blocked user won't be able to access Cloud Pipeline via the GUI. The error message will be displayed when trying to login: Note : any content generated by the blocked users is kept in the Cloud Pipeline, all the log trails on the users activities are kept as well. Only access to the platform is being restricted. To unblock click the UNBLOCK button that appeared instead of the BLOCK button: All these actions could be performed also to a group. Users from a blocked group will not have access to the platform. For more details about users blocking/unblocking see here . For more details about groups blocking/unblocking see here .","title":"Blocking/unblocking users and groups"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#displaying-additional-node-metrics-at-the-runs-page","text":"In v0.16 , the displaying of high-level metrics information for the Active Runs is implemented: - \"Idle\" - this auxiliary label is shown when node's CPU consumption is lower than a certain level, defined by the admin. This label should attract the users attention cause such run may produce extra costs. - \"Pressure\" - this auxiliary label is shown when node's Memory/Disk consumption is higher than a certain level, defined by the admin. This label should attract the users attention cause such run may accidentally fail. These labels are displayed: at the Runs page at the Run logs page at the main dashboard (the ACTIVE RUNS panel) Note : if a user clicks this label from the Runs or the Run logs page the Cluster node Monitor will be opened to view the current node consumption. Admins can configure the emergence of these labels by the set of previous implemented system-level parameters. For the IDLE label: system.max.idle.timeout.minutes - specifies the duration in minutes how often the system should check node's activity system.idle.cpu.threshold - specifies the percentage of the CPU utilization, below which label will be displayed For the PRESSURE label: system.disk.consume.threshold - specifies the percentage of the node disk threshold above which the label will be displayed system.memory.consume.threshold - specifies the percentage of the node memory threshold above which the label will be displayed For more details see here .","title":"Displaying additional node metrics at the Runs page"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#export-user-list","text":"In the current version, the ability to export list of the platform users is implemented. This feature is available only for admins via User management tab of the system-level settings: Admin can download list with all users and their properties (user name, email, roles, attributes, state, etc.) by default or custom configure which properties should be exported: The downloaded file will have the .csv format. For more details see here .","title":"Export user list"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#displaying-ssh-link-for-the-active-runs-in-the-dashboard-view","text":"Users can run SSH session over launched container. Previously, SSH link is only available from the Run logs page. In v0.16 , a new helpful capability was implemented that allows to open the SSH connection right from the Active Runs panel of the main Dashboard: That SSH link is available for all the non-paused active jobs (interactive and non-interactive) after all run's checks and initializations have passed (same as at the Run logs page).","title":"Displaying SSH link for the active runs in the Dashboard view"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#enable-slurm-workload-manager-for-the-cloud-pipelines-clusters","text":"A new feature for the Cloud Pipeline's clusters was implemented in v0.16 . Now, Slurm can be configured within the Cluster tab. It is available only for the fixed size clusters. To enable this feature - tick the Enable Slurm checkbox and set the child nodes count at cluster settings. By default, this checkbox is unticked. Also users can manually enable Slurm functionality by the CP_CAP_SLURM system parameter: This feature allows you to use the full stack of Slurm cluster's commands to allocate the workload over the cluster, for example: For more information about using Slurm via the Cloud Pipeline see here .","title":"Enable Slurm workload manager for the Cloud Pipeline's clusters"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#the-ability-to-generate-the-pipe-run-command-from-the-gui","text":"A user has a couple of options to launch a new job in the Cloud Pipeline: API CLI GUI The easiest way to perform it is the GUI . But for automation purposes - the CLI is much more handy. Previously, users had to construct the commands manually, which made it hard to use. Now, the ability to automatically generate the CLI commands for job runs appeared in the GUI . Now, users can get a generated CLI command (that assembled all the run information): at the Launch form: at the Run logs form: Note : this button is available for completed runs too Once click these buttons - the popup with the corresponding pipe run command will appear: User can copy such command and paste it to the CLI for further launch. Also user can select the API tab in that popup and get the POST request for a job launch: See an example here .","title":"The ability to generate the pipe run command from the GUI"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#pipe-cli-view-tools-definitions","text":"In v0.16 the ability to view details of a tool/tool version or tools group via the CLI was implemented. The general command to perform these operations: pipe view-tools [OPTIONS] Via the options users can specify a Docker registry ( -r option), a tools group ( -g option), a tool ( -t option), a tool version ( -v option) and view the corresponding information. For example, to show a full tools list of a group: To show a tool definition with versions list: Also the specifying of \"path\" to the object (registry/group/tool) is supported. The \"full path\" format is: registry_name : port / group_name / tool_name : verion_name : For more details and usage examples see here .","title":"pipe CLI: view tools definitions"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#list-the-usersgroupsobjects-permissions-globally-via-pipe-cli","text":"Administrators may need to receive the following information - in a quick and convenient way: Which objects are accessible by a user? Which objects are accessible by a user group? Which user(s)/group(s) have access to the object? The lattest case was implemented early - see the command pipe view-acl . For other cases, new commands were implemented: pipe view-user-objects Username [OPTIONS] and pipe view-group-objects Groupname [OPTIONS] - to get a list of objects accessible by a user and by a user group/role respectively: Each of these commands has the non-required option -t ( --object-type ) OBJECT_TYPE - to restrict the output list of accessible objects only for the specific type (e.g., \"pipeline\" or \"tool\", etc.): For more details see: pipe view-user-objects and pipe view-group-objects .","title":"List the users/groups/objects permissions globally via pipe CLI"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#storage-usage-statistics-retrieval-via-pipe","text":"In some cases, it may be necessary to obtain an information about storage usage or some inner folder(s). In the current version, the command pipe storage du is implemented that provides \"disk usage\" information on the supported data storages/path: number of files in the storage/path summary size of the files in the storage/path In general, the command has the following format: pipe storage du [OPTIONS] [STORAGE] Without specifying any options and storage this command prints the full list of the available storages (both types - object and FS) with the \"usage\" information for each of them. With specifying the storage name this command prints the \"usage\" information only by that storage, e.g.: With -p ( --relative-path ) option the command prints the \"usage\" information for the specified path in the required storage, e.g.: With -d ( --depth ) option the command prints the \"usage\" information in the required storage (and path) for the specified folders nesting depth, e.g.: For more details about that command and full list of its options see here .","title":"Storage usage statistics retrieval via pipe"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#ge-autoscaler-respects-cpu-requirements-of-the-job-in-the-queue","text":"At the moment, GE Autoscaler treats each job in the queue as a single-core job. Previously, autoscale workers could have only fixed instance type (the same as the master) - that could lead to unschedulable jobs in the queue - for examle, if one of the jobs requested 4 slots in the local parallel environment within a 2 -cored machine. Described job waited in a queue forever (as autoscaler could setup only new 2 -cored nodes). In the current version the hybrid behavior for the GE Autoscaler was implemented, that allows processing the data even if the initial node type is not enough. That behavior allows to scale-up the cluster (attach a worker node) with the instance type distinct of the master - worker is being picked up based on the amount of unsatisfied CPU requirements of all pending jobs (according to required slots and parallel environment types). To enable hybrid mode for auto-scaled cluster set the corresponding checkbox in the cluster settings before the run: On the other hand, there are several System parameters to configure hybrid behavior in details: CP_CAP_AUTOSCALE_HYBRID ( boolean ) - enables the hybrid mode ( the same as the \"Enable Hybrid cluster\" checkbox setting ). In that mode the additional worker type can vary within either master instance type family (or CP_CAP_AUTOSCALE_HYBRID_FAMILY if specified). If disabled or not specified - the GE Autoscaler will work in a general regimen (when scaled-up workers have the same instance type as the master node) CP_CAP_AUTOSCALE_HYBRID_FAMILY ( string ) - defines the instance \"family\", from which the GE Autoscaler should pick up the worker node in case of hybrid behavior. If not specified (by default) - the GE Autoscaler will pick up worker instance from the same \"family\" as the master node CP_CAP_AUTOSCALE_HYBRID_MAX_CORE_PER_NODE ( string ) - determines the maximum number of instance cores for the node to be scaled up by the GE Autoscaler in case of hybrid behavior Also now, if no matching instance is present for the job (no matter - in hybrid or general regimen), GE Autoscaler logs error message and rejects such job: for example, when try to request 32 slots for the autoscaled cluster launched in hybrid mode with the parameter CP_CAP_AUTOSCALE_HYBRID_MAX_CORE_PER_NODE set to 20 : and in the logs console at the same time: For more details about GE Autoscaler see here .","title":"GE Autoscaler respects CPU requirements of the job in the queue"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#restrictions-of-other-users-permissions-for-the-storages-mounted-via-the-pipe-storage-mount-command","text":"As was introduced in Release Notes v.0.15 , the ability to mount Cloud data storages (both - File Storages and Object Storages) to Linux and Mac workstations (requires FUSE installed) was added. For that, the pipe storage mount command was implemented. Previously, Cloud Pipeline allowed read access to the mounted cloud storages for the other users, by default. This might introduce a security issue when dealing with the sensitive data. In the current version, for the pipe storage mount command the new option is added: -m ( --mode ), that allows to set the permissions on the mountpoint at a mount time. Permissions are being configured by the numerical mask - similarly to chmod Linux command. E.g. to mount the storage with RW access to the OWNER , R access to the GROUP and no access to the OTHERS : If the option -m isn't specified - the default permission mask will be set - 700 (full access to the OWNER ( RWX ), no access to the GROUP and OTHERS ). For more details about mounting data storages via the pipe see here .","title":"Restrictions of \"other\" users permissions for the storages mounted via the pipe storage mount command"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#the-ability-to-find-the-tool-by-its-versionpackage-name","text":"Cloud Pipeline allows searching for the tools in the registry by its name or description. But in some cases, it is more convenient and useful to find which tool contains a specific software package and then use it. In the current version, this ability - to find a tool by its content - is implemented based on the global search capabilities. Now, via the Global Search, you may find a tool by its version name, e.g.: And by the package name (from any available ecosystem), e.g.:","title":"The ability to find the tool by its version/package name"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#the-ability-to-restrict-which-run-statuses-trigger-the-email-notification","text":"Cloud Pipeline can be configured to send the email to the owner of the job (or specific users) if its status is changed. But in certain cases - it is not desired to get a notification about all changes of the run state. To reduce a number of the emails in these cases, the ability to configure, which statuses are triggering the notifications, is implemented in v0.16 . Now, when the administrator configures the emails sending linked to the run status changes - he can select specific run states that will trigger the notifications. It is done through the PIPELINE_RUN_STATUS section at the Email notifications tab of the System Settings (the \" Statuses to inform \" field): The email notifications will be sent only if the run enters one of the selected states. Note : if no statuses are selected in the \" Statuses to inform \" field - email notifications will be sent as previously - for all status changes. For more information how to configure the email notifications see here .","title":"The ability to restrict which run statuses trigger the email notification"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#the-ability-to-force-the-usage-of-the-specific-cloud-providerregion-for-a-given-image","text":"Previously, the platform allowed to select a Cloud Provider (and Cloud Region ) for a particular job execution via the Launch Form, but a tool/version itself didn't not have any link with a region. In certain cases, it's necessary to enforce users to run some tools in a specific Cloud Provider / Region . In the current version, such ability was implemented. The Tool / Version Settings forms contain the field for specifying a Cloud Region , e.g.: By default, this parameter has Not configured value. This means, that a tool will be launched in a Default region (configured by the Administrator in the global settings). Or a user can set any allowed Cloud Region / Provider manually. This behavior will be the same as previously. Admin or a tool owner can forcibly set a specific Cloud Region / Provider where the run shall be launched, e.g.: Then, if a specific Cloud Region / Provider is configured - users will have to use it, when launching a tool (regardless of how the launch was started - with default or custom settings): And if a user does not have access to that Cloud Region / Provider - tool won't launch: Note : if a specific Cloud Region / Provider is being specified for the Tool, in general - this action enforce the Region / Provider only for the latest version of that tool. For other versions the settings will remain previous. See for more details about tool execution settings here . See for more details about tool version execution settings here .","title":"The ability to force the usage of the specific Cloud Provider/Region for a given image"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#restrict-mounting-of-data-storages-for-a-given-cloud-provider","text":"Previously, Cloud Pipeline attempted to mount all data storages available for the user despite the Cloud Providers / Regions of these storages. E.g. if a job was launched in the GCP , but the user has access to AWS S3 buckets - they were also mounted to the GCP instance. In the current version, the ability to restrict storage mount availability for a run, based on its Cloud Provider / Region , was implemented. Cloud Regions system configuration now has a separate parameter \" Mount storages across other regions \": This parameter has 3 possible values: None - if set, storages from this region will be unavailable for a mount to any jobs. Such storages will not be available even to the same regions (e.g. storage from AWS us-east-1 will be unavailable for a mount to instances launched in AWS eu-central-1 or any GCP region and even in AWS us-east-1 ) Same Cloud - if set, storages from this region will be available only to different Cloud Regions of the same Cloud Provider (e.g. storage from AWS us-east-1 will be available to instances launched in AWS eu-central-1 too, but not in any GCP region) All - if set, storages from this region will be available to all other Cloud Regions / Providers","title":"Restrict mounting of data storages for a given Cloud Provider"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#ability-to-symlink-the-tools-between-the-tools-groups","text":"The majority of the tools are managed by the administrators and are available via the library tool group. But for some of the users it would be convenient to have separate tool groups, which are going to contain a mix of the custom tools (managed by the users themselves) and the library tools (managed by the admins). For the latter ones the ability to create \" symlinks \" into the other tool groups was implemented. \"Symlinked\" tools are displayed in that users' tool groups as the original tools but can't be edited/updated. When a run is started with \"symlinked\" tool as docker image it is being replaced with original image for Kubernetes pod spec. Example of the \"symlinked\" ubuntu tool: The following behavior is implemented: to create a \"symlink\" to the tool, the user shall have READ access to the source tool and WRITE access to the destination tool group for the \"symlinked\" tool all the same description, icon, settings as in the source image are displayed. It isn't possible to make any changes to the \"symlink\" data (description, icon, settings. attributes, issues, etc.), even for the admins admins and image OWNERs are able to manage the permissions for the \"symlinks\". Permissions on the \"symlinked\" tools are configured separately from the original tool two levels of \"symlinks\" is not possible (\"symlink\" to the \"symlinked\" tool can't be created) it isn't possible to \"push\" into the \"symlinked\" tool For more details see here .","title":"Ability to \"symlink\" the tools between the tools groups"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#notable-bug-fixes","text":"","title":"Notable Bug fixes"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#parameter-values-changes-cannot-be-saved-for-a-tool","text":"#871 Previously, if a user changed only the parameter value within a tool's settings form - the SAVE button stayed still unavailable. One had to modify some other option (e.g. disk size) to save the overall changes.","title":"Parameter values changes cannot be saved for a tool"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#packages-are-duplicated-in-the-tools-version-details","text":"#843 Previously, certain tools packages were duplicated in the PACKAGES details page of the tools version.","title":"Packages are duplicated in the tool's version details"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#autoscaling-cluster-can-be-autopaused","text":"#819 Previously, when users launched auto-scaled clusters without default child-nodes and the PAUSE action was specified as action for the \"idle\" run (via system-levels settings), such cluster runs could be paused. Any cluster runs shall not have the ability to be paused, only stopped.","title":"Autoscaling cluster can be autopaused"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#pipe-not-handled-error-while-trying-to-execute-commands-with-invalid-config","text":"#750 Previously, if pipe config contained some invalid data (e.g. outdated or invalid access token), then trying to execute any pipe command had been causing an not-handled error.","title":"pipe: not-handled error while trying to execute commands with invalid config"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#setting-of-the-tool-icon-size","text":"#493 Previously, setting of any value for the maximum tool's icon size via the sytem-level preference misc.max.tool.icon.size.kb didn't lead to anything - restriction for the size while trying to change an icon was remaining the same - 50 Kb.","title":"Setting of the tool icon size"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#npe-while-building-cloud-specific-environment-variables-for-run","text":"#486 For each run a set of cloud-specific environment variables (including account names, credentials, etc.) is build. This functionality resulted to fails with NPE when some of these variables are null . Now, such null variables are filtered out with warn logs.","title":"NPE while building cloud-specific environment variables for run"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#worker-nodes-fail-due-to-mismatch-of-the-regions-with-the-parent-run","text":"#485 In certain cases, when a new child run was launching in cluster, cloud region was not specified directly and it might be created in a region differing from the parent run, that could lead to fails. Now, worker runs inherit parent's run cloud region.","title":"Worker nodes fail due to mismatch of the regions with the parent run"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#worker-nodes-shall-not-be-restarted-automatically","text":"#483 Cloud Pipeline has a functionality to restart so called batch job runs automatically when run is terminated due to some technical issues, e.g. spot instance termination. Previously, runs that were created as child nodes for some parent run were also restarted. Now, automatically child reruns for the described cases with the batch job runs are rejected.","title":"Worker nodes shall not be restarted automatically"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#uploaded-storage-file-content-is-downloaded-back-to-client","text":"#478 Cloud Pipeline clients use specific POST API method to upload local files to the cloud storages. Previously, this method not only uploaded files to the cloud storage but also mistakenly returned uploaded file content back to the client. It led to a significant upload time increase.","title":"Uploaded storage file content is downloaded back to client"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#gui-improperly-works-with-detached-configurations-in-a-non-default-region","text":"#476 Saved instance type of a non-default region in a detached configuration wasn't displayed in case when such configuration was reopened (instance type field was displayed as empty in that cases).","title":"GUI improperly works with detached configurations in a non-default region"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#detached-configuration-doesnt-respect-region-setting","text":"#458 Region setting was not applied when pipeline is launched using detached configuration. Now, cloud region ID is merged into the detached configuration settings.","title":"Detached configuration doesn't respect region setting"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#incorrect-behavior-of-the-transfer-to-the-cloud-form-in-case-when-a-subfolder-has-own-metadata","text":"#434 Previously, when you tried to download files from external resources using metadata (see here ) and in that metadata's folder there was any subfolder with its own metadata - on the \"Transfer to the Cloud\" form attributes (columns) of both metadata files were mistakenly displaying.","title":"Incorrect behavior of the \"Transfer to the cloud\" form in case when a subfolder has own metadata"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#incorrect-displaying-of-the-start-idle-checkbox","text":"#418 If for the configuration form with several tabs user was setting the Start idle checkbox on any tab and then switched between sub-configurations tabs - the \"checked\" state of the Start idle checkbox didn't change, even if Cmd template field was appearing with its value (these events are mutually exclusive).","title":"Incorrect displaying of the \"Start idle\" checkbox"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#limit-check-of-the-maximum-cluster-size-is-incorrect","text":"#412 Maximum allowed number of runs (size of the cluster) created at once is limited by system preference launch.max.scheduled.number . This check used strictly \"less\" check rather then \"less or equal\" to allow or deny cluster launch. Now, the \"less or equal\" check is used.","title":"Limit check of the maximum cluster size is incorrect"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#fixed-cluster-with-sge-and-dind-capabilities-fails-to-start","text":"#392 Previously, fixed cluster with both CP_CAP_SGE and CP_CAP_DIND_CONTAINER options enabled with more than one worker failed to start. Some of the workers failed on either SGEWorkerSetup or SetupDind task with different errors. Scripts were executed in the same one shared analysis directory. So, some workers could delete files downloaded by other workers.","title":"Fixed cluster with SGE and DIND capabilities fails to start"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#azure-server-shall-check-azure-blob-existence-when-a-new-storage-is-created","text":"#768 During the creation of AZ Storage, the validation whether Azure Blob exists or not didn't perform. In that case, if Azure Blob had already existed, the user was getting failed request with Azure exception.","title":"Azure: Server shall check Azure Blob existence when a new storage is created"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#azure-pipe-cli-cannot-transfer-empty-files-between-storages","text":"#386 Previously, empty files couldn't be transferred within a single Azure storage or between two Azure storages using pipe CLI, it throwed an error. So for example, a folder that contained empty files couldn't be copied correctly.","title":"Azure: pipe CLI cannot transfer empty files between storages"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#azure-runs-with-enabled-ge-autoscaling-doesnt-stop","text":"#377 All Azure runs with enabled GE autoscaling were stuck after the launch.sh script has finished its execution. Daemon GE autoscaler process kept container alive. It was caused by the run process stdout and stderr aren't handled the same way for different Cloud Provider. So background processes launched from launch.sh directly could prevent Azure run finalization.","title":"Azure: runs with enabled GE autoscaling doesn't stop"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#incorrect-behavior-while-download-files-from-external-resources-into-several-folders","text":"#373 If user was tried to download files from external resources and at the Transfer settings form was set Create folders for each path field checkbox without setting any name field, all files downloaded into one folder without creating folders for each path field (column).","title":"Incorrect behavior while download files from external resources into several folders"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#detach-configuration-doesnt-setup-sge-for-a-single-master-run","text":"#342 Grid Engine installation was mistakenly being skipped, if pipeline was launched with enabled system parameter CP_CAP_SGE via a detach configuration.","title":"Detach configuration doesn't setup SGE for a single master run"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#broken-layouts","text":"#747 , #834 Previously, pipeline versions page had broken layout if there \"Attributes\" and \"Issues\" panels were simultaneously opened. If there were a lot of node labels at the Cluster nodes page, some of them were \"broken\" and spaced to different lines. Some of the other page layouts also were broken.","title":"Broken layouts"}]}